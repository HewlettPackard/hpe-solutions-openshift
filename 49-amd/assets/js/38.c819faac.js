(window.webpackJsonp=window.webpackJsonp||[]).push([[38],{552:function(e,t,a){"use strict";a.r(t);var s=a(54),n=Object(s.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"virtual-node-configuration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#virtual-node-configuration"}},[e._v("#")]),e._v(" Virtual node configuration")]),e._v(" "),a("p",[e._v("This section describes the process to deploy virtualization hosts for OpenShift. This section outlines the steps required to configure virtual machine master and worker nodes. At a high level, these steps are as follows:")]),e._v(" "),a("ul",[a("li",[a("p",[e._v("Deploying the vSphere hosts")])]),e._v(" "),a("li",[a("p",[e._v("Creating the data center, cluster, and adding hosts into the cluster")])]),e._v(" "),a("li",[a("p",[e._v("Creating a datastore in vCenter")])]),e._v(" "),a("li",[a("p",[e._v("Create virtual master nodes")])]),e._v(" "),a("li",[a("p",[e._v("Deploying virtual worker nodes")])])]),e._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[e._v("NOTE")]),e._v(" "),a("p",[e._v("Hewlett Packard Enterprise utilized a consistent method for deployment that would allow for mixed deployments of virtual and physical master and worker nodes and built this solution on bare metal using the Red Hat OpenShift Container Platform user-provisioned infrastructure. For more details on the bare metal provisioner, refer to "),a("a",{attrs:{href:"https://cloud.redhat.com/openshift/install/metal/user-provisioned",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://cloud.redhat.com/openshift/install/metal/user-provisioned"),a("OutboundLink")],1),e._v(". If the intent is to have an overall virtual environment, it is recommended the installation user utilizes Red Hat's virtual provisioning methods found at "),a("a",{attrs:{href:"https://docs.openshift.com/container-platform/4.9/installing/installing_vsphere/installing-vsphere.html#installing-vsphere",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://docs.openshift.com/container-platform/4.9/installing/installing_vsphere/installing-vsphere.html#installing-vsphere."),a("OutboundLink")],1)])]),e._v(" "),a("h3",{attrs:{id:"deploying-vsphere-hosts"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#deploying-vsphere-hosts"}},[e._v("#")]),e._v(" Deploying vSphere hosts")]),e._v(" "),a("p",[e._v("The following steps describes the process to install the hypervisor:")]),e._v(" "),a("ol",[a("li",[a("p",[e._v("From the HPE iLO interface, navigate to Remote Session and launch Console.")])]),e._v(" "),a("li",[a("p",[e._v("From the Remote Console window, choose "),a("strong",[e._v("Virtual Drives -> Image File CD-ROM/DVD")]),e._v(" from the "),a("strong",[e._v("iLO options")]),e._v(" menu bar.")])]),e._v(" "),a("li",[a("p",[e._v("Navigate to the VMware ESXi 6.7 ISO file located on the installation system. Select the ISO file and click "),a("strong",[e._v("Open")]),e._v(".")])]),e._v(" "),a("li",[a("p",[e._v("If the server is in the powered off state, power switch on the server by selecting "),a("strong",[e._v("Power Switch -> Momentary Press.")])])]),e._v(" "),a("li",[a("p",[e._v("During boot, press "),a("strong",[e._v("F11")]),e._v(" Boot Menu and select iLO Virtual USB 3: iLO Virtual CD-ROM.")])]),e._v(" "),a("li",[a("p",[e._v("When the VMware ESXi installation media has finished loading, proceed through the VMware user prompts. For storage device, select the 40 GiB OS volume and "),a("strong",[e._v("set the root password.")])])]),e._v(" "),a("li",[a("p",[e._v("Wait until the vSphere installation is complete.")])]),e._v(" "),a("li",[a("p",[e._v("After the installation is complete, press "),a("strong",[e._v("F2")]),e._v(" to enter the vSphere host configuration page and update the IP address, gateway, DNS, hostname of the host and enable SSH.")])]),e._v(" "),a("li",[a("p",[e._v("After the host is reachable, proceed with the next section.")])])]),e._v(" "),a("h3",{attrs:{id:"creating-the-data-center-cluster-and-adding-hosts-in-vmware-vcenter"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#creating-the-data-center-cluster-and-adding-hosts-in-vmware-vcenter"}},[e._v("#")]),e._v(" Creating the Data center, Cluster and adding Hosts in VMware vCenter")]),e._v(" "),a("p",[e._v("This section assumes a VMware vCenter server is available within the installation environment. A data center is a structure in VMware vCenter which contains clusters, hosts, and datastore. To begin with, a data center needs to be created, followed by the clusters and adding hosts into the clusters.")]),e._v(" "),a("p",[e._v("To create a data center, a cluster enabled with vSAN and DRS and adding hosts, the installation user will need to edit the vault file and the variables YAML file. Using an editor, open the file /opt/hpe/solutions/ocp/hpe-solutions-openshift/DL/scalable/vsphere/vcenter/roles/prepare_vcenter/vars/main.yml to provide the names for data center, clusters and vSphere hostnames. A sample input file is listed and as follows. Installation user should modify this file to suit the environment.")]),e._v(" "),a("p",[e._v("In the Ansible vault file ("),a("em",[e._v("secret.yml")]),e._v(") found at /opt/hpe/solutions/ocp/"),a("em",[e._v("hpe-solutions-openshift/DL/scalable/vsphere/vcenter")]),e._v(", provide the vCenter and the vSphere host credentials.")]),e._v(" "),a("div",{staticClass:"language-yaml extra-class"},[a("pre",{pre:!0,attrs:{class:"language-yaml"}},[a("code",[e._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# vsphere hosts credentials")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("vsphere_username")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" <username"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(">")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("vsphere_password")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" <password"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(">")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# vcenter hostname/ip address and credentials")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("vcenter_hostname")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" x.x.x.x\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("vcenter_username")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" <username"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(">")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("vcenter_password")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" <password"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(">")]),e._v("\n\n")])])]),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[e._v("NOTE")]),e._v(" "),a("p",[e._v("This section assumes all the virtualization hosts have a common username and password. If it does not have a common username and password, it is up to the installation user to add the virtualization hosts within the appropriate cluster.")])]),e._v(" "),a("p",[e._v("Variables for running the playbook can be found at /opt/hpe/solutions/ocp/hpe-solutions-openshift/DL/scalable/vsphere/vcenter/roles/prepare_vcenter/vars/main.yml.")]),e._v(" "),a("div",{staticClass:"language-yaml extra-class"},[a("pre",{pre:!0,attrs:{class:"language-yaml"}},[a("code",[e._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# custom name for data center to be created.")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("datacenter_name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" datacenter\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# custom name of the compute clusters with the ESXi hosts for Management VMs.")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("management_cluster_name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" management"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("-")]),e._v("cluster\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# hostname or IP address of the vsphere hosts utilized for the management nodes.")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("vsphere_host_01")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" 10.0.x.x\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("vsphere_host_02")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" 10.0.x.x\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("vsphere_host_03")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" 10.0.x.x\n")])])]),a("p",[e._v("After the variable files are updated with the appropriate values, execute the following command within the installer VM to create the data center, clusters, and add hosts into respective clusters.")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[e._v("cd")]),e._v(" /opt/hpe/solutions/ocp/hpe-solutions-openshift/DL/scalable/vsphere/vcenter/\n")])])]),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">")]),e._v(" ansible-playbook playbooks/prepare_vcenter.yml â€“ask-vault-pass\n")])])]),a("h3",{attrs:{id:"creating-a-datastore-in-vcenter"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#creating-a-datastore-in-vcenter"}},[e._v("#")]),e._v(" Creating a Datastore in vCenter")]),e._v(" "),a("p",[e._v("A datastore needs to be created in VMware vCenter from the volume carved out of HPE Storage SANs to store the VMs. The following are the steps to create a datastore in vCenter:")]),e._v(" "),a("ol",[a("li",[a("p",[e._v("From the vSphere Web Client navigator, right-click the cluster, select "),a("strong",[e._v("Storage")]),e._v(" from the menu, and then select the "),a("strong",[e._v("New Datastore")]),e._v(".")])]),e._v(" "),a("li",[a("p",[e._v("From the Type page, select "),a("strong",[e._v("VMFS")]),e._v(" as the Datastore type and click "),a("strong",[e._v("Next")]),e._v(".")])]),e._v(" "),a("li",[a("p",[e._v("Enter the datastore name and if necessary, select the placement location for the datastore and click "),a("strong",[e._v("Next")]),e._v(".")])]),e._v(" "),a("li",[a("p",[e._v("Select the device to use for the datastore and click "),a("strong",[e._v("Next")]),e._v(".")])]),e._v(" "),a("li",[a("p",[e._v("From VMFS version page, select "),a("strong",[e._v("VMFS 6")]),e._v(" and click "),a("strong",[e._v("Next")]),e._v(".")])]),e._v(" "),a("li",[a("p",[e._v("Define the following configuration requirements for the datastore as per the installation environment and click "),a("strong",[e._v("Next")]),e._v(".")]),e._v(" "),a("p",[e._v("a.  Specify partition configuration")]),e._v(" "),a("p",[e._v("b.  Datastore Size")]),e._v(" "),a("p",[e._v("c.  Block Size")]),e._v(" "),a("p",[e._v("d.  Space Reclamation Granularity")]),e._v(" "),a("p",[e._v("e.  Space Reclamation Priority")])]),e._v(" "),a("li",[a("p",[e._v("On the Ready to complete page, review the Datastore configuration and click "),a("strong",[e._v("Finish")]),e._v(".")]),e._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[e._v("NOTE")]),e._v(" "),a("p",[e._v("If you utilize virtual worker nodes, repeat this section to create a Datastore to store the worker virtual machines.")])])])]),e._v(" "),a("h3",{attrs:{id:"red-hat-openshift-container-platform-sizing"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#red-hat-openshift-container-platform-sizing"}},[e._v("#")]),e._v(" Red Hat OpenShift Container Platform sizing")]),e._v(" "),a("p",[e._v("Red Hat OpenShift Container Platform sizing varies depending on the requirements of the organization and type of deployment. This section highlights the host sizing details recommended by Red Hat.")]),e._v(" "),a("table",[a("thead",[a("tr",[a("th",[e._v("Resource")]),e._v(" "),a("th",[e._v("Bootstrap node")]),e._v(" "),a("th",[e._v("Master node")]),e._v(" "),a("th",[e._v("Worker node")])])]),e._v(" "),a("tbody",[a("tr",[a("td",[e._v("CPU")]),e._v(" "),a("td",[e._v("4")]),e._v(" "),a("td",[e._v("4")]),e._v(" "),a("td",[e._v("4")])]),e._v(" "),a("tr",[a("td",[e._v("Memory")]),e._v(" "),a("td",[e._v("16GB")]),e._v(" "),a("td",[e._v("16GB")]),e._v(" "),a("td",[e._v("16GB")])]),e._v(" "),a("tr",[a("td",[e._v("Disk storage")]),e._v(" "),a("td",[e._v("120GB")]),e._v(" "),a("td",[e._v("120GB")]),e._v(" "),a("td",[e._v("120GB")])]),e._v(" "),a("tr",[a("td",[e._v("Disk storage")]),e._v(" "),a("td",[e._v("120GB")]),e._v(" "),a("td",[e._v("120GB")]),e._v(" "),a("td",[e._v("120GB")])])])]),e._v(" "),a("p",[e._v("Disk partitions on each of the nodes are as follows.")]),e._v(" "),a("ul",[a("li",[a("p",[e._v("/var -- 40GB")])]),e._v(" "),a("li",[a("p",[e._v("/usr/local/bin -- 1GB")])]),e._v(" "),a("li",[a("p",[e._v("Temporary directory -- 1GB")])])]),e._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[e._v("NOTE")]),e._v(" "),a("p",[e._v("Sizing for worker nodes is ultimately dependent on the container workloads and their CPU, memory, and disk requirements.")]),e._v(" "),a("p",[e._v("For more information about Red Hat OpenShift Container Platform sizing, refer to the Red Hat OpenShift Container Platform 4.9 product documentation at "),a("a",{attrs:{href:"https://access.redhat.com/documentation/en-us/openshift_container_platform/4.9/html/scalability_and_performance/index",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://access.redhat.com/documentation/en-us/openshift_container_platform/4.9/html/scalability_and_performance/index."),a("OutboundLink")],1)])]),e._v(" "),a("h3",{attrs:{id:"deploying-virtual-master-nodes"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#deploying-virtual-master-nodes"}},[e._v("#")]),e._v(" Deploying virtual master nodes")]),e._v(" "),a("p",[e._v("This section outlines the steps to create the virtual machines used as the master nodes.")]),e._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[e._v("NOTE")]),e._v(" "),a("p",[e._v("This section utilized vSphere rules such as affinity and anti-affinity rules to ensure no two master nodes are present on the same vSphere host, hence it is essential to enable vMotion in all the vSphere hosts. If not enabled, select the vSphere host in the VMware vCenter server user interface, "),a("strong",[e._v("click -> Configure -> Networking -> VMkernel adapters -> Management Network -> Edit")]),e._v(" and select the checkbox against vMotion to enable vMotion.")])]),e._v(" "),a("p",[e._v("To create the virtual machines for the OpenShift master nodes, edit the variables file. Use an editor such as Vim or Nano, open the file /opt/hpe/solutions/ocp/"),a("em",[e._v("hpe-solutions-openshift/DL/scalable/vsphere/virtual_nodes /roles/deploy_vm/vars/main.yml.")]),e._v(" The variable file contains information about the VMs, vCenter, hostnames, IP addresses, memory, and CPU. A sample variable file is provided and as follows. The installation user should modify the file to make it suitable for the target environment.")]),e._v(" "),a("div",{staticClass:"language-yaml extra-class"},[a("pre",{pre:!0,attrs:{class:"language-yaml"}},[a("code",[e._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# Name of the Data center")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("datacenter_name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" <datacentername"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(">")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# Name of the compute clusters with the ESXi hosts for Management VMs")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("management_cluster_name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" <data_cluster_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(">")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# Name of the Datastore to store the VMs")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("management_datastore_name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" <datastore_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(">")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# Name of the coreOS guest image")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("guest_template")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" coreos64Guest\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# Disk size in GB/GiB")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("bootstrap_disk")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[e._v("120")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("master_disk")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[e._v("120")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("lb_disk")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[e._v("50")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# number of CPUs")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("bootstrap_cpu")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[e._v("4")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("master_cpu")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[e._v("4")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("lb_cpu")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[e._v("4")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# Memory size in MB/MiB")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("bootstrap_memory")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[e._v("16400")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("master_memory")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[e._v("16400")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("lb_memory")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[e._v("16400")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("gateway")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" <replace_with_gateway_ip"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(">")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("dns_server")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" <replace_with_dns_server_ip"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(">")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("domain")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" <replace_with_domain_ip"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(">")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# name of the master, bootstrap and lb nodes < short names, not the FQDN >")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("bootstrap01_name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" <bootstrap01_host_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(">")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("master01_name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" <master01_host_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(">")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("master02_name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" <master02_host_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(">")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("master03_name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" <master03_host_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(">")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("lb01_name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" <lb01_host_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(">")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("domain_name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[e._v('"<sub_domain>"')]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# Network names for the management network")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("datacenter_network_name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[e._v('"<network_name>"')]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# vSphere affinity & anti-affinity rules")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("affinity_rule_name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[e._v('"vsphere-anti-affinty-rule"')]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token key atrule"}},[e._v("anti_affinity_rule_name")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[e._v('"vsphere-affinty-rule"')]),e._v("\n")])])]),a("p",[e._v("After the variable file is updated, execute the following command from the installer machine to deploy the specified VMs.")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[e._v("cd")]),e._v(" /opt/hpe/solutions/ocp/hpe-solutions-openshift/DL/scalable/vsphere/virtual_nodes\n")])])]),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">")]),e._v(" ansible-playbook playbooks/deploy_vm.yml â€“ask-vault-pass\n")])])]),a("p",[a("em",[e._v("deploy_vm.yml")]),e._v(" playbooks create 3x VMs to be used as master nodes, 1x VM to be used as load balancer node and 1x VM to be used as a bootstrap node. All the master VMs will be deployed on different hosts whereas bootstrap & haproxy VMs will be deployed on any single host.")]),e._v(" "),a("p",[e._v("Wait for some time for vSphere rules to be applicable on VMs. vSphere rules can be viewed at "),a("strong",[e._v("vCenter server -> Datacenter -> Cluster -> Configure -> VM/Host Rules")]),e._v(".The 3 master nodes are part of the "),a("em",[e._v("vsphere-anti-affinity-rule")]),e._v(" and each master VM will reside on a different vSphere host. The bootstrap and load balancer VMs are part of the "),a("em",[e._v("vsphere-affinity-rule")]),e._v(" and they are co-resident on one of 3 vSphere hosts.")]),e._v(" "),a("p",[e._v("It is recommended to ensure the "),a("em",[e._v("Boot Delay")]),e._v(" is long enough to enable OS installation via PXE server.")]),e._v(" "),a("p",[e._v("After the virtual machines are successfully created, refer to the following steps to install the operating system on the bootstrap node and the master nodes:")]),e._v(" "),a("ol",[a("li",[a("p",[e._v("Ensure that the location of ignition files of the corresponding nodes is updated in the PXE configuration files.")])]),e._v(" "),a("li",[a("p",[e._v("Ensure the MAC address of the network adapter in VM is updated with the corresponding IP address in the DHCP configuration file.")])]),e._v(" "),a("li",[a("p",[e._v("Ensure that the load balancer server is up and running.")])]),e._v(" "),a("li",[a("p",[e._v("From the VMware vCenter Server, select the VM, and launch the VM Remote console.")])]),e._v(" "),a("li",[a("p",[e._v("From the Remote Console window, power on the VM.")])]),e._v(" "),a("li",[a("p",[e._v("While booting, select the appropriate OS label.")])]),e._v(" "),a("li",[a("p",[e._v("Wait until the OS installation is complete.")])]),e._v(" "),a("li",[a("p",[e._v("Verify the installation by logging on to the node from the installer VM using the following command.")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("ssh")]),e._v(" core@"),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("<")]),e._v(" replace_with_node_fqdn_or_ip "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">")]),e._v("\n")])])])])]),e._v(" "),a("p",[e._v("After the RHCOS master nodes are ready, refer to the section "),a("a",{attrs:{href:"../solution-deployment/ocp-cluster-deployment#red-hat-openShift-container-platform-cluster-deployment"}},[e._v("Red Hat OpenShift Container Platform Cluster deployment")]),e._v(" in this document to create the OpenShift 4 cluster.")]),e._v(" "),a("h3",{attrs:{id:"deploying-virtual-worker-nodes"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#deploying-virtual-worker-nodes"}},[e._v("#")]),e._v(" Deploying virtual worker nodes")]),e._v(" "),a("p",[e._v("This section outlines the steps to create virtual machines and configure them to be used as worker nodes.")]),e._v(" "),a("h3",{attrs:{id:"creating-virtual-machines"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#creating-virtual-machines"}},[e._v("#")]),e._v(" Creating virtual machines")]),e._v(" "),a("p",[e._v("This section outlines the steps to create virtual machines.")]),e._v(" "),a("ol",[a("li",[a("p",[e._v("Login to vCenter using the Web Client and select an ESXi Host. Right-click the host and then click "),a("strong",[e._v("New Virtual Machine")]),e._v(" to open a "),a("em",[e._v("New Virtual Machine Wizard")]),e._v(".")])]),e._v(" "),a("li",[a("p",[e._v("From "),a("em",[e._v("Select a creation type")]),e._v(", select "),a("strong",[e._v("Create a new virtual machine")]),e._v(" and click "),a("strong",[e._v("Next")]),e._v(".")])]),e._v(" "),a("li",[a("p",[e._v("Enter a unique "),a("em",[e._v("Name")]),e._v(" for the VM and select the "),a("strong",[e._v("Datacenter")]),e._v(". Click "),a("strong",[e._v("Next")]),e._v(".")])]),e._v(" "),a("li",[a("p",[e._v("Select the "),a("strong",[e._v("Cluster")]),e._v(" on which the VM can be deployed. Click "),a("strong",[e._v("Next")]),e._v(".")])]),e._v(" "),a("li",[a("p",[e._v("Select the "),a("strong",[e._v("Datastore")]),e._v(" on which the VM can be stored and click "),a("strong",[e._v("Next")]),e._v(".")])]),e._v(" "),a("li",[a("p",[e._v("On the Select compatibility page, choose "),a("strong",[e._v("ESXI 6.7 and later")]),e._v(" and click "),a("strong",[e._v("Next")]),e._v(".")])]),e._v(" "),a("li",[a("p",[e._v("On the Select a guest OS page, choose the Guest OS family as "),a("strong",[e._v("Linux")]),e._v(" and Guest OS Version as "),a("strong",[e._v("Red Hat Enterprise Linux 7 (64 bit)")]),e._v(" (in case of RHEL worker) and "),a("strong",[e._v("Red Hat CoreOS")]),e._v(" (in case of Red Hat CoreOS worker) and select "),a("strong",[e._v("Next")]),e._v(".")])]),e._v(" "),a("li",[a("p",[e._v("In the Customize hardware page, configure the Virtual Hardware with "),a("strong",[e._v("4 CPU")]),e._v(", "),a("strong",[e._v("16 GB")]),e._v(" Memory, "),a("strong",[e._v("150 GB")]),e._v(" "),a("strong",[e._v("dual Hard Disk")]),e._v(" as per requirement and attach the Operating System from the datastore.\nSelect the Connect at "),a("strong",[e._v("Power on")]),e._v(" option and click "),a("strong",[e._v("Next")]),e._v(".")])]),e._v(" "),a("li",[a("p",[e._v("Review the virtual machine configuration before deploying the virtual machine and click "),a("strong",[e._v("Finish")]),e._v(" to complete the New Virtual Machine wizard.")])])]),e._v(" "),a("h3",{attrs:{id:"red-hat-coreos-worker-nodes"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#red-hat-coreos-worker-nodes"}},[e._v("#")]),e._v(" Red Hat CoreOS worker nodes")]),e._v(" "),a("p",[e._v("Refer to the section "),a("a",{attrs:{href:"#creating-virtual-machines"}},[e._v("Creating virtual machines")]),e._v(" in the document to create virtual machines. After the virtual machines are successfully created, refer to the following steps to install the\noperating system on the worker nodes:")]),e._v(" "),a("ol",[a("li",[a("p",[e._v("Ensure that the location of ignition files of the corresponding nodes is updated in the PXE configuration files.")])]),e._v(" "),a("li",[a("p",[e._v("Ensure the MAC address of the network adapter in VM is updated with the corresponding IP address in the DHCP configuration file.")])]),e._v(" "),a("li",[a("p",[e._v("Ensure that the load balancer server is up and running.")])]),e._v(" "),a("li",[a("p",[e._v("From the VMware vCenter Server, select the VM and launch the VM Remote console.")])]),e._v(" "),a("li",[a("p",[e._v("From the Remote Console window, power on the VM.")])]),e._v(" "),a("li",[a("p",[e._v("While booting, select the appropriate OS label.")])]),e._v(" "),a("li",[a("p",[e._v("Wait until the OS installation is complete.")])]),e._v(" "),a("li",[a("p",[e._v("Verify the installation by logging on to the node from the installer VM using the following command.")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("ssh")]),e._v(" core@"),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("<")]),e._v(" replace_with_node_fqdn_or_ip "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">")]),e._v("\n")])])])])]),e._v(" "),a("p",[e._v("After the RHCOS worker nodes are up and running, refer to the section "),a("a",{attrs:{href:"../solution-deployment/ocp-worker-nodes#adding-rhel-7.6-worker-nodes"}},[e._v("Adding RHEL 7.6 worker nodes")]),e._v(" in the document to add them to OpenShift 4 cluster.")]),e._v(" "),a("h3",{attrs:{id:"rhel-7-6-worker-nodes"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#rhel-7-6-worker-nodes"}},[e._v("#")]),e._v(" RHEL 7.6 worker nodes")]),e._v(" "),a("p",[e._v("Refer to the section "),a("a",{attrs:{href:"#creating-virtual-machines"}},[e._v("Creating virtual machines")]),e._v(" in this document to create virtual machines. After the virtual machines are successfully created, follow these steps to install the operating system on the worker nodes:")]),e._v(" "),a("ol",[a("li",[a("p",[e._v("Ensure that the location of ignition files of the corresponding nodes is updated in the PXE configuration files.")])]),e._v(" "),a("li",[a("p",[e._v("Ensure the MAC address of the network adapter in VM is updated with the corresponding IP address in the DHCP configuration file.")])]),e._v(" "),a("li",[a("p",[e._v("Ensure that the load balancer server is up and running.")])]),e._v(" "),a("li",[a("p",[e._v("From the VMware vCenter Server, select the VM, and launch the VM Remote console.")])]),e._v(" "),a("li",[a("p",[e._v("From the Remote Console window, power on the VM.")])]),e._v(" "),a("li",[a("p",[e._v("While booting, select the appropriate OS label.")])]),e._v(" "),a("li",[a("p",[e._v("Wait until the OS installation is complete.")])]),e._v(" "),a("li",[a("p",[e._v("Verify the installation by logging on to the node from the installer VM using the following command.")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("ssh")]),e._v(" root@"),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("<")]),e._v(" node_fqdn or node_ip_address"),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">")]),e._v("\n")])])])])]),e._v(" "),a("p",[e._v("Once the RHEL 7.6 nodes are reachable, refer to the section "),a("a",{attrs:{href:"../solution-deployment/physical-node-configuration#preparing-worker-nodes-with-rhel"}},[e._v("RHEL 7.6 worker nodes")]),e._v(" in the document to prepare the RHEL worker nodes. After preparing the worker nodes, refer to the section "),a("a",{attrs:{href:"../solution-deployment/ocp-worker-nodes##adding-rhel-7.6-worker-nodes"}},[e._v("Adding RHEL 7.6 worker nodes")]),e._v(" in the document to add them to the OpenShift 4 cluster.")])])}),[],!1,null,null,null);t.default=n.exports}}]);