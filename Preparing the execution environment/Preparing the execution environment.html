<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Preparing the execution environment | RED HAT OPENSHIFT CONTAINER PLATFORM 4 ON HPE SYNERGY</title>
    <meta name="description" content="">
    <meta name="generator" content="VuePress 1.4.0">
    
    
    <link rel="preload" href="/hpe-solutions-openshift/assets/css/0.styles.03110986.css" as="style"><link rel="preload" href="/hpe-solutions-openshift/assets/js/app.adad66b3.js" as="script"><link rel="preload" href="/hpe-solutions-openshift/assets/js/2.ac0f675e.js" as="script"><link rel="preload" href="/hpe-solutions-openshift/assets/js/8.97285f82.js" as="script"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/10.bbe6a8b1.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/11.c231a443.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/12.381ec328.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/13.bc9c4fc5.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/14.6c2a59eb.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/15.10105a64.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/16.de305cb4.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/17.c566bc9d.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/18.1a3abf91.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/19.e7a9cad7.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/20.826352b1.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/21.e9b44a36.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/3.f3b6057a.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/4.d098f71d.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/5.c9b2efa3.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/6.e3886d8a.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/7.8d533ade.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/9.cd9b026d.js">
    <link rel="stylesheet" href="/hpe-solutions-openshift/assets/css/0.styles.03110986.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/hpe-solutions-openshift/" class="home-link router-link-active"><!----> <span class="site-name">RED HAT OPENSHIFT CONTAINER PLATFORM 4 ON HPE SYNERGY</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/hpe-solutions-openshift/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="http://www.hpe.com/info/ra" target="_blank" rel="noopener noreferrer" class="nav-link external">
  RA Library
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/hpe-solutions-openshift/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="http://www.hpe.com/info/ra" target="_blank" rel="noopener noreferrer" class="nav-link external">
  RA Library
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/hpe-solutions-openshift/" class="sidebar-link">Introduction</a></li><li><a href="/hpe-solutions-openshift/Solution overview/Solution overview.html" class="sidebar-link">Solution overview</a></li><li><a href="/hpe-solutions-openshift/Solution components/Solution components.html" class="sidebar-link">Solution components</a></li><li><a href="/hpe-solutions-openshift/Preparing the execution environment/Preparing the execution environment.html" class="active sidebar-link">Preparing the execution environment</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Preparing the execution environment/Preparing the execution environment.html#non-root-user-access" class="sidebar-link">Non-root user access</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Preparing the execution environment/Preparing the execution environment.html#installer-machine" class="sidebar-link">Installer machine</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Preparing the execution environment/Preparing the execution environment.html#kubernetes-manifests-and-ignition-files" class="sidebar-link">Kubernetes manifests and ignition files</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Preparing the execution environment/Preparing the execution environment.html#pxe-server" class="sidebar-link">PXE Server</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Preparing the execution environment/Preparing the execution environment.html#load-balancer" class="sidebar-link">Load balancer</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Preparing the execution environment/Preparing the execution environment.html#user-provisioned-dns-requirements" class="sidebar-link">User-provisioned DNS requirements</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Preparing the execution environment/Preparing the execution environment.html#bootstrap-node" class="sidebar-link">Bootstrap node</a></li></ul></li><li><a href="/hpe-solutions-openshift/Physical environment configuration/Physical environment configuration.html" class="sidebar-link">Physical environment configuration</a></li><li><a href="/hpe-solutions-openshift/Physical node configuration/Physical node configuration.html" class="sidebar-link">Physical node configuration</a></li><li><a href="/hpe-solutions-openshift/Virtual nodes configuration/Virtual nodes configuration.html" class="sidebar-link">Virtual nodes configuration</a></li><li><a href="/hpe-solutions-openshift/Red Hat OpenShift Container Platform deployment/Red Hat OpenShift Container Platform deployment.html" class="sidebar-link">Red Hat OpenShift Container Platform deployment</a></li><li><a href="/hpe-solutions-openshift/Red Hat Local Storage Operator/Red Hat Local Storage Operator.html" class="sidebar-link">Red Hat Local Storage Operator</a></li><li><a href="/hpe-solutions-openshift/Securing RedHat OpenShift Container Platform using Sysdig Secure and Sysdig Monitor/Securing RedHat OpenShift Container Platform using Sysdig Secure and Sysdig Monitor.html" class="sidebar-link">Securing RedHat OpenShift Container Platform using Sysdig Secure and Sysdig Monitor</a></li><li><a href="/hpe-solutions-openshift/Physical worker node labeling in OpenShift/Physical worker node labeling in OpenShift.html" class="sidebar-link">Physical worker node labeling in OpenShift</a></li><li><a href="/hpe-solutions-openshift/OpenShift Operators/OpenShift Operators.html" class="sidebar-link">OpenShift Operators</a></li><li><a href="/hpe-solutions-openshift/Validating OpenShift Container Platform deployment/Validating OpenShift Container Platform deployment.html" class="sidebar-link">Validating OpenShift Container Platform deployment</a></li><li><a href="/hpe-solutions-openshift/Resources and additional links/Resources and additional links.html" class="sidebar-link">Resources and additional links</a></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="preparing-the-execution-environment"><a href="#preparing-the-execution-environment" class="header-anchor">#</a> Preparing the execution environment</h1> <p>This section provides a detailed overview and steps to configure the components deployed for this solution.</p> <h2 id="non-root-user-access"><a href="#non-root-user-access" class="header-anchor">#</a> Non-root user access</h2> <p>The industry-wide security best practice to avoid the use of root user account for administration of Linux based servers. However, certain operations require root user privileges to perform tasks. In those cases, it is best to use the sudo command to obtain the necessary privilege escalation on a short-term basis. The sudo command allows programs and commands to be run with the security privileges of another user (Root is the default user) and can restrict the permissions to specific groups, users, and individual commands.</p> <p>The root user is not active by default in RHCOS. Instead, log in as the core user.</p> <p>Use the following steps to create a non-root user for the OpenShift installation process:</p> <ol><li><p>Login to the installer VM as root. Refer to the section [Installer machine] in this document for more details about the installer VM.</p></li> <li><p>Execute the following command to create a new user.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> adduser openshift_admin
</code></pre></div></li> <li><p>Execute the following command to set password for the new user.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">passwd</span> openshift_admin
</code></pre></div></li> <li><p>Edit the sudoers file and add the entry of new user in the sudoers file using the following command.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> visudo
</code></pre></div><p>An example of sudoers file is as follows.</p> <div class="language- extra-class"><pre class="language-text"><code># Allow root to run any commands anywhere
root ALL=(ALL) ALL
openshift_admin ALL=(ALL) NOPASSWD:ALL

# Allow members of group sudo to execute any command
%sudo ALL=(ALL:ALL) NOPASSWD: Allow
</code></pre></div></li> <li><p>Execute the following command to change the user.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">su</span> openshift_admin
</code></pre></div></li></ol> <h2 id="installer-machine"><a href="#installer-machine" class="header-anchor">#</a> Installer machine</h2> <p>This document assumes that a server running Red Hat Enterprise Linux 7.6 exists within the deployment environment and is accessible to the installation user to be used as an installer machine. This server must have internet connectivity. In this solution, a virtual machine is used to act as an installer machine and the same host is utilized as an Ansible Engine host.</p> <p>Log in to the installer VM as non-root user and perform the following steps:</p> <ol><li><p>Register the host and attach the host pool with Red Hat by executing the following command.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">sudo</span> subscription-manager register --username<span class="token operator">=</span><span class="token operator">&lt;</span>username<span class="token operator">&gt;</span> --password<span class="token operator">=</span><span class="token operator">&lt;</span>password<span class="token operator">&gt;</span> --auto-attach
</code></pre></div></li> <li><p>Disable all repositories and enable only the repositories required for the installer VM.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">sudo</span> yum-config-manager --disable <span class="token punctuation">\</span>*

<span class="token operator">&gt;</span> <span class="token function">sudo</span> subscription-manager repos --disable<span class="token operator">=</span><span class="token string">&quot;*&quot;</span> <span class="token punctuation">\</span>

--enable<span class="token operator">=</span><span class="token string">&quot;rhel-7-server-rpms&quot;</span> <span class="token punctuation">\</span>

--enable<span class="token operator">=</span><span class="token string">&quot;rhel-7-server-extras-rpms&quot;</span>

</code></pre></div></li> <li><p>Execute the following commands to download the hpe-solutions-openshift repository.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token builtin class-name">cd</span> /etc/ansible

<span class="token operator">&gt;</span> <span class="token function">git</span> clone <span class="token operator">&lt;</span>https://github.com/HewlettPackard/hpe-solutions-openshift.git<span class="token operator">&gt;</span>

</code></pre></div></li> <li><p>After the hpe-solutions-openshift repository is downloaded, navigate to the path <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/playbooks</em>. The scripts within this directory assists in configuring the prerequisites for the environment. The details of the scripts are as follows:</p> <ul><li><p>python_env.sh – this script installs Python 3.</p></li> <li><p>ansible_env.sh – this script creates a Python 3 virtual environment and installs Ansible within the virtual environment.</p></li> <li><p>download_oneview_packages.sh – this script installs the prerequisite modules such as HPE oneview-ansible, HPE oneview-python and VMware pyVmomi within the virtual environment.</p></li></ul></li> <li><p>Steps to configure the prerequisite environment are as follows:</p> <ul><li><p>Change the directory to /etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/playbooks</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token builtin class-name">cd</span> /etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/playbooks
</code></pre></div></li> <li><p>Execute the following command to setup prerequisite Python environment.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">sudo</span> <span class="token function">sh</span> python_env.sh
</code></pre></div></li> <li><p>Execute the following command to enable Python3.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> scl <span class="token builtin class-name">enable</span> rh-python36 <span class="token function">bash</span>
</code></pre></div></li> <li><p>Execute the following command to configure the Ansible environment.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">sudo</span> <span class="token function">sh</span> ansible_env.sh
</code></pre></div></li> <li><p>Execute the following command to download the HPE OneView packages.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">sudo</span> <span class="token function">sh</span> download_oneview_packages.sh
</code></pre></div></li> <li><p>Enable the virtual environment with the following command.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token builtin class-name">source</span> <span class="token punctuation">..</span>/ocp_venv/bin/activate
</code></pre></div></li> <li><p>Execute the following command to set the environment variables.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token builtin class-name">export</span> <span class="token assign-left variable">ANSIBLE_LIBRARY</span><span class="token operator">=</span>/etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/library/oneview-ansible/library

<span class="token operator">&gt;</span> <span class="token builtin class-name">export</span> <span class="token assign-left variable">ANSIBLE_MODULE_UTILS</span><span class="token operator">=</span>/etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/library/oneview-ansible/library/module_utils
</code></pre></div></li></ul></li></ol> <h3 id="openshift-inventory-file"><a href="#openshift-inventory-file" class="header-anchor">#</a> OpenShift inventory file</h3> <p>The files that are cloned from the GitHub site include a sample inventory file. The installation user should review this file (located on the installer VM at <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/infrastructure/hosts</em>) and ensure that the information within the file accurately reflects the information in their environment.</p> <p>Use an editor such as vim or nano to edit the inventory file.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">vim</span> /etc/ansible/hpe-solutions-openshift/synergy/scalable/infrastructure/hosts
</code></pre></div><p><strong>NOTE</strong></p> <p>The values provided in the variable files, inventory files, figures, and tables in this document are intended to be used for reference purposes. It is expected that the installation user updates them to suit the local environment.</p> <h3 id="ansible-vault"><a href="#ansible-vault" class="header-anchor">#</a> Ansible Vault</h3> <p>A preconfigured Ansible vault file (<em>secret.yml</em>) is provided as part of this solution which consists of sensitive information to support the host and virtual machine deployment.</p> <p>Run the following command on the installer VM to edit the vault to match the installation environment.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> ansible-vault edit /etc/ansible/hpe-solutions-openshift/synergy/scalable/infrastructure/secret.yml
</code></pre></div><p><strong>NOTE</strong></p> <p>The default password for the Ansible vault file is <em>changeme.</em></p> <h2 id="kubernetes-manifests-and-ignition-files"><a href="#kubernetes-manifests-and-ignition-files" class="header-anchor">#</a> Kubernetes manifests and ignition files</h2> <p>Manifests and ignition files define the master node and worker node configurations and are key components of the Red Hat OpenShift Container Platform 4 installation.</p> <p>Before creating the manifest files and ignition files, it is necessary to download the Red Hat OpenShift 4 packages. Execute the following command on the installer VM to download the required packages.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token builtin class-name">cd</span> /etc/ansible/hpe-solutions-openshift/synergy/scalable/installer

<span class="token operator">&gt;</span> ansible-playbook playbooks/download_ocp_package.yml
</code></pre></div><p>The OpenShift packages downloaded after executing the <em>download_ocp_package.yml</em> playbook can be found on the installer VM at <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/library/openshift_components</em>. To execute any OpenShift related adhoc commands, it is advised to execute them from within this folder.</p> <p>To create the manifest files and the ignition files, edit the <em>install-config.yaml</em> file provided in the directory <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/ignitions</em> to include the following details:</p> <ul><li><p>baseDomain - Base domain of the DNS which hosts Red Hat OpenShift Container Platform.</p></li> <li><p>name – Name for the OpenShift cluster. This is same as the new domain created in DNS.</p></li> <li><p>replicas – Update this field to reflect the corresponding number of master or worker instances required for the OpenShift cluster as per the installation environment requirements. It is recommended to have a minimum of 3 master nodes and 2 worker nodes per OpenShift cluster.</p></li> <li><p>clusterNetworks – This field is pre-populated by Red Hat. Update this field only if a custom cluster network is to be used.</p></li> <li><p>pullSecret – Update this field with the pull secret for the Red Hat account. Login to Red Hat account <a href="https://cloud.redhat.com/openshift/install/metal/user-provisioned" target="_blank" rel="noopener noreferrer">https://cloud.redhat.com/openshift/install/metal/user-provisioned<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> and retrieve the pull secret.</p></li> <li><p>sshKey – Update this field with the sshKey of the installer VM. Generate the SSH key with following command.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> ssh-keygen
</code></pre></div><p>An example <em>install-config.yaml</em> file appears below. Update the fields to suit the installation environment.</p> <div class="language-yaml extra-class"><pre class="language-yaml"><code>
<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1

<span class="token key atrule">baseDomain</span><span class="token punctuation">:</span> <span class="token important">&amp;lt</span>;name of the base domain<span class="token important">&amp;gt</span>;

<span class="token key atrule">compute</span><span class="token punctuation">:</span>

<span class="token punctuation">-</span> <span class="token key atrule">hyperthreading</span><span class="token punctuation">:</span> Enabled

<span class="token key atrule">name</span><span class="token punctuation">:</span> worker

<span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">2</span>

<span class="token key atrule">controlPlane</span><span class="token punctuation">:</span>

<span class="token key atrule">hyperthreading</span><span class="token punctuation">:</span> Enabled

<span class="token key atrule">name</span><span class="token punctuation">:</span> master

<span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>

<span class="token key atrule">metadata</span><span class="token punctuation">:</span>

<span class="token key atrule">name</span><span class="token punctuation">:</span> <span class="token important">&amp;lt</span>;name of the cluster<span class="token punctuation">,</span> same as the new domain under the base domain created<span class="token important">&amp;gt</span>;

<span class="token key atrule">networking</span><span class="token punctuation">:</span>

<span class="token key atrule">clusterNetworks</span><span class="token punctuation">:</span>

<span class="token punctuation">-</span> <span class="token key atrule">cidr</span><span class="token punctuation">:</span> 12.128.0.0/14

<span class="token key atrule">hostPrefix</span><span class="token punctuation">:</span> <span class="token number">23</span>

<span class="token key atrule">networkType</span><span class="token punctuation">:</span> OpenShiftSDN

<span class="token key atrule">serviceNetwork</span><span class="token punctuation">:</span>

<span class="token punctuation">-</span> 172.30.0.0/16

<span class="token key atrule">platform</span><span class="token punctuation">:</span>

<span class="token key atrule">none</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>

<span class="token key atrule">pullSecret</span><span class="token punctuation">:</span> ‘pull secret provided as per the Red Hat account’

<span class="token key atrule">sshKey</span><span class="token punctuation">:</span> ‘ ssh key of the installer VM ’

</code></pre></div><p>Execute the following command on the installer VM to create the manifest files and the ignition files required to install Red Hat OpenShift.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token builtin class-name">cd</span> /etc/ansible/hpe-solutions-openshift/synergy/scalable/installer
<span class="token operator">&gt;</span> ansible-playbook playbooks/create_manifest_ignitions.yml
</code></pre></div></li></ul> <p>The ignition files are generated on the installer VM within the folder <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/ignitions</em>.</p> <p><strong>NOTE</strong></p> <p>The ignition files have a time-out period of 24 hours and it is critical that the clusters are created within 24 hours of generating the ignition files. If 24 hours is passed, then regenerate the ignition files again by clearing up the files from the directory where the ignition files were saved.</p> <h2 id="pxe-server"><a href="#pxe-server" class="header-anchor">#</a> PXE Server</h2> <p>In this solution, a PXE Server is used for OS deployment and is configured on CentOS (version: CentOS Linux release 7.6.1810 (Core)). The PXE server uses the FTP service for file distribution but can be altered to support HTTP or NFS.</p> <p>This section highlights the steps to configure a PXE server:</p> <ol><li><p>Login to the CentOS server to be configured as a PXE server as a user that can run commands as root via sudo.</p></li> <li><p>Install packages such as DHCP, TFTP-server, vSFTPD (FTP server) and xinetd using the following command.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">sudo</span> yum <span class="token function">install</span> dhcp tftp tftp-server syslinux vsftpd xinetd
</code></pre></div></li> <li><p>Update the DHCP configuration file at <em>/etc/dhcp/dhcpd.conf</em> with the MAC addresses, IP addresses, DNS, and routing details of the installation environment. Domain search is optional. A sample DHCP configuration file is shown as follows.</p> <div class="language- extra-class"><pre class="language-text"><code>ddns-update-style interim;
ignore client-updates;
authoritative;
allow booting;
allow bootp;

# internal subnet for my DHCP Server
subnet 20.0.x.x netmask 255.0.0.0 {
range 20.0.x.x 20.0.x.x;
deny unknown-clients;
option domain-name-servers 20.x.x.x;
option domain-name &quot;twentynet.local&quot;;
option routers 20.x.x.x;
option broadcast-address 20.255.255.255;
default-lease-time 600;
max-lease-time 7200;
next-server 20.x.x.x;
filename &quot;pxelinux.0&quot;;
}

#######################################
host bootstrap {
hardware ethernet 00:50:56:xx:98:df;
fixed-address 20.0.x.x;
}
host master01 {
hardware ethernet 00:50:56:95:xx:82;
fixed-address 20.0.x.x;
}
host worker01 {
hardware ethernet 00:50:56:xx:ab:82;
fixed-address 20.0.x.x;
}
</code></pre></div></li> <li><p>Trivial File Transfer Protocol (TFTP) is used to transfer files from data server to clients without any kind of authentication. TFTP is used for ignition file loading in PXE based environments. To configure the TFTP server, edit the configuration file <em>/etc/xinetd.d/tftp</em>. Change the parameter ‘disable = yes’ to ‘disable = no’ and leave the other parameters as is. To edit the <em>/etc/xinetd.d/tftp</em> file, execute the following command.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">sudo</span> <span class="token function">vi</span>  /etc/xinetd.d/tftp
</code></pre></div><p>The TFTP configuration file is shown below.</p> <div class="language- extra-class"><pre class="language-text"><code>service tftp
   {

        socket_type = dgram
        protocol = udp
        wait = yes
        user = root
        server = /usr/sbin/in.tftpd
        server_args = -s /var/lib/tftpboot
        disable = no
        per_source = 11
        cps = 100 2
        flags = IPv4
    }
</code></pre></div><p>Network boot related files must be placed in the tftp root directory <em>/var/lib/tftpboot</em>. Run the following commands to copy the required network boot files to <em>/var/lib/tftpboot/</em>.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">sudo</span> <span class="token function">cp</span> –v /usr/share/syslinux/pxelinux.0 /var/lib/tftpboot

<span class="token operator">&gt;</span> <span class="token function">sudo</span> <span class="token function">cp</span> –v /usr/share/syslinux/menu.c32 /var/lib/tftpboot

<span class="token operator">&gt;</span> <span class="token function">sudo</span> <span class="token function">cp</span> –v /usr/share/syslinux/memdisk /var/lib/tftpboot

<span class="token operator">&gt;</span> <span class="token function">sudo</span> <span class="token function">cp</span> –v /usr/share/syslinux/mboot.c32 /var/lib/tftpboot

<span class="token operator">&gt;</span> <span class="token function">sudo</span> <span class="token function">cp</span> –v /usr/share/syslinux/chain.c32 /var/lib/tftpboot

<span class="token operator">&gt;</span> <span class="token function">sudo</span> <span class="token function">mkdir</span> /var/lib/tftpboot/pxelinux.cfg

<span class="token operator">&gt;</span> <span class="token function">sudo</span> <span class="token function">mkdir</span> /var/lib/tftpboot/networkboot
</code></pre></div></li> <li><p>Copy the RHCOS 4 and RHEL 7.6 ISO files to the PXE server. Mount it to the <em>/mnt/</em> directory and then copy the contents of the ISO to the local FTP server using the following commands.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">sudo</span> <span class="token function">mount</span> –o loop <span class="token operator">&amp;</span>lt<span class="token punctuation">;</span>OS <span class="token function">file</span> name<span class="token operator">&amp;</span>gt<span class="token punctuation">;</span> /mnt/

<span class="token operator">&gt;</span> <span class="token builtin class-name">cd</span> /mnt/

<span class="token operator">&gt;</span> <span class="token function">sudo</span> <span class="token function">cp</span> –av * /var/ftp/pub/
</code></pre></div></li> <li><p>Copy the kernel file (vmlinuz) and initrd file from <em>/mnt</em> to <em>/var/lib/tftpboot/networkboot/</em> using the following commands.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">sudo</span> <span class="token function">cp</span> /mnt/images/pxeboot/vmlinuz /var/lib/tftpboot/networkboot/

<span class="token operator">&gt;</span> <span class="token function">sudo</span> <span class="token function">cp</span> /mnt/images/pxeboot/initrd.img /var/lib/tftpboot/networkboot
</code></pre></div></li> <li><p>Unmount the ISO files using the following command.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">sudo</span> unmount /mnt/
</code></pre></div></li> <li><p>For RHEL nodes, create and utilize a new kickstart file under the folder <em>/var/ftp/pub</em> with the name “<em>rhel7.cfg</em>” using the following command.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">sudo</span> <span class="token function">vi</span> /var/ftp/pub/rhel7.cfg
</code></pre></div><p>An example kickstart file is shown below. The installation user should create a kickstart file to meet the requirements of their installation environment.</p> <div class="language- extra-class"><pre class="language-text"><code>firewall --disabled
# Install OS instead of upgrade
install
# Use FTP installation media
url --url=&quot;ftp://&amp;lt;FTP_server_IP_address&amp;gt;/pub/rhel76/&quot;
# Root password
# root password can be plaintext as shown below
# rootpw –plaintext &amp;lt;password&amp;gt;
# root password is encrypted using the command “openssl passwd -1 &amp;lt;password&amp;gt;” and resultant output is provided for rootpw as shown below
rootpw --iscrypted $6$uiq8l/7xEWsYXhrvaEgan4N21yhLa8K.U7UA12Th3PD11GOXvEcI40gp
# System authorization information
auth useshadow passalgo=sha512
# Use graphical install
graphical
firstboot disable
# System keyboard, timezone, language
keyboard us
timezone Europe/Amsterdam
lang en_US
# SELinux configuration
selinux disabled
# Installation logging level
logging level=info
# System bootloader configuration
bootloader location=mbr
clearpart --all --initlabel
part swap --asprimary --fstype=&quot;swap&quot; --size=1
part /boot --fstype xfs --size=300
part pv.01 --size=1 --grow
volgroup root_vg01 pv.01
logvol / --fstype xfs --name=lv_01 --vgname=root_vg01 --size=1 --grow
%packages
@^minimal
@core
%end
%addon com_redhat_kdump --disable --reserve-mb='auto'
%end
</code></pre></div></li> <li><p>Create a PXE menu:</p> <ul><li><p>Create a PXE menu file at the location <em>/var/lib/tftpboot/pxelinux.cfg/default</em> using the command.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">sudo</span> <span class="token function">vi</span> /var/lib/tftpboot/pxelinux.cfg/default
</code></pre></div></li> <li><p>For each of the OS boot options, provide the following details:</p> <ul><li><p>MENU LABEL – Custom name of the respective menu label.</p></li> <li><p>KERNEL – Kernel details of the operating system.</p></li> <li><p>APPEND - Path of bootloader file along with path of cfg or ignition files (in case of RHCOS) or configuration file (in case of RHEL).</p></li></ul></li></ul> <p>A sample PXE menu is shown below.</p> <div class="language- extra-class"><pre class="language-text"><code>default menu.c32

prompt 0

timeout 30

MENU TITLE LinuxTechi.com PXE Menu

LABEL rhel76

MENU LABEL RHEL76-Buedata

KERNEL /rhel76/vmlinuz

APPEND initrd=/rhel76/initrd.img inst.repo=ftp://&amp;lt;FTP_server_IP_address&amp;gt;/pub/rhel76 ks=ftp://&amp;lt;FTP_server_IP_address&amp;gt;/pub/rhel76-hcp.cfg

LABEL rhcos-bootstrap

MENU LABEL Install RHCOS4.3 sec-Bootstrap

KERNEL /networkboot/rhcos-4.3.0-x86_64-installer-kernel

APPEND ip=dhcp rd.neednet=1 initrd=/networkboot/rhcos-4.3.0-x86_64-installer-initramfs.img console=tty0 console=ttyS0 coreos.inst=yes coreos.inst.install_dev=sda coreos.inst.image_url= ftp://&amp;lt;FTP_server_IP_address&amp;gt;/pub/rhcos-4.3.0-x86_64-metal-bios.raw.gz coreos.inst.ignition_url= ftp://&amp;lt;FTP_server_IP_address&amp;gt;/pub/sec/bootstrap.ign

LABEL rhcos-master

MENU LABEL Install RHCOS4.2 sec-Master

KERNEL /networkboot/rhcos-4.3.0-x86_64-installer-kernel

APPEND ip=dhcp rd.neednet=1 initrd=/networkboot/rhcos-4.3.0-x86_64-installer-initramfs.img console=tty0 console=ttyS0 coreos.inst=yes coreos.inst.install_dev=sda coreos.inst.image_url= ftp://&amp;lt;FTP_server_IP_address&amp;gt;/pub/rhcos-4.3.0-x86_64-metal-bios.raw.gz coreos.inst.ignition_url=ftp://&amp;lt;FTP_server_IP_address&amp;gt;/pub/sec/master.ign

LABEL rhcos-worker

MENU LABEL Install RHCOS4.2 sec-Worker

KERNEL /networkboot/rhcos-4.3.0-x86_64-installer-kernel

APPEND ip=dhcp rd.neednet=1 initrd=/networkboot/rhcos-4.3.0-x86_64-installer-initramfs.img console=tty0 console=ttyS0 coreos.inst=yes coreos.inst.install_dev=sda coreos.inst.image_url= ftp://&amp;lt;FTP_server_IP_address&amp;gt;/pub/rhcos-4.3.0-x86_64-metal-bios.raw.gz coreos.inst.ignition_url=ftp://&amp;lt;FTP_server_IP_address&amp;gt;/pub/sec/worker.ign
</code></pre></div></li> <li><p>Start and enable xinetd, dhcpd and vsftpd using the following commands.</p> <div class="language-bash extra-class"><pre class="language-bash"><code>    <span class="token operator">&gt;</span> <span class="token function">sudo</span> systemctl start xinetd

    <span class="token operator">&gt;</span> <span class="token function">sudo</span> systemctl <span class="token builtin class-name">enable</span> xinetd

    <span class="token operator">&gt;</span> <span class="token function">sudo</span> systemctl start dhcpd.service

    <span class="token operator">&gt;</span> <span class="token function">sudo</span> systemctl <span class="token builtin class-name">enable</span> dhcpd.service

    <span class="token operator">&gt;</span> <span class="token function">sudo</span> systemctl start vsftpd

    <span class="token operator">&gt;</span> <span class="token function">sudo</span> systemctl <span class="token builtin class-name">enable</span> vsftpd
</code></pre></div></li> <li><p>Configure SELinux for FTP.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">sudo</span> setsebool –P allow_ftpd_full_access <span class="token number">1</span>
</code></pre></div></li> <li><p>Open ports in the firewall using the following firewall-cmd commands.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">sudo</span> firewall-cmd --add-service-ftp --permanent

<span class="token operator">&gt;</span>sudo firewall-cmd --add-service-dhcp --permanent

<span class="token operator">&gt;</span>sudo firewall-cmd –reload
</code></pre></div></li></ol> <p><strong>NOTE</strong></p> <p>It is crucial to generate ignition files, copy them to the TFTP server, and update the path in the PXE default file. For more information about generating the ignition files, refer to the section [Kubernetes manifests and ignition files] in this document.</p> <h2 id="load-balancer"><a href="#load-balancer" class="header-anchor">#</a> Load balancer</h2> <p>Red Hat OpenShift Container Platform 4 uses an external load balancer to communicate from outside the cluster with services running inside the cluster. This section assumes that there is a load balancer available within the deployment environment and is available for use. This solution was developed using <strong>HA Proxy</strong>, an open source solution with one (1) virtual machine for load balancing functionality. This section covers its configuration. In a production environment, Hewlett Packard Enterprise recommends the use of enterprise load balancing such as F5 Networks Big-IP and its associated products.</p> <p>The following entries are made in the haproxy.cfg file.</p> <div class="language- extra-class"><pre class="language-text"><code>Sample haproxy.cfg file
#---------------------------------------------------------------------
# static backend for serving up images, stylesheets and such
#---------------------------------------------------------------------
#backend static
# balance roundrobin
# server static 127.0.0.1:4331 check
#---------------------------------------------------------------------
# round robin balancing between the various backends
#---------------------------------------------------------------------
backend ocp4-kubernetes-api-server
    mode tcp
    balance source
    server bootstrap bootstrap.ocp.pxelocal.local:6443 check
    server master01 master01.ocp.pxelocal.local:6443 check
    server master02 master02.ocp.pxelocal.local:6443 check
    server master03 master03.ocp.pxelocal.local:6443 check

backend ocp4-machine-config-server
    balance source
    mode tcp
    server bootstrap bootstrap.ocp.pxelocal.local:22623 check
    server master01 master01.ocp.pxelocal.local:22623 check
    server master02 master02.ocp.pxelocal.local:22623 check
    server master03 master03.ocp.pxelocal.local:22623 check

backend ocp4-router-http
    balance source
    mode tcp
    server worker03 worker03.ocp.pxelocal.local:80 check
    server worker04 worker04.ocp.pxelocal.local:80 check
    server worker01 worker01.ocp.pxelocal.local:80 check
    server worker02 worker02.ocp.pxelocal.local:80 check

# if creating a cluster with only master nodes to begin with and later adding the worker nodes, master nodes should be added in this section instead of worker nodes. Once all the worker nodes are added into the cluster, this configuration needs to be updated with the worker nodes.
    # server master01 master01.ocp.pxelocal.local:80 check
    # server master02 master02.ocp.pxelocal.local:80 check
    # server master03 master03.ocp.pxelocal.local:80 check

    backend ocp4-router-https
        balance source
        mode tcp
        server worker03 worker03.ocp.pxelocal.local:443 check
        server worker04 worker04.ocp.pxelocal.local:443 check
        server worker01 worker01.ocp.pxelocal.local:443 check
        server worker01 worker02.ocp.pxelocal.local:443 check

# if creating a cluster with only master nodes to begin with and later adding the worker nodes, master nodes should be added in this section instead of worker nodes. Once all the worker nodes are added into the cluster, this configuration needs to be updated with the worker nodes.
    # server master01 master01.ocp.pxelocal.local:443 check
    # server master02 master02.ocp.pxelocal.local:443 check
    # server master03 master03.ocp.pxelocal.local:443 check
</code></pre></div><p><strong>NOTE</strong></p> <p>The load balancer configuration should contain values that are aligned to the installation environment.</p> <h2 id="user-provisioned-dns-requirements"><a href="#user-provisioned-dns-requirements" class="header-anchor">#</a> User-provisioned DNS requirements</h2> <p>This section covers the host entries that need to be made in the base domain to enable installation of Red Hat OpenShift Container Platform 4.</p> <p>Red Hat OpenShift Container Platform 4 uses three types of DNS records (A, CNAME, and SRV). The host names and their types are described in Table 7.</p> <p><strong>Table 7.</strong> DNS entries for Red Hat OpenShift Container Platform</p> <table><thead><tr><th>Hosts</th> <th>DNS Record Types</th></tr></thead> <tbody><tr><td>master_nodes</td> <td>A</td></tr> <tr><td>worker_nodes</td> <td>A</td></tr> <tr><td>bootstrap_nodes</td> <td>A</td></tr> <tr><td>installer VM</td> <td>A</td></tr> <tr><td>*, api, api-int, haproxy</td> <td>A</td></tr> <tr><td>etcd</td> <td>CNAME, SRV</td></tr></tbody></table> <p>To add the appropriate records, follow these steps:</p> <ol><li><p>‘A’ type resource record [ Host (A) ]. An A record specifies an IPv4 address. Example entries are shown in Table 8. Ensure that these entries are created for all the nodes in the installation environment.</p> <p><strong>Table 8.</strong> ‘A’ type DNS entry</p> <table><thead><tr><th>Host (A)</th> <th>IP Address</th> <th>Host name</th></tr></thead> <tbody><tr><td>master_nodes</td> <td>master_ip</td> <td>master_name.cluster_name.baseDomain</td></tr> <tr><td>worker_nodes</td> <td>worker_ip</td> <td>worker_name.cluster_name.baseDomain</td></tr> <tr><td>bootstrap_nodes</td> <td>bootstrap_ip</td> <td>bootstrap_name.cluster_name.baseDomain</td></tr> <tr><td>installer VM</td> <td>installer_ip</td> <td>installer_name.cluster_name.baseDomain</td></tr> <tr><td>*</td> <td>haproxy_ip</td> <td>*.apps.cluster_name.baseDomain</td></tr> <tr><td>api</td> <td>haproxy_ip</td> <td>api.cluster_name.baseDomain</td></tr> <tr><td>api-int</td> <td>haproxy_ip</td> <td>api-int.cluster_name.baseDomain</td></tr> <tr><td>haproxy</td> <td>haproxy_ip</td> <td>haproxy_name.cluster_name.baseDomain</td></tr></tbody></table></li> <li><p>Create a CNAME resource record [ Alias (CNAME) ] in DNS. Example entries are shown in Table 9. Ensure that these entries are created for all of the master nodes.</p> <p><strong>Table 9.</strong> DNS entries for Red Hat OpenShift Container Platform</p> <table><thead><tr><th>Host (CNAME)</th> <th>Target host name</th></tr></thead> <tbody><tr><td>etcd-0</td> <td>*master01_name.cluster_name.baseDomain</td></tr> <tr><td>etcd-1</td> <td>*master02_name.cluster_name.baseDomain</td></tr> <tr><td>etcd-2</td> <td>*master03_name.cluster_name.baseDomain</td></tr></tbody></table> <p><strong>NOTE</strong></p> <p>Replace the italicized components in the examples above with the actual values that align to the installation environment.</p></li> <li><p>For each master node, Red Hat OpenShift Container Platform also requires a Service Location (SRV) DNS record for the etcd server on that machine with priority 0, weight 10, and port 2380. The SRV record is used to identify computers that host specific services. Figure 5 shows the creation of an SRV record.</p> <p><img src="/hpe-solutions-openshift/assets/img/figure5.11040a94.png" alt="Creating an SRV record"></p> <p>Figure 5. Creating an SRV record</p></li></ol> <h2 id="bootstrap-node"><a href="#bootstrap-node" class="header-anchor">#</a> Bootstrap node</h2> <p>A temporary bootstrap node is required for OpenShift cluster creation. This section assumes that a VMware vSphere host is present within the deployment environment and is associated with a VMware vCenter server. The host should be configured with appropriate storage and networking configurations.</p> <h3 id="playbooks-for-creating-the-bootstrap-node"><a href="#playbooks-for-creating-the-bootstrap-node" class="header-anchor">#</a> Playbooks for creating the bootstrap node</h3> <ol><li><p><strong>inputs.yml</strong>: This file contains input variables to create the bootstrap VM. Some of the variables pertaining to the VM configuration are provided with default values as per the Red Hat guidelines. It is expected that the installation user updates the values to suit their installation environment.</p> <ul><li><p><strong>datacenter_name</strong>: name of the VMware data center.</p></li> <li><p><strong>cluster_name</strong>: name of the VMware cluster.</p></li> <li><p><strong>datastore_name</strong>: name of the VMware datastore.</p></li> <li><p><strong>network_name:</strong> name of the network associated with the vSphere host**.**</p></li> <li><p><strong>bootstrap_disk:</strong> disk size for the bootstrap node.</p></li> <li><p><strong>bootstrap_cpu:</strong> number of vCPUs for the bootstrap node.</p></li> <li><p><strong>bootstrap_name:</strong> custom name of the bootstrap node.</p></li></ul></li> <li><p><strong>secret.yml</strong>: This is an Ansible vault file that contains sensitive information such as the VMware vCenter server IP address and credentials.</p></li> <li><p><strong>playbooks/deploy_vm.yml</strong>: This playbook is used to create the bootstrap VM.</p></li> <li><p><strong>roles/deploy_vm.yml</strong>: This is the Ansible role file that is required to create the bootstrap VM. Each role is associated with a set of tasks to accomplish the expected output and they are present in the tasks directory within the role.</p></li></ol> <p>Follow these steps to create the bootstrap node:</p> <ol><li><p>Login to the installer VM.</p></li> <li><p>Change the directory using the following command.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token builtin class-name">cd</span> /etc/ansible/hpe-solutions-openshift/synergy/scalable/infrastructure
</code></pre></div></li> <li><p>Update the <em>secret.yml</em> to provide the details of the VMware vCenter server using the following command. A sample input is provided, and it is expected that the installation user updates the configuration to suit the deployment environment.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> ansible-vault edit secret.yml
</code></pre></div><div class="language- extra-class"><pre class="language-text"><code># vcenter hostname/ip address and credentials
vcenter_hostname: &lt;vcenter hostname&gt;;
vcenter_username: &lt;vcenter username&gt;;
vcenter_password: &lt;vcenter password&gt;;
</code></pre></div></li> <li><p>Update the <em>input.yml</em> file with the data center, cluster, and datastore information that will be used within the VMware vCenter server. This file provides default configuration information for the bootstrap node, if required. The configuration can be updated to suit the environment needs.</p> <div class="language- extra-class"><pre class="language-text"><code># Variables for creating the bootstrap VM, as per the vSphere host configuration within the vCenter
datacenter_name: &lt;name of data center within vcenter&gt;;
cluster_name: &amp;lt;name of cluster within vcenter&gt;;
datastore_name: &amp;lt;name of datastore within vcenter&gt;;
network_name: &lt;name of the network within vcenter&gt;;

# Default values for creating the bootstrap VM
bootstrap_disk: 150
bootstrap_cpu: 4
bootstrap_memory: 16400
bootstrap_name: Bootstrap
</code></pre></div></li> <li><p>After the <em>input.yml</em> and <em>secret.yml</em> files are updated with appropriate values, execute the playbook with the following command to create the bootstrap VM.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> ansible-playbook –i hosts playbooks/deploy_vm.yml –ask-vault-pass
</code></pre></div></li></ol> <p><strong>NOTE</strong></p> <p>It is essential that all the nodes within the deployment environment are synchronized for time using an NTP server. Failure to do so will result in an installation failure due to mismatches in certificates or other files with time dependencies.</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/hpe-solutions-openshift/Solution components/Solution components.html" class="prev">
        Solution components
      </a></span> <span class="next"><a href="/hpe-solutions-openshift/Physical environment configuration/Physical environment configuration.html">
        Physical environment configuration
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/hpe-solutions-openshift/assets/js/app.adad66b3.js" defer></script><script src="/hpe-solutions-openshift/assets/js/2.ac0f675e.js" defer></script><script src="/hpe-solutions-openshift/assets/js/8.97285f82.js" defer></script>
  </body>
</html>
