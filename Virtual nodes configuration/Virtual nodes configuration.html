<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Virtual nodes configuration | RED HAT OPENSHIFT CONTAINER PLATFORM 4 ON HPE SYNERGY</title>
    <meta name="description" content="">
    <meta name="generator" content="VuePress 1.4.0">
    
    
    <link rel="preload" href="/hpe-solutions-openshift/assets/css/0.styles.03110986.css" as="style"><link rel="preload" href="/hpe-solutions-openshift/assets/js/app.adad66b3.js" as="script"><link rel="preload" href="/hpe-solutions-openshift/assets/js/2.ac0f675e.js" as="script"><link rel="preload" href="/hpe-solutions-openshift/assets/js/19.e7a9cad7.js" as="script"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/10.bbe6a8b1.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/11.c231a443.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/12.381ec328.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/13.bc9c4fc5.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/14.6c2a59eb.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/15.10105a64.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/16.de305cb4.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/17.c566bc9d.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/18.1a3abf91.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/20.826352b1.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/21.e9b44a36.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/3.f3b6057a.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/4.d098f71d.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/5.c9b2efa3.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/6.e3886d8a.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/7.8d533ade.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/8.97285f82.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/9.cd9b026d.js">
    <link rel="stylesheet" href="/hpe-solutions-openshift/assets/css/0.styles.03110986.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/hpe-solutions-openshift/" class="home-link router-link-active"><!----> <span class="site-name">RED HAT OPENSHIFT CONTAINER PLATFORM 4 ON HPE SYNERGY</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/hpe-solutions-openshift/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="http://www.hpe.com/info/ra" target="_blank" rel="noopener noreferrer" class="nav-link external">
  RA Library
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/hpe-solutions-openshift/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="http://www.hpe.com/info/ra" target="_blank" rel="noopener noreferrer" class="nav-link external">
  RA Library
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/hpe-solutions-openshift/" class="sidebar-link">Introduction</a></li><li><a href="/hpe-solutions-openshift/Solution overview/Solution overview.html" class="sidebar-link">Solution overview</a></li><li><a href="/hpe-solutions-openshift/Solution components/Solution components.html" class="sidebar-link">Solution components</a></li><li><a href="/hpe-solutions-openshift/Preparing the execution environment/Preparing the execution environment.html" class="sidebar-link">Preparing the execution environment</a></li><li><a href="/hpe-solutions-openshift/Physical environment configuration/Physical environment configuration.html" class="sidebar-link">Physical environment configuration</a></li><li><a href="/hpe-solutions-openshift/Physical node configuration/Physical node configuration.html" class="sidebar-link">Physical node configuration</a></li><li><a href="/hpe-solutions-openshift/Virtual nodes configuration/Virtual nodes configuration.html" class="active sidebar-link">Virtual nodes configuration</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Virtual nodes configuration/Virtual nodes configuration.html#deploying-vsphere-hosts" class="sidebar-link">Deploying vSphere hosts</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Virtual nodes configuration/Virtual nodes configuration.html#hpe-oneview-for-vmware-vcenter" class="sidebar-link">HPE OneView for VMware vCenter</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Virtual nodes configuration/Virtual nodes configuration.html#creating-the-data-center-cluster-and-adding-hosts-in-vmware-vcenter" class="sidebar-link">Creating the Data center, Cluster and adding Hosts in VMware vCenter</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Virtual nodes configuration/Virtual nodes configuration.html#creating-a-datastore-in-vcenter" class="sidebar-link">Creating a Datastore in vCenter</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Virtual nodes configuration/Virtual nodes configuration.html#red-hat-openshift-container-platform-sizing" class="sidebar-link">Red Hat OpenShift Container Platform sizing</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Virtual nodes configuration/Virtual nodes configuration.html#deploying-virtual-master-nodes" class="sidebar-link">Deploying virtual master nodes</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Virtual nodes configuration/Virtual nodes configuration.html#deploying-virtual-worker-nodes" class="sidebar-link">Deploying virtual worker nodes</a></li></ul></li><li><a href="/hpe-solutions-openshift/Red Hat OpenShift Container Platform deployment/Red Hat OpenShift Container Platform deployment.html" class="sidebar-link">Red Hat OpenShift Container Platform deployment</a></li><li><a href="/hpe-solutions-openshift/Red Hat Local Storage Operator/Red Hat Local Storage Operator.html" class="sidebar-link">Red Hat Local Storage Operator</a></li><li><a href="/hpe-solutions-openshift/Securing RedHat OpenShift Container Platform using Sysdig Secure and Sysdig Monitor/Securing RedHat OpenShift Container Platform using Sysdig Secure and Sysdig Monitor.html" class="sidebar-link">Securing RedHat OpenShift Container Platform using Sysdig Secure and Sysdig Monitor</a></li><li><a href="/hpe-solutions-openshift/Physical worker node labeling in OpenShift/Physical worker node labeling in OpenShift.html" class="sidebar-link">Physical worker node labeling in OpenShift</a></li><li><a href="/hpe-solutions-openshift/OpenShift Operators/OpenShift Operators.html" class="sidebar-link">OpenShift Operators</a></li><li><a href="/hpe-solutions-openshift/Validating OpenShift Container Platform deployment/Validating OpenShift Container Platform deployment.html" class="sidebar-link">Validating OpenShift Container Platform deployment</a></li><li><a href="/hpe-solutions-openshift/Resources and additional links/Resources and additional links.html" class="sidebar-link">Resources and additional links</a></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="virtual-nodes-configuration"><a href="#virtual-nodes-configuration" class="header-anchor">#</a> Virtual nodes configuration</h1> <p>This section describes the process to deploy virtualization hosts for OpenShift. This section outlines the steps required to configure virtual machine master and worker nodes. At a high level, these steps are as follows:</p> <ol><li><p>Deploying the vSphere hosts</p></li> <li><p>Creating the data center, cluster, and adding hosts into the cluster</p></li> <li><p>Creating a datastore in vCenter</p></li> <li><p>Create virtual master nodes</p></li> <li><p>Deploying virtual worker nodes</p></li></ol> <p><strong>NOTE</strong></p> <p>Hewlett Packard Enterprise utilized a consistent method for deployment that would allow for mixed deployments of virtual and physical master and worker nodes and built this solution on bare metal using the Red Hat OpenShift Container Platform user-provisioned infrastructure. For more details on the bare metal provisioner, refer to <a href="https://cloud.redhat.com/openshift/install/metal/user-provisioned" target="_blank" rel="noopener noreferrer">https://cloud.redhat.com/openshift/install/metal/user-provisioned<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>. If the intent is to have an overall virtual environment, it is recommended the installation user utilizes Red Hat’s virtual provisioning methods found at <a href="https://docs.openshift.com/container-platform/4.3/installing/installing_vsphere/installing-vsphere.html#installing-vsphere" target="_blank" rel="noopener noreferrer">https://docs.openshift.com/container-platform/4.3/installing/installing_vsphere/installing-vsphere.html#installing-vsphere<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h2 id="deploying-vsphere-hosts"><a href="#deploying-vsphere-hosts" class="header-anchor">#</a> Deploying vSphere hosts</h2> <p>Refer to the section <a href="/hpe-solutions-openshift/Physical environment configuration/Physical environment configuration.html#server-profiles">Server Profiles</a> in this document to create the server profile for the vSphere hosts.</p> <p>After the successful creation of the server profile, install the hypervisor. The following steps describes the process to install the hypervisor:</p> <ol><li><p>From the HPE OneView interface, navigate to Server Profiles and select ESXi-empty-volume Server Profile, Select <strong>Actions &gt; Launch Console.</strong></p></li> <li><p>From the Remote Console window, choose <strong>Virtual Drives -&gt; Image File CD-ROM/DVD</strong> from the <strong>iLO options</strong> menu bar.</p></li> <li><p>Navigate to the VMware ESXi 6.7 ISO file located on the installation system.
Select the ISO file and click <strong>Open</strong>.</p></li> <li><p>If the server is in the powered off state, power switch on the server by selecting <strong>Power Switch -&gt; Momentary Press.</strong></p></li> <li><p>During boot, press <strong>F11</strong> Boot Menu and select iLO Virtual USB 3: iLO Virtual CD-ROM.</p></li> <li><p>When the VMware ESXi installation media has finished loading, proceed through the VMware user prompts. For storage device, select the 40GiB OS volume created on the HPE Image Streamer during server profile creation and <strong>set the root password.</strong></p></li> <li><p>Wait until the vSphere installation is complete.</p></li> <li><p>After the installation is complete, press <strong>F2</strong> to enter the vSphere host configuration page and update the IP address, gateway, DNS, hostname of the host and enable SSH.</p></li> <li><p>After the host is reachable, proceed with the next section.</p></li></ol> <h2 id="hpe-oneview-for-vmware-vcenter"><a href="#hpe-oneview-for-vmware-vcenter" class="header-anchor">#</a> HPE OneView for VMware vCenter</h2> <p>HPE OneView for VMware vCenter is a single, integrated plug-in application for VMware vCenter management. This enables the vSphere administrator to quickly obtain context-aware information about HPE Servers and HPE Storage in their VMware vSphere data center directly from within vCenter. This application enables the vSphere administrator to easily manage physical servers and storage, datastores, and virtual machines. By providing the ability to clearly view and directly manage the HPE Infrastructure from within the vCenter console, the productivity of VMware administrator increases. This also enhances the ability to ensure quality of service.</p> <p>For more details, refer to the HPE documentation at <a href="https://h20392.www2.hpe.com/portal/swdepot/displayProductInfo.do?productNumber=HPVPR" target="_blank" rel="noopener noreferrer">https://h20392.www2.hpe.com/portal/swdepot/displayProductInfo.do?productNumber=HPVPR<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h2 id="creating-the-data-center-cluster-and-adding-hosts-in-vmware-vcenter"><a href="#creating-the-data-center-cluster-and-adding-hosts-in-vmware-vcenter" class="header-anchor">#</a> Creating the Data center, Cluster and adding Hosts in VMware vCenter</h2> <p>This section assumes a VMware vCenter server is available within the installation environment. A data center is a structure in VMware vCenter which contains clusters, hosts, and datastore. To begin with, a data center needs to be created, followed by the clusters and adding hosts into the clusters.</p> <p>To create a data center, a cluster enabled with vSAN and DRS and adding hosts, the installation user will need to edit the vault file and the variables YAML file. Using an editor, open the file <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/vsphere/vcenter/roles/prepare_vcenter/vars/main.yml</em> to provide the names for data center, clusters and vSphere hostnames. A sample input file is listed and as follows. Installation user should modify this file to suit the environment.</p> <p>In the Ansible vault file (<em>secret.yml</em>) found at <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/vsphere/vcenter</em>, provide the vCenter and the vSphere host credentials.</p> <div class="language-yaml extra-class"><pre class="language-yaml"><code><span class="token comment"># vsphere hosts credentials</span>

<span class="token key atrule">vsphere_username</span><span class="token punctuation">:</span> &lt;username<span class="token punctuation">&gt;</span>

<span class="token key atrule">vsphere_password</span><span class="token punctuation">:</span> &lt;password<span class="token punctuation">&gt;</span>

<span class="token comment"># vcenter hostname/ip address and credentials</span>

<span class="token key atrule">vcenter_hostname</span><span class="token punctuation">:</span> x.x.x.x

<span class="token key atrule">vcenter_username</span><span class="token punctuation">:</span> &lt;username<span class="token punctuation">&gt;</span>

<span class="token key atrule">vcenter_password</span><span class="token punctuation">:</span> &lt;password<span class="token punctuation">&gt;</span>
</code></pre></div><p><strong>NOTE</strong></p> <p>This section assumes all the virtualization hosts have a common username and password. If it does not have a common username and password, it is up to the installation user to add the virtualization hosts within the appropriate cluster.</p> <p>Variables for running the playbook can be found at <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/vsphere/vcenter/roles/prepare_vcenter/vars/main.yml</em>.</p> <div class="language-yaml extra-class"><pre class="language-yaml"><code><span class="token comment"># custom name for data center to be created.</span>

<span class="token key atrule">datacenter_name</span><span class="token punctuation">:</span> datacenter

<span class="token comment"># custom name of the compute clusters with the ESXi hosts for Management VMs.</span>

<span class="token key atrule">management_cluster_name</span><span class="token punctuation">:</span> management<span class="token punctuation">-</span>cluster

<span class="token comment"># hostname or IP address of the vsphere hosts utilized for the management nodes.</span>

<span class="token key atrule">vsphere_host_01</span><span class="token punctuation">:</span> 10.0.x.x

<span class="token key atrule">vsphere_host_02</span><span class="token punctuation">:</span> 10.0.x.x

<span class="token key atrule">vsphere_host_03</span><span class="token punctuation">:</span> 10.0.x.x
</code></pre></div><p>After the variable files are updated with the appropriate values, execute the following command within the installer VM to create the data center, clusters, and add hosts into respective clusters.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token builtin class-name">cd</span> /etc/ansible/hpe-solutions-openshift/synergy/scalable/vsphere/vcenter/

<span class="token operator">&gt;</span> ansible-playbook playbooks/prepare_vcenter.yml –ask-vault-pass
</code></pre></div><h2 id="creating-a-datastore-in-vcenter"><a href="#creating-a-datastore-in-vcenter" class="header-anchor">#</a> Creating a Datastore in vCenter</h2> <p>A datastore needs to be created in VMware vCenter from the volume carved out of HPE Storage SANs to store the VMs. The following are the steps to create a datastore in vCenter:</p> <ol><li><p>From the vSphere Web Client navigator, right-click the cluster, select <strong>Storage</strong> from the menu, and then select the <strong>New Datastore</strong>.</p></li> <li><p>From the Type page, select <strong>VMFS</strong> as the Datastore type and click <strong>Next</strong>.</p></li> <li><p>Enter the datastore name and if necessary, select the placement location for the datastore and click <strong>Next</strong>.</p></li> <li><p>Select the device to use for the datastore and click <strong>Next</strong>.</p></li> <li><p>From VMFS version page, select <strong>VMFS 6</strong> and click <strong>Next</strong>.</p></li> <li><p>Define the following configuration requirements for the datastore as per the installation environment and click <strong>Next</strong>.</p> <p>a.  Specify partition configuration</p> <p>b.  Datastore Size</p> <p>c.  Block Size</p> <p>d.  Space Reclamation Granularity</p> <p>e.  Space Reclamation Priority</p></li> <li><p>On the Ready to complete page, review the Datastore configuration and click <strong>Finish</strong>.</p></li></ol> <p><strong>NOTE</strong></p> <p>If you utilize virtual worker nodes, repeat this section to create a Datastore to store the worker virtual machines.</p> <h2 id="red-hat-openshift-container-platform-sizing"><a href="#red-hat-openshift-container-platform-sizing" class="header-anchor">#</a> Red Hat OpenShift Container Platform sizing</h2> <p>Red Hat OpenShift Container Platform sizing varies depending on the requirements of the organization and type of deployment. This section highlights the host sizing details recommended by Red Hat.</p> <table><thead><tr><th>Resource</th> <th>Bootstrap node</th> <th>Master node</th> <th>Worker node</th></tr></thead> <tbody><tr><td>CPU</td> <td>4</td> <td>4</td> <td>4</td></tr> <tr><td>Memory</td> <td>16GB</td> <td>16GB</td> <td>16GB</td></tr> <tr><td>Disk storage</td> <td>120GB</td> <td>120GB</td> <td>120GB</td></tr> <tr><td>Disk storage</td> <td>120GB</td> <td>120GB</td> <td>120GB</td></tr></tbody></table> <p>Disk partitions on each of the nodes are as follows.</p> <ul><li><p>/var – 40GB</p></li> <li><p>/usr/local/bin – 1GB</p></li> <li><p>Temporary directory – 1GB</p></li></ul> <p><strong>NOTE</strong></p> <p>Sizing for worker nodes is ultimately dependent on the container workloads and their CPU, memory, and disk requirements.</p> <p>For more information about Red Hat OpenShift Container Platform sizing, refer to the Red Hat OpenShift Container Platform 4 product documentation at <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.3/html/scalability_and_performance/index" target="_blank" rel="noopener noreferrer">https://access.redhat.com/documentation/en-us/openshift_container_platform/4.3/html/scalability_and_performance/index<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h2 id="deploying-virtual-master-nodes"><a href="#deploying-virtual-master-nodes" class="header-anchor">#</a> Deploying virtual master nodes</h2> <p>This section outlines the steps to create the virtual machines used as the master nodes.</p> <p><strong>NOTE</strong></p> <p>This section utilized vSphere rules such as affinity and anti-affinity rules to ensure no two master nodes are present on the same vSphere host, hence it is essential to enable vMotion in all the vSphere hosts. If not enabled, select the vSphere host in the VMware vCenter server user interface, <strong>click -&gt; Configure -&gt; Networking -&gt; VMkernel adapters -&gt; Management Network -&gt; Edit</strong> and select the checkbox against vMotion to enable vMotion.</p> <p>To create the virtual machines for the OpenShift master nodes, edit the variables file. Use an editor such as Vim or Nano, open the file <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/vsphere/virtual_nodes /roles/deploy_vm/vars/main.yml.</em> The variable file contains information about the VMs, vCenter, hostnames, IP addresses, memory, and CPU. A sample variable file is provided and as follows. The installation user should modify the file to make it suitable for the target environment.</p> <div class="language-yaml extra-class"><pre class="language-yaml"><code><span class="token comment"># Name of the Data center</span>

<span class="token key atrule">datacenter_name</span><span class="token punctuation">:</span> &lt;datacentername<span class="token punctuation">&gt;</span>

<span class="token comment"># Name of the compute clusters with the ESXi hosts for Management VMs</span>

<span class="token key atrule">management_cluster_name</span><span class="token punctuation">:</span> &lt;data_cluster_name<span class="token punctuation">&gt;</span>

<span class="token comment"># Name of the Datastore to store the VMs</span>

<span class="token key atrule">management_datastore_name</span><span class="token punctuation">:</span> &lt;datastore_name<span class="token punctuation">&gt;</span>

<span class="token comment"># Name of the coreOS guest image</span>

<span class="token key atrule">guest_template</span><span class="token punctuation">:</span> coreos64Guest

<span class="token comment"># Disk size in GB/GiB</span>

<span class="token key atrule">bootstrap_disk</span><span class="token punctuation">:</span> <span class="token number">120</span>

<span class="token key atrule">master_disk</span><span class="token punctuation">:</span> <span class="token number">120</span>

<span class="token key atrule">lb_disk</span><span class="token punctuation">:</span> <span class="token number">50</span>

<span class="token comment"># number of CPUs</span>

<span class="token key atrule">bootstrap_cpu</span><span class="token punctuation">:</span> <span class="token number">4</span>

<span class="token key atrule">master_cpu</span><span class="token punctuation">:</span> <span class="token number">4</span>

<span class="token key atrule">lb_cpu</span><span class="token punctuation">:</span> <span class="token number">4</span>

<span class="token comment"># Memory size in MB/MiB</span>

<span class="token key atrule">bootstrap_memory</span><span class="token punctuation">:</span> <span class="token number">16400</span>

<span class="token key atrule">master_memory</span><span class="token punctuation">:</span> <span class="token number">16400</span>

<span class="token key atrule">lb_memory</span><span class="token punctuation">:</span> <span class="token number">16400</span>

<span class="token key atrule">gateway</span><span class="token punctuation">:</span> &lt;replace_with_gateway_ip<span class="token punctuation">&gt;</span>

<span class="token key atrule">dns_server</span><span class="token punctuation">:</span> &lt;replace_with_dns_server_ip<span class="token punctuation">&gt;</span>

<span class="token key atrule">domain</span><span class="token punctuation">:</span> &lt;replace_with_domain_ip<span class="token punctuation">&gt;</span>

<span class="token comment"># name of the master, bootstrap and lb nodes &lt; short names, not the FQDN &gt;</span>

<span class="token key atrule">bootstrap01_name</span><span class="token punctuation">:</span> &lt;bootstrap01_host_name<span class="token punctuation">&gt;</span>

<span class="token key atrule">master01_name</span><span class="token punctuation">:</span> &lt;master01_host_name<span class="token punctuation">&gt;</span>

<span class="token key atrule">master02_name</span><span class="token punctuation">:</span> &lt;master02_host_name<span class="token punctuation">&gt;</span>

<span class="token key atrule">master03_name</span><span class="token punctuation">:</span> &lt;master03_host_name<span class="token punctuation">&gt;</span>

<span class="token key atrule">lb01_name</span><span class="token punctuation">:</span> &lt;lb01_host_name<span class="token punctuation">&gt;</span>

<span class="token key atrule">domain_name</span><span class="token punctuation">:</span> <span class="token string">&quot;&lt;sub_domain&gt;&quot;</span>

<span class="token comment"># Network names for the management network</span>

<span class="token key atrule">datacenter_network_name</span><span class="token punctuation">:</span> <span class="token string">&quot;&lt;network_name&gt;&quot;</span>

<span class="token comment"># vSphere affinity &amp; anti-affinity rules</span>

<span class="token key atrule">affinity_rule_name</span><span class="token punctuation">:</span> <span class="token string">&quot;vsphere-anti-affinty-rule&quot;</span>

<span class="token key atrule">anti_affinity_rule_name</span><span class="token punctuation">:</span> <span class="token string">&quot;vsphere-affinty-rule&quot;</span>
</code></pre></div><p>After the variable file is updated, execute the following command from the installer machine to deploy the specified VMs.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token builtin class-name">cd</span> /etc/ansible/hpe-solutions-openshift/synergy/scalable/vsphere/virtual_nodes
<span class="token operator">&gt;</span> ansible-playbook playbooks/deploy_vm.yml –ask-vault-pass
</code></pre></div><p><em>deploy_vm.yml</em> playbooks create 3x VMs to be used as master nodes, 1x VM to be used as load balancer node and 1x VM to be used as a bootstrap node. All the master VMs will be deployed on different hosts whereas bootstrap &amp; haproxy VMs will be deployed on any single host.</p> <p>Wait for some time for vSphere rules to be applicable on VMs. vSphere rules can be viewed at <strong>vCenter server -&gt; Datacenter -&gt; Cluster -&gt; Configure -&gt; VM/Host Rules</strong>.The 3 master nodes are part of the <em>vsphere-anti-affinity-rule</em> and each master VM will reside on a different vSphere host. The bootstrap and load balancer VMs are part of the <em>vsphere-affinity-rule</em> and they are co-resident on one of 3 vSphere hosts.</p> <p>It is recommended to ensure the <em>Boot Delay</em> is long enough to enable OS installation via PXE server.</p> <p>After the virtual machines are successfully created, refer to the following steps to install the operating system on the bootstrap node and the master nodes:</p> <ol><li><p>Ensure that the location of ignition files of the corresponding nodes is updated in the PXE configuration files.</p></li> <li><p>Ensure the MAC address of the network adapter in VM is updated with the corresponding IP address in the DHCP configuration file.</p></li> <li><p>Ensure that the load balancer server is up and running.</p></li> <li><p>From the VMware vCenter Server, select the VM, and launch the VM Remote console.</p></li> <li><p>From the Remote Console window, power on the VM.</p></li> <li><p>While booting, select the appropriate OS label.</p></li> <li><p>Wait until the OS installation is complete.</p></li> <li><p>Verify the installation by logging on to the node from the installer VM using the following command.</p></li></ol> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">ssh</span> core@<span class="token operator">&lt;</span> replace_with_node_fqdn_or_ip <span class="token operator">&gt;</span>
</code></pre></div><p>After the RHCOS master nodes are ready, refer to the section <a href="/hpe-solutions-openshift/Red Hat OpenShift Container Platform deployment/Red Hat OpenShift Container Platform deployment.html">Red Hat OpenShift Container Platform deployment</a> in this document to create the OpenShift 4 cluster.</p> <h2 id="deploying-virtual-worker-nodes"><a href="#deploying-virtual-worker-nodes" class="header-anchor">#</a> Deploying virtual worker nodes</h2> <p>This section outlines the steps to create virtual machines and configure them to be used as worker nodes.</p> <h3 id="creating-virtual-machines"><a href="#creating-virtual-machines" class="header-anchor">#</a> Creating virtual machines</h3> <p>This section outlines the steps to create virtual machines.</p> <ol><li><p>Login to vCenter using the Web Client and select an ESXi Host. Right-click the host and then click <strong>New Virtual Machine</strong> to open a <em>New Virtual Machine Wizard</em>.</p></li> <li><p>From <em>Select a creation type</em>, select <strong>Create a new virtual machine</strong> and click <strong>Next</strong>.</p></li> <li><p>Enter a unique <em>Name</em> for the VM and select the <strong>Datacenter</strong>. Click <strong>Next</strong>.</p></li> <li><p>Select the <strong>Cluster</strong> on which the VM can be deployed. Click <strong>Next</strong>.</p></li> <li><p>Select the <strong>Datastore</strong> on which the VM can be stored and click <strong>Next</strong>.</p></li> <li><p>On the Select compatibility page, choose <strong>ESXI 6.7 and later</strong> and click <strong>Next</strong>.</p></li> <li><p>On the Select a guest OS page, choose the Guest OS family as <strong>Linux</strong> and Guest OS Version as <strong>Red Hat Enterprise Linux 7 (64 bit)</strong> (in case of RHEL worker) and <strong>Red Hat CoreOS</strong> (in case of Red Hat CoreOS worker) and select <strong>Next</strong>.</p></li> <li><p>In the Customize hardware page, configure the Virtual Hardware with <strong>4 CPU</strong>, <strong>16 GB</strong> Memory, <strong>150 GB</strong> <strong>dual Hard Disk</strong> as per requirement and attach the Operating System from the datastore. Select the Connect at <strong>Power on</strong> option and click <strong>Next</strong>.</p></li> <li><p>Review the virtual machine configuration before deploying the virtual machine and click <strong>Finish</strong> to complete the New Virtual Machine wizard.</p></li></ol> <h3 id="red-hat-coreos-worker-nodes"><a href="#red-hat-coreos-worker-nodes" class="header-anchor">#</a> Red Hat CoreOS worker nodes</h3> <p>Refer to the section <a href="/hpe-solutions-openshift/Virtual nodes configuration/Virtual nodes configuration.html#creating-virtual-machines">Creating virtual machines</a> in the document to create virtual machines. After the virtual machines are successfully created, refer to the following steps to install the operating system on the worker nodes:</p> <ol><li><p>Ensure that the location of ignition files of the corresponding nodes is updated in the PXE configuration files.</p></li> <li><p>Ensure the MAC address of the network adapter in VM is updated with the corresponding IP address in the DHCP configuration file.</p></li> <li><p>Ensure that the load balancer server is up and running.</p></li> <li><p>From the VMware vCenter Server, select the VM and launch the VM Remote console.</p></li> <li><p>From the Remote Console window, power on the VM.</p></li> <li><p>While booting, select the appropriate OS label.</p></li> <li><p>Wait until the OS installation is complete.</p></li> <li><p>Verify the installation by logging on to the node from the installer VM using the following command.</p></li></ol> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">ssh</span> core@<span class="token operator">&lt;</span> replace_with_node_fqdn_or_ip <span class="token operator">&gt;</span>
</code></pre></div><p>After the RHCOS worker nodes are up and running, refer to the section <a href="/hpe-solutions-openshift/Red Hat OpenShift Container Platform deployment/Red Hat OpenShift Container Platform deployment.html#adding-red-hat-coreos-worker-nodes">Adding Red Hat CoreOS worker nodes</a> in the document to add them to OpenShift 4 cluster.</p> <h3 id="rhel-7-6-worker-nodes"><a href="#rhel-7-6-worker-nodes" class="header-anchor">#</a> RHEL 7.6 worker nodes</h3> <p>Refer to the section <a href="/hpe-solutions-openshift/Virtual nodes configuration/Virtual nodes configuration.html#creating-virtual-machines">Creating virtual machines</a> in this document to create virtual machines. After the virtual machines are successfully created, follow these steps to install the operating system on the worker nodes:</p> <ol><li><p>Ensure that the location of ignition files of the corresponding nodes is updated in the PXE configuration files.</p></li> <li><p>Ensure the MAC address of the network adapter in VM is updated with the corresponding IP address in the DHCP configuration file.</p></li> <li><p>Ensure that the load balancer server is up and running.</p></li> <li><p>From the VMware vCenter Server, select the VM, and launch the VM Remote console.</p></li> <li><p>From the Remote Console window, power on the VM.</p></li> <li><p>While booting, select the appropriate OS label.</p></li> <li><p>Wait until the OS installation is complete.</p></li> <li><p>Verify the installation by logging on to the node from the installer VM using the following command.</p></li></ol> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token function">ssh</span> root@<span class="token operator">&lt;</span> node_fqdn or node_ip_address<span class="token operator">&gt;</span>
</code></pre></div><p>Once the RHEL 7.6 nodes are reachable, refer to the section <a href="/hpe-solutions-openshift/Physical node configuration/Physical node configuration.html#preparing-worker-nodes-with-rhel">Preparing worker nodes with RHEL</a> in the document to prepare the RHEL worker nodes. After preparing the worker nodes, refer to the section <a href="/hpe-solutions-openshift/Red Hat OpenShift Container Platform deployment/Red Hat OpenShift Container Platform deployment.html#adding-rhel-7-6-worker-nodes">Adding RHEL 7.6 worker nodes</a> in the document to add them to the OpenShift 4 cluster.</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/hpe-solutions-openshift/Physical node configuration/Physical node configuration.html" class="prev">
        Physical node configuration
      </a></span> <span class="next"><a href="/hpe-solutions-openshift/Red Hat OpenShift Container Platform deployment/Red Hat OpenShift Container Platform deployment.html">
        Red Hat OpenShift Container Platform deployment
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/hpe-solutions-openshift/assets/js/app.adad66b3.js" defer></script><script src="/hpe-solutions-openshift/assets/js/2.ac0f675e.js" defer></script><script src="/hpe-solutions-openshift/assets/js/19.e7a9cad7.js" defer></script>
  </body>
</html>
