### **Adding RH CoreOS Worker Nodes to Existing Openshift Cluster**

This section covers the steps to add Baremetal RHCOS worker nodes to an existing Red Hat OpenShift Container Platform cluster.

1. Login to the Rhel 8.8 Installer VM (that we created as a part of rhel8_installerVM.yml -- it would have been created as one KVM VM on one of the head nodes)

2. Navigate to the directory $BASE_DIR(**/opt/hpe-solutions-openshift/DL-LTI-Openshift/**) then copy **input file and hosts** file to $BASE_DIR/coreos_BareMetalworker_nodes/ and later get the input file from the $BASE_DIR for ocp worker details. 

```
ansible-vault edit input.yaml

ocp_workers:
 - name: worker1
   ip: 172.X.X.X
   fqdn: worker1.XXX.XX.XXXX                          #ex. mworker1.ocp.isv.local
   mac_address:  ### update mac address of the worker node NIC
 - name: worker2
   ip: 172.X.X.X
   fqdn: worker2.XXX.XX.XXXX                          #ex. mworker2.ocp.isv.local
   mac_address:  ### update mac address of the worker node NIC
 - name: worker3
   ip: 172.2X.X.X
   fqdn: worker3.XX.XX.XXXX                          #ex. mworker3.ocp.isv.local
   mac_address:  ### update mac address of the worker node NIC

```

**NOTE**
ansible vault password is **changeme**

3. Execute the following command to add the worker nodes to the cluster

           'ansible-playbook -i hosts site.yml --ask-vault-pass'

In case, if user want to deploy through individual playbooks. Sequence of playbooks to be followed are:

			ansible-playbook -i hosts playbooks/binddns.yml --ask-vault-pass
			ansible-playbook -i hosts playbooks/haproxy.yml --ask-vault-pass
			ansible-playbook -i hosts playbooks/deploy_ipxe_ocp.yml --ask-vault-pass

4. Execute the following command for creating bonding on the network interfaces for baremetal CoreOS worker nodes 

	'ssh core@<CoreOS IP>
	ip -o link show|grep 'state UP' | awk -F ': ' '{print $2}'							### to retrieve only the names of the network interfaces that are currently UP

	sample output from the above command:
	ens1f0np0
	ens1f1np1 '

	'sudo nmcli connection add type bond con-name "bond0" ifname bond0
	 sudo nmcli connection modify bond0 bond.options "mode=active-backup,downdelay=0,miimon=100,updelay=0"
	 sudo nmcli connection add type ethernet slave-type bond con-name bond0-if1 ifname ens1f0np0 master bond0		###ens1f0np0 interface names from the sample output
	sudo nmcli connection add type ethernet slave-type bond con-name bond0-if2 ifname ens1f1np1 master bond0		###ens1f1np1 interface names from the sample output
	sudo nmcli connection up bond0
	sudo nmcli connection modify "bond0" ipv4.addresses '<<CoreOS IP  with netmask>>' ipv4.gateway '<<gateway IP>>' ipv4.dns  '<<dns server IP(all the head node IP)>>' ipv4.dns-search '<<domain name>>' ipv4.method manual
	
	example:
	sudo nmcli connection modify "bond0" ipv4.addresses '172.X.X.X/X' ipv4.gateway '172.X.X.X' ipv4.dns  '172.X.X.X,172.X.X.X,172.X.X.X' ipv4.dns-search 'isv.local' ipv4.method manual 

	sudo reboot '

### **Playbook description**

**site.yml**

-   This playbook contains the script for bringing up baremetal coreOS Worker Nodes and adds RHCOS worker nodes to an existing Red Hat OpenShift Container Platform cluster

**binddns.yml**

-   This playbook contains the script to deploy bind dns on three head nodes and it will work as both Active & Passive.

**haproxy.yml**

-   This playbook contains the script to deploy haproxy on the head nodes and it will act as Active.

**deploy_ipxe_ocp.yml**

-   This playbook contains the script to deploy the ipxe code on the RHEL8 installer machine.

After successful execution of all playbooks, check the node status as below.

** Approving server certificates (CSR) for newly added nodes **

The administrator needs to approve the CSR requests generated by each kubelet.

You can approve all Pending CSR requests using below command

        '$ oc get csr -o json | jq -r '.items[] | select(.status == {} ) | .metadata.name' | xargs oc adm certificate approve '
		
Later, Verify Node status using below command

         '$ oc get nodes'
