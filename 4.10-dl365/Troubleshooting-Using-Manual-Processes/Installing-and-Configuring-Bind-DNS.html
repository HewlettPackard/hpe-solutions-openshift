<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>OpenShift Container Platform 4.10 on HPE DL365 Gen10 Plus Servers</title>
    <meta name="generator" content="VuePress 1.9.7">
    
    <meta name="description" content="Hewlett Packard Enterprise">
    
    <link rel="preload" href="/hpe-solutions-openshift/4.10-dl/assets/css/0.styles.63fe4194.css" as="style"><link rel="preload" href="/hpe-solutions-openshift/4.10-dl/assets/js/app.f7bd77fb.js" as="script"><link rel="preload" href="/hpe-solutions-openshift/4.10-dl/assets/js/2.7c26c33b.js" as="script"><link rel="preload" href="/hpe-solutions-openshift/4.10-dl/assets/js/25.c3304277.js" as="script"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/10.b9aac181.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/11.a7f10f64.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/12.4dd9c041.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/13.4a65d88c.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/14.df2fa083.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/15.aa710849.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/16.facc29a4.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/17.15c44ee1.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/18.fcb650a7.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/19.2c3d1323.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/20.58a6ad97.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/21.548533a3.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/22.0fb16ed7.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/23.0f457265.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/24.2c3f4ae7.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/26.ba38ffac.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/3.7a36b079.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/4.f1950966.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/5.e7dabacf.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/6.57b774f0.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/7.7351e6ab.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/8.d609138f.js"><link rel="prefetch" href="/hpe-solutions-openshift/4.10-dl/assets/js/9.99fc6a60.js">
    <link rel="stylesheet" href="/hpe-solutions-openshift/4.10-dl/assets/css/0.styles.63fe4194.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/hpe-solutions-openshift/4.10-dl/" class="home-link router-link-active"><!----> <span class="site-name">OpenShift Container Platform 4.10 on HPE DL365 Gen10 Plus Servers</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/hpe-solutions-openshift/4.10-dl/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="http://www.hpe.com/info/ra" target="_blank" rel="noopener noreferrer" class="nav-link external">
  RA Library
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/hpe-solutions-openshift/4.10-dl/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="http://www.hpe.com/info/ra" target="_blank" rel="noopener noreferrer" class="nav-link external">
  RA Library
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Red Hat OpenShift Container Platform 4.10 on DL Servers</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Solution Overview</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Solution Components</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Solution Deployment</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Additional Features and Functionality</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>Troubleshooing by Using Manual Processes</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/hpe-solutions-openshift/4.10-dl/Troubleshooting-Using-Manual-Processes/Preparing-execution-environment.html" class="sidebar-link">/Troubleshooting-Using-Manual-Processes/Preparing-execution-environment.html</a></li><li><a href="/hpe-solutions-openshift/4.10-dl/Troubleshooting-Using-Manual-Processes/Deploying-OS-on-Head-Nodes.html" class="sidebar-link">/Troubleshooting-Using-Manual-Processes/Deploying-OS-on-Head-Nodes.html</a></li><li><a href="/hpe-solutions-openshift/4.10-dl/Troubleshooting-Using-Manual-Processes/Installing-and-Configuring-Bind-DNS.html" aria-current="page" class="active sidebar-link">/Troubleshooting-Using-Manual-Processes/Installing-and-Configuring-Bind-DNS.html</a></li><li><a href="/hpe-solutions-openshift/4.10-dl/Troubleshooting-Using-Manual-Processes/Configuring-Squid-Proxy.html" class="sidebar-link">/Troubleshooting-Using-Manual-Processes/Configuring-Squid-Proxy.html</a></li><li><a href="/hpe-solutions-openshift/4.10-dl/Troubleshooting-Using-Manual-Processes/OCP-Cluster-Deployment.html" class="sidebar-link">/Troubleshooting-Using-Manual-Processes/OCP-Cluster-Deployment.html</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Resosurces and Additional Links</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><p><strong>Installing and configuring master/slave Bind DNS on head nodes</strong></p> <p>Domain Name System (DNS) is a distributed database system that associates hostnames with their respective IP addresses. Berkeley Internet Name Domain (BIND) comprises a set of DNS-related programs. It is also known as the service named in Red Hat Enterprise Linux. The /etc/named.conf is the main configuration file in the BIND configuration. This section focuses on installing, configuring, and managing BIND on the DNS server for all head nodes to create master and slave configuration.</p> <p>Prerequisites:</p> <ul><li>All hosts must be running RHEL 8.5 OS.</li> <li>Red Hat subscription must be active.</li> <li><code></code>DNS server must be configured with the following settings:</li></ul> <p><strong>TABLE 9.</strong> DNS server settings</p> <table><thead><tr><th style="text-align:left;">DNS server settings</th> <th style="text-align:left;">Description</th></tr></thead> <tbody><tr><td style="text-align:left;">isv.local</td> <td style="text-align:left;">Zone (Domain name)</td></tr> <tr><td style="text-align:left;">172.28.230.11</td> <td style="text-align:left;">IP address of master DNS node</td></tr> <tr><td style="text-align:left;">172.28.230.12</td> <td style="text-align:left;">IP address of the 1st slave DNS node</td></tr> <tr><td style="text-align:left;">172.28.230.13</td> <td style="text-align:left;">IP address of the 2nd slave DNS node</td></tr></tbody></table> <p>Installing and configuring Bind DNS service on master nodes</p> <p>To install and configure Bind DNS service on master nodes:</p> <ol><li>Install Bind DNS service on the master node with the following command:</li></ol> <p># yum -y install bind bind-utils vim</p> <ol start="2"><li>Configure BIND DNS on the master node.</li> <li>Edit /etc/named.conf configuration file.</li></ol> <p>Options {</p> <p>listen-on port 53 { 127.0.0.1; 172.28.230.11; }😐</p> <p>llsten-on-v6 port 53 { ::1; };</p> <p>directory  &quot;/var/named&quot;;</p> <p>dump-file &quot;/var/named/data/cache_dump.db&quot;;</p> <p>statistics-file &quot;/var/named/data/named _stats.txt&quot;;</p> <p>memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;;</p> <p>secroots-file &quot;/var/named/data/named.secroots&quot;;</p> <p>recursing-file &quot;/var/named/data/named.recursing&quot;;</p> <p>allow-query  { localhost; 172.0.0.0/24; };</p> <p>allow-transfer { localhost; 172-28.230.12; 172.28.230.13; };</p> <p>recursion no;</p> <p>dnssec-enable yes;</p> <p>dnssec-validation yes;</p> <p>managed-keys-directory &quot;/var/named/dynamic &quot;;</p> <p>pid:-file &quot;/run/named/named.pid&quot;;</p> <p>session-keyfile &quot;/run/named./session.key&quot;;</p> <p>include&quot; /etc/crypto-policies/back.-ends/blind.config&quot;;</p> <p>};</p> <p>logging {</p> <p>channel default_debug {</p> <p>file &quot;data/named.run&quot;;</p> <p>severity dynamic;</p> <p><code></code>};</p> <p>);</p> <p>zone &quot;.&quot; IN {</p> <p>type hint;</p> <p>file &quot;named.ca&quot;;</p> <p>};</p> <p>zone &quot;isv.local&quot; IN{</p> <p>type master;</p> <p>file &quot;forward,isv&quot;;</p> <p>allow-update { none; };</p> <p>};</p> <p>zone &quot;230.28.172.in-addr.arpa&quot; IN{</p> <p>type master;</p> <p>file &quot;reverse. isv&quot;;</p> <p>allow-update { none; };</p> <p>};</p> <p>include &quot;/etc/named.rfc1912.zones&quot;;</p> <p>include &quot;/etc/named.root.key&quot;;</p> <ol start="3"><li>Create zone files.</li> <li>After setting the files in the named.conf configuration file, create the zone files in the /var/named/ director directory and place all the records that you would wish to add such as A/AAAA, MX, PTR, and so on.</li></ol> <p>[root@kvm~]# cat /var/named/forward.isv</p> <p>$TTL 86400</p> <p>@     IN SOA kvm1.isv.local. root.isv.local.(</p> <p>2011071001; serial</p> <p>300 ; refresh</p> <p>1800; retry</p> <p>604800; expire</p> <p>86400 ) ; minimum</p> <p>@      IN     NS      kvm1.isv.local.</p> <p>@      IN     NS      kvm2.isv.local.</p> <p>@      IN     NS      kvm3.isv.local.</p> <p>@      IN      A       172.28.230.11</p> <p>@      IN      A       172.28.230.12</p> <p>@      IN      A       172.28.230.13</p> <p>boot.ocp.isv.local.           IN     A     172.28.230.21</p> <p>master0.ocp.isv.local.     IN     A     172.28.230.22</p> <p>master1.ocp.isv.local.     IN     A     172.28.230.23</p> <p>master2.ocp.isv.local.     IN     A     172.28.230.24</p> <p>haproxy.ocp.isv.local.     IN     A     172.28.230.25</p> <p>api.int.ocp.isv.local.        IN     A     172.28.230.25</p> <p>api.ocp.isv.local.             IN     A     172.28.230.25</p> <p>workerl.ocp.isv.local.     IN     A     172.28.230.26</p> <p>worker2.ocp.isv.local.    IN     A     172.28.230.27</p> <p>worker3.ocp.isv.local.    IN     A     172.28.230.28</p> <p>* .apps.ocp.isv.local.      IN     A     172.28.230.25</p> <ol start="2"><li>Create the corresponding reverse records for the same domain that were defined in the named.conf configuration file.</li></ol> <p>[root@kvm1 ~]# cat /var/named/reverse.isv</p> <p>$TTL 86400</p> <p>@ IN SOA kvm1.isv.local.root.isv.local(</p> <p>2011071001 ; serial</p> <p>300 ; refresh</p> <p>1800; retry</p> <p>604800 ; expire</p> <p>86400 ) ; minimum</p> <p>@    IN    NS     kvml.isv.local.</p> <p>@    IN    NS     kvm2.isv.local.</p> <p>@    IN    NS     kvm3.isv.local.</p> <p>@    IN    PTR    isv.local.</p> <p>@    IN    A      172.28.230.11</p> <p>4P   IN    A      172.28.230.12</p> <p>11   IN    A      172.28.230.13</p> <p>11   IN    PTR    kvm1.isv.local.</p> <p>12   IN    PTR    kvm2.isv.local.</p> <p>13   IN    PTR    kvm3.isv.local.</p> <p>21   IN    PTR    boot.ocp.isv.local.</p> <p>22   IN    PTR    master0.ocp.isv.local.</p> <p>23   IN    PTR    master1.ocp.isv.local.</p> <p>24   IN    PTR    master2.ocp.isv.local.</p> <p>25   IN    PTR    haproxy.ocp.isv.local.</p> <p>25   IN    PTR    api.int.ocp.isv.local.</p> <p>25   IN    PTR    api.ocp.isv.local.</p> <p>26   IN    PTR    worker1.ocp.isv.local.</p> <p>27   IN    PTR    worker2.ocp.isv.local.</p> <p>28   IN    PTR    worker3.ocp.isv.local.</p> <ol start="4"><li>Modify DNS settings on the master server.</li> <li>Set the new DNS server as the default name server.</li> <li>Edit /etc/resolv.conf configuration file and add the following lines:</li></ol> <p><strong>NOTE</strong></p> <p>Ensure that the IP address is replaced to match your environment.</p> <p>[root@kvm1~]# cat/etc/resolv.conf.</p> <p>search isv.local</p> <p>nameserver 172.28.230.11</p> <ol start="5"><li>Configure firewall to allow DNS service.</li></ol> <p># sudo firewall-cmd--add-service=dns--permanent</p> <p># sudo firewall-cmd--reload</p> <p># sudo named-checkconf</p> <p># sudo systemctl start named</p> <p># sudo systemctl enable named</p> <p>The master BIND DNS server configuration is complete.</p> <p>Installing Bind DNS service on slave nodes</p> <p>To install Bind DNS service on slave nodes:</p> <ol><li>On the both the slave nodes with IP addresses as 172.28.230.12 and 172.28.230.13, install bind and bind-utils:</li></ol> <p># yum -y install bind bind-utils vim</p> <ol start="2"><li>Configure the slave nodes and edit /etc/named.conf configuration file.</li></ol> <p>options {</p> <p>listen-on port 53 { 127.0.0.1; 172.28.230.12; 172.28.230.13; };</p> <p>listen-on-v6 port 53 { ::1; };</p> <p>directory &quot; /var/named&quot;;</p> <p>dump-file &quot;/var/named/data/cache_dump.db&quot;;</p> <p>statistics-file &quot;/var/named/data/named_stats.txt&quot;;</p> <p>memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;;</p> <p>secroots-file &quot;/var/named/data/named.secroots&quot;;</p> <p>recursing-file &quot;var/named/data/named.recursing&quot;;</p> <p>allow-query { localhost; 172.0.0.0/24; };</p> <p>recursion yes;</p> <p>dnssec-enable yes;</p> <p>dnssec-validation yes;</p> <p>managed-keys-directory &quot;/var/named/dynamic&quot;;</p> <p>pid-file &quot; /run/named/named.pid&quot;;</p> <p>session-keyfile &quot;/run/named/session.key&quot;;</p> <p>include &quot;/etc/crypto-policies/back-ends/bind_config&quot;;</p> <p>};</p> <p>logging {</p> <p>channel default_debug {</p> <p>file &quot;data/named.run&quot;;</p> <p>severity dynamic;</p> <p>};</p> <p>};</p> <p>zone &quot;.&quot; IN {</p> <p>type hint;</p> <p>file &quot;named.ca&quot;;</p> <p>};</p> <p>zone &quot;isv.local&quot; IN{</p> <p>type slave;</p> <p>file &quot;slaves/forward,isv&quot;;</p> <p>master { 172.28.230.11; };</p> <p>};</p> <p>zone &quot;230.28.172.in-addr.arpa&quot; IN{</p> <p>type slave;</p> <p>file &quot;slaves/reverse. isv&quot;;</p> <p>master { 172.28.230.11; };</p> <p>};</p> <p>include &quot;/etc/named.rfc1912.zones&quot;;</p> <p>include &quot;/etc/named.root.key&quot; ;</p> <ol start="3"><li>Modify DNS settings on the slave server.</li> <li>Set the new DNS servers (both master and slaves) as the default name servers.</li> <li>Edit /etc/resolv.conf configuration file and add the following lines:</li></ol> <p><strong>NOTE</strong></p> <p>Ensure that the IP addresses are replaced to match your environment.</p> <p>[root.kvm2~]9# cat/etc/resolv.conf</p> <p>search isv.local</p> <p>nameserver 172.28.230.11</p> <p>nameserver 172.28.230.12</p> <p>nameserver 172.28.230.13</p> <ol start="3"><li>Check the configurations and then start and enable BIND:</li></ol> <p># sudo named-checkconf</p> <p># sudo systemctl start named</p> <p># sudo systemctl enable named</p> <ol start="4"><li>Verify that the /var/named/slaves directory includes the zone files that were transferred from the master node.</li></ol> <p>[root@kvm2~]# ls -| /var/named/slaves/</p> <p>-rw-r-r-1 named named 900 Jun 23 09:11 forward.isv</p> <p>-rw-r-r-1 named named 1168 Jun 23 09:12 reverse.isv</p> <p>[root@)kvm3~]#ls-| /var/named/slaves/</p> <p>-rw-r-r-1 named named 900 Jun 23 09:11 forward.isv</p> <p>-rw-r-r-1 named named 1168 Jun 23 09:12 reverse.isv</p> <p>Testing master/slave DNS configuration</p> <p>To test the DNS resolution from the master/slave DNS servers:</p> <ol><li>On the master DNS server, run the following command:</li></ol> <ul><li>Master DNS node:</li></ul> <p>[root@kvm1~]# dig +short master0.ocp.isv.local</p> <p>172.28.230.22</p> <p>[root@kvm1~]# dig +short -x 172.28.230.22</p> <p>master0.ocp.isv.local.</p> <ol start="2"><li>On both the slave DNS servers, run the following commands:</li></ol> <ul><li>1st Slave DNS node:</li></ul> <p>[root@kvm2~]# dig +short master0.ocp.isv.local</p> <p>172.28.230.22</p> <p>[root@kvm2~]# dig +short -x 172.28.230.22</p> <p>master0.ocp.isv.local.</p> <ul><li>2nd Slave DNS node:</li></ul> <p>[root@kvm3~]# dig +short master0.ocp.isv.local</p> <p>172.28.230.22</p> <p>[root@kvm3~]# dig +short -x 172.28.230.22</p> <p>master0.ocp.isv.local.</p> <p><strong>Configuring load balancer on head nodes</strong></p> <p>The RH OCP 4.10 uses an external load balancer to communicate from outside the cluster with services running inside the cluster. This section assumes that there is a load balancer available within our deployment environment and is available for use. Our deployment environment is developed using HAProxy, an open-source solution with one virtual machine for load balancing functionality.</p> <p>In a production environment, Hewlett Packard Enterprise recommends the use of enterprise load balancing such as F5 Networks Big-IP and its associated products.</p> <p>To configure load balancer on all head nodes:</p> <ol><li>Install haproxy and keepalived service on all head nodes.</li></ol> <p># yum install haproxy</p> <p># systemctl enable haproxy</p> <p># systemctl start haproxy</p> <ol start="2"><li>Edit /etc/haproxy/haproxy.conf configuration file on all head nodes.</li></ol> <p>frontend openshift-api-server</p> <p><code></code>bind *:6443</p> <p><code></code>default_backend openshift-api-server</p> <p><code></code>mode tcp</p> <p><code></code>option tcplog</p> <p>backend openshift-api-server</p> <p><code></code>balance source</p> <p><code></code>mode tcp</p> <p>server ocpboot1 boot.ocp.isv.local:6443 check</p> <p><code></code>server ocpmaster1 master0.ocp.isv.local:6443 check</p> <p><code></code>server ocpmaster2 master1.ocp.isv.local:6443 check</p> <p><code></code>server ocpmaster3 master2.ocp.isv.local:6443 check</p> <p>frontend machine-config-server68</p> <p><code></code>bind *:22623</p> <p><code></code>default_backend machine-config-server</p> <p><code></code>mode tcp</p> <p><code></code>option tcplog</p> <p>backend machine-config-server</p> <p><code></code>balance source</p> <p><code></code>mode tcp</p> <p>server ocpboot1 boot.ocp.isv.local:22623 check</p> <p><code></code>server ocpmaster1 master0.ocp.isv.local:22623 check</p> <p><code></code>server ocpmaster2 master1.ocp.isv.local:22623 check</p> <p><code></code>server ocpmaster3 master2.ocp.isv.local:22623 check</p> <p>frontend ingress-http</p> <p><code></code>bind *:80</p> <p><code></code>default_backend ingress-http</p> <p><code></code>mode tcp</p> <p><code></code>option tcplog</p> <p>backend ingress-http</p> <p><code></code>balance source</p> <p><code></code>mode tcp</p> <p><code></code>server ocpworker1 worker1.ocp.isv.local:80 check</p> <p><code></code>server ocpworker2 worker2.ocp.isv.local:80 check</p> <p><code></code>server ocpworker3 worker3.ocp.isv.local:80 check</p> <p># if creating a cluster with only master nodes to begin with and later adding the worker nodes, master nodes should be added in this section instead of worker nodes. Once all the worker nodes are added into the cluster, this configuration needs to be updated with the worker nodes.</p> <p><code></code># server ocpmaster01 master0.ocp.isv.local:80 check</p> <p><code></code># server ocpmaster02 master1.ocp.isv.local:80 check</p> <p><code></code># server ocpmaster03 master2.ocp.isv.local:80 check</p> <p>frontend ingress-https</p> <p><code></code>bind *:443</p> <p><code></code>default_backend ingress-https</p> <p><code></code>mode tcp</p> <p><code></code>option tcplog</p> <p>backend ingress-https</p> <p><code></code>balance source</p> <p><code></code>mode tcp</p> <p><code></code>server ocpworker1 worker1.ocp.isv.local:443 check</p> <p><code></code>server ocpworker2 worker2.ocp.isv.local:443 check</p> <p><code></code>server ocpworker3 worker3.ocp.isv.local:443 check</p> <p># if creating a cluster with only master nodes to begin with and later adding the worker nodes, master nodes should be added in this section instead of worker nodes. Once all the worker nodes are added into the cluster, this configuration needs to be updated with the worker nodes.</p> <p><code></code># server ocpmaster01 master0.ocp.isv.local:443 check</p> <p><code></code># server ocpmaster02 master1.ocp.isv.local:443 check</p> <p><code></code># server ocpmaster03 master2.ocp.isv.local:443 check</p> <p><strong>NOTE</strong></p> <p>The load balancer configuration should contain values that are aligned to the installation environment.</p> <p>Installing Keepalived service for HAProxy servers</p> <p>The Keepalived service provides highly available capabilities on the load balancer for HAProxy server configuration.</p> <p>To install the Keepalived service on all three head nodes:</p> <ol><li>Install Keepalived and psmisc service.</li></ol> <p><strong>NOTE</strong></p> <p>The psmisc service provides killall to check the HAProxy for VRRP.</p> <p># yum install -y keepalived psmisc</p> <ol start="2"><li>Determine the interface that can be used with the services. For example, the highlighted interface is used with the services on our load balancer for HAProxy servers.</li></ol> <p>[root@kvm3 slaves]# ip a</p> <p>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</p> <p><code></code>link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</p> <p><code></code>inet 127.0.0.1/8 scope host lo</p> <p><code></code>valid_lft forever preferred_lft forever</p> <p><code></code>inet6 ::1/128 scope host</p> <p><code></code>valid_lft forever preferred_lft forever</p> <p>2: ens2: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc mq master bond0 state UP group default qlen 1000</p> <p><code></code>link/ether 88:e9:a4:41:55:b8 brd ff:ff:ff:ff:ff:ff</p> <p>3: ens3: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc mq master bond0 state UP group default qlen 1000</p> <p><code></code>link/ether 88:e9:a4:41:55:b8 brd ff:ff:ff:ff:ff:ff permaddr 88:e9:a4:40:f6:d8</p> <p>4: bridge0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</p> <p><code></code>link/ether 88:e9:a4:41:55:b8 brd ff:ff:ff:ff:ff:ff</p> <p><code></code>inet6 fe80::8034:bdc:f7d0:b444/64 scope link noprefixroute</p> <p><code></code>valid_lft forever preferred_lft forever</p> <p>5: bond0: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master bridge0 state UP group default qlen 1000</p> <p><code></code>link/ether 88:e9:a4:41:55:b8 brd ff:ff:ff:ff:ff:ff</p> <p>6: bridge0.230: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</p> <p><code></code>link/ether 88:e9:a4:41:55:b8 brd ff:ff:ff:ff:ff:ff</p> <p><code></code>inet 172.28.230.13/24 brd 172.28.230.255 scope global noprefixroute bridge0.230</p> <p><strong>NOTE</strong></p> <p>This interface could be a dedicated interface or a shared interface. It needs to belong on the same broadcast domain as the VIP.</p> <ol><li>Allow all connections from the interface for traffic use. Use the following IP addresses as interfaces for HAProxy:</li></ol> <p># iptables -A INPUT -s 172.28.230.11 -j ACCEPT</p> <p># iptables -A INPUT -s 172.28.230.12 -j ACCEPT</p> <p># iptables -A INPUT -s 172.28.230.13 -j ACCEPT</p> <ol start="2"><li>Allocate an IP address from the existing HAproxy network for the floating IP address of the load balancers.</li> <li>Update the existing cluster DNS name to the newly assigned IP address.</li> <li>Generate a random external password for Keepalived AUTH_PASS.</li></ol> <p># uuidgen</p> <p>f30fedc5-2b19-414f-b67e-f05742a82e78</p> <p>Configuring Keepalived service for each node</p> <p>To configure Keepalived service for each node:</p> <ol><li>Edit /etc/keepalived/keepalived.conf configuration file on all three nodes for active/active VIP.</li></ol> <p>[root@kvm3 slaves]# vi /etc/keepalived/keepalived.conf</p> <p>router_id ovp_vrrp</p> <p>}</p> <p>vrrp_script haproxy_check {</p> <p><code></code>script &quot;killall -0 haproxy&quot;</p> <p><code></code>interval 2</p> <p><code></code>weight 2</p> <p><code></code>}</p> <p>vrrp_instance OCP_EXT {</p> <p><code></code>interface bridge0.230</p> <p>virtual_router_id 51</p> <p>priority 100</p> <p><code></code>state MASTER</p> <p><code></code>virtual_ipaddress {</p> <p><code></code>172.28.230.25 dev bridge0.230}</p> <p><code></code>track_script {</p> <p><code></code>haproxy_check</p> <p>}</p> <p><code></code>authentication {</p> <p><code></code>auth_type PASS</p> <p><code></code>auth_pass 1cee4b6e-2cdc-48bf-83b2-01a96d1593e4</p> <p><code></code>}</p> <p><code></code>}</p> <p>The keepalived configuration file consists of the following parts:</p> <ul><li><strong>state MASTER:</strong> It is the primary HAproxy server.</li> <li><strong>priority line:</strong> It determines the priority of the server configuration. If a master server must be elected, the server with the highest priority is selected.</li> <li><strong>virtual_ipaddress:</strong> It denotes the IP address to be used for floating VIP and the local device to bind to 10.0.15.30 and enp1s0 interface.</li></ul> <ol start="2"><li>Start and enable the services on all head nodes.</li></ol> <p># systemctl enable keepalived; systemctl start keepalived</p> <p>Testing HAProxy with Keepalived service</p> <p>After successful deployment of Keepalived service on the head nodes, the HAProxy nodes route traffic via HAProxy when the VRRP VIP is available.</p> <p>[root@kvm3 slaves]# ip a</p> <p>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</p> <p><code></code>link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</p> <p><code></code>inet 127.0.0.1/8 scope host lo</p> <p><code></code>valid_lft forever preferred_lft forever</p> <p><code></code>inet6 ::1/128 scope host</p> <p><code></code>valid_lft forever preferred_lft forever</p> <p>2: ens2: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc mq master bond0 state UP group default qlen 1000</p> <p><code></code>link/ether 88:e9:a4:41:55:b8 brd ff:ff:ff:ff:ff:ff</p> <p>3: ens3: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc mq master bond0 state UP group default qlen 1000</p> <p><code></code>link/ether 88:e9:a4:41:55:b8 brd ff:ff:ff:ff:ff:ff permaddr 88:e9:a4:40:f6:d8</p> <p>4: bridge0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</p> <p><code></code>link/ether 88:e9:a4:41:55:b8 brd ff:ff:ff:ff:ff:ff</p> <p><code></code>inet6 fe80::8034:bdc:f7d0:b444/64 scope link noprefixroute</p> <p><code></code>valid_lft forever preferred_lft forever</p> <p>5: bond0: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master bridge0 state UP group default qlen 1000</p> <p><code></code>link/ether 88:e9:a4:41:55:b8 brd ff:ff:ff:ff:ff:ff</p> <p>6: bridge0.230: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</p> <p><code></code>link/ether 88:e9:a4:41:55:b8 brd ff:ff:ff:ff:ff:ff</p> <p><code></code>inet 172.28.230.13/24 brd 172.28.230.255 scope global noprefixroute bridge0.230</p> <p><code></code>valid_lft forever preferred_lft forever</p> <p><code></code>inet 172.28.230.25/32 scope global bridge0.230</p> <p><code></code>valid_lft forever preferred_lft forever</p> <p><code></code>inet6 fe80::c48b:8f0f:9902:b9a1/64 scope link noprefixroute</p> <p><code></code>valid_lft forever preferred_lft forever</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/hpe-solutions-openshift/4.10-dl/Troubleshooting-Using-Manual-Processes/Deploying-OS-on-Head-Nodes.html" class="prev">
        /Troubleshooting-Using-Manual-Processes/Deploying-OS-on-Head-Nodes.html
      </a></span> <span class="next"><a href="/hpe-solutions-openshift/4.10-dl/Troubleshooting-Using-Manual-Processes/Configuring-Squid-Proxy.html">
        /Troubleshooting-Using-Manual-Processes/Configuring-Squid-Proxy.html
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/hpe-solutions-openshift/4.10-dl/assets/js/app.f7bd77fb.js" defer></script><script src="/hpe-solutions-openshift/4.10-dl/assets/js/2.7c26c33b.js" defer></script><script src="/hpe-solutions-openshift/4.10-dl/assets/js/25.c3304277.js" defer></script>
  </body>
</html>
