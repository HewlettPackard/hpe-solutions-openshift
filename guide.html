<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>HPE Deployment Guide for Red Hat OpenShift Container Platform 4 on HPE Synergy | RED HAT OPENSHIFT CONTAINER PLATFORM 4 ON HPE SYNERGY</title>
    <meta name="description" content="">
    <meta name="generator" content="VuePress 1.4.0">
    
    
    <link rel="preload" href="/hpe-solutions-openshift/assets/css/0.styles.03110986.css" as="style"><link rel="preload" href="/hpe-solutions-openshift/assets/js/app.adad66b3.js" as="script"><link rel="preload" href="/hpe-solutions-openshift/assets/js/2.ac0f675e.js" as="script"><link rel="preload" href="/hpe-solutions-openshift/assets/js/20.826352b1.js" as="script"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/10.bbe6a8b1.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/11.c231a443.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/12.381ec328.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/13.bc9c4fc5.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/14.6c2a59eb.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/15.10105a64.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/16.de305cb4.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/17.c566bc9d.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/18.1a3abf91.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/19.e7a9cad7.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/21.e9b44a36.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/3.f3b6057a.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/4.d098f71d.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/5.c9b2efa3.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/6.e3886d8a.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/7.8d533ade.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/8.97285f82.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/9.cd9b026d.js">
    <link rel="stylesheet" href="/hpe-solutions-openshift/assets/css/0.styles.03110986.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/hpe-solutions-openshift/" class="home-link router-link-active"><!----> <span class="site-name">RED HAT OPENSHIFT CONTAINER PLATFORM 4 ON HPE SYNERGY</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/hpe-solutions-openshift/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="http://www.hpe.com/info/ra" target="_blank" rel="noopener noreferrer" class="nav-link external">
  RA Library
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/hpe-solutions-openshift/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="http://www.hpe.com/info/ra" target="_blank" rel="noopener noreferrer" class="nav-link external">
  RA Library
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/hpe-solutions-openshift/" class="sidebar-link">Introduction</a></li><li><a href="/hpe-solutions-openshift/Solution overview/Solution overview.html" class="sidebar-link">Solution overview</a></li><li><a href="/hpe-solutions-openshift/Solution components/Solution components.html" class="sidebar-link">Solution components</a></li><li><a href="/hpe-solutions-openshift/Preparing the execution environment/Preparing the execution environment.html" class="sidebar-link">Preparing the execution environment</a></li><li><a href="/hpe-solutions-openshift/Physical environment configuration/Physical environment configuration.html" class="sidebar-link">Physical environment configuration</a></li><li><a href="/hpe-solutions-openshift/Physical node configuration/Physical node configuration.html" class="sidebar-link">Physical node configuration</a></li><li><a href="/hpe-solutions-openshift/Virtual nodes configuration/Virtual nodes configuration.html" class="sidebar-link">Virtual nodes configuration</a></li><li><a href="/hpe-solutions-openshift/Red Hat OpenShift Container Platform deployment/Red Hat OpenShift Container Platform deployment.html" class="sidebar-link">Red Hat OpenShift Container Platform deployment</a></li><li><a href="/hpe-solutions-openshift/Red Hat Local Storage Operator/Red Hat Local Storage Operator.html" class="sidebar-link">Red Hat Local Storage Operator</a></li><li><a href="/hpe-solutions-openshift/Securing RedHat OpenShift Container Platform using Sysdig Secure and Sysdig Monitor/Securing RedHat OpenShift Container Platform using Sysdig Secure and Sysdig Monitor.html" class="sidebar-link">Securing RedHat OpenShift Container Platform using Sysdig Secure and Sysdig Monitor</a></li><li><a href="/hpe-solutions-openshift/Physical worker node labeling in OpenShift/Physical worker node labeling in OpenShift.html" class="sidebar-link">Physical worker node labeling in OpenShift</a></li><li><a href="/hpe-solutions-openshift/OpenShift Operators/OpenShift Operators.html" class="sidebar-link">OpenShift Operators</a></li><li><a href="/hpe-solutions-openshift/Validating OpenShift Container Platform deployment/Validating OpenShift Container Platform deployment.html" class="sidebar-link">Validating OpenShift Container Platform deployment</a></li><li><a href="/hpe-solutions-openshift/Resources and additional links/Resources and additional links.html" class="sidebar-link">Resources and additional links</a></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="hpe-deployment-guide-for-red-hat-openshift-container-platform-4-on-hpe-synergy"><a href="#hpe-deployment-guide-for-red-hat-openshift-container-platform-4-on-hpe-synergy" class="header-anchor">#</a> HPE Deployment Guide for Red Hat OpenShift Container Platform 4 on HPE Synergy</h1> <h1 id="introduction"><a href="#introduction" class="header-anchor">#</a> Introduction</h1> <p>This document describes the steps required to create a Red Hat OpenShift Container Platform 4 environment running on HPE Synergy. It is intended to be used in conjunction with Ansible playbooks and python scripts found at <a href="https://github.com/HewlettPackard/hpe-solutions-openshift" target="_blank" rel="noopener noreferrer">https://github.com/HewlettPackard/hpe-solutions-openshift<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>. This document was created using OpenShift Container Platform 4.3 and the documents for that version are available at <a href="https://docs.openshift.com/container-platform/4.3/welcome/index.html" target="_blank" rel="noopener noreferrer">https://docs.openshift.com/container-platform/4.3/welcome/index.html<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>. Hewlett Packard Enterprise strives to make regular updates to this solution with the goal of validating against the latest available version of Red Hat OpenShift Container Platform. As a result, references in the document will generally refer to OpenShift Container Platform 4 and will mention specific sub-versions in strategic locations to note the version that was tested.</p> <p>This document should be reviewed in its entirety and the installation user should understand all prerequisites and procedures prior to installation. It is also recommended that the installation user review the OpenShift Container Platform 4 installation process as described by Red Hat.</p> <p><strong>NOTE</strong></p> <p>Hewlett Packard Enterprise plans to update this document over time with enhancements to deployment methodologies as well as new software versions, features, and functions. Check for the latest document at <a href="https://github.com/HewlettPackard/hpe-solutions-openshift/tree/master/synergy/scalable" target="_blank" rel="noopener noreferrer">https://github.com/HewlettPackard/hpe-solutions-openshift/tree/master/synergy/scalable<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p>This solution is built with the bare metal provisioner provided by Red Hat. Further details on the bare metal provisioner are available at <a href="https://docs.openshift.com/container-platform/4.3/installing/installing_bare_metal/installing-bare-metal.html#installing-bare-metal" target="_blank" rel="noopener noreferrer">https://docs.openshift.com/container-platform/4.3/installing/installing_bare_metal/installing-bare-metal.html#installing-bare-metal<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h1 id="solution-overview"><a href="#solution-overview" class="header-anchor">#</a> Solution overview</h1> <h2 id="solution-design"><a href="#solution-design" class="header-anchor">#</a> Solution design</h2> <p>Red Hat OpenShift Container Platform master nodes and worker nodes can be deployed on HPE Synergy 480 Gen10 Compute Modules running Red Hat CoreOS (RHCOS) on bare metal or as virtual machines. Worker nodes may also run Red Hat Enterprise Linux (RHEL) 7.6 on bare metal or as virtual machines. As the workload and number of container pods grow, the user can scale up the number of worker nodes to increase the number of pods that can be scheduled.</p> <p>Figure 1 describes the minimal configuration of Red Hat OpenShift Container Platform 4 deployment on HPE Synergy.</p> <p>![Solution layout](/figure1.png)</p> <p>![]</p> <p><strong>Figure 1.</strong> Solution layout</p> <p>**NOTE **</p> <p>The scripts and reference files provided with this document are included as examples of how to build the solution. They are not supported by Hewlett Packard Enterprise or Red Hat. It is expected that the scripts and reference files will be modified to meet the requirements of the deployment environment by the installation user prior to installation.</p> <p>Figure 2 provides an overview of the layout and data storage design for this solution. A detailed description on each of the components used in this solution are outlined further in this document.</p> <p>![Solution design by function and type](/figure2.png)</p> <p>![][1]</p> <p><strong>Figure 2.</strong> Solution design by function and storage type</p> <p><strong>NOTE</strong></p> <p>In addition to Red Hat CoreOS worker nodes as shown in figure 2, this solution also supports worker nodes running Red Hat Enterprise Linux (RHEL) 7.6. Refer to the section [Red Hat OpenShift worker nodes with RHEL] in this document for more details.</p> <h3 id="solution-creation-process"><a href="#solution-creation-process" class="header-anchor">#</a> Solution creation process</h3> <p>Figure 3 shows the flow of the installation process and aligns with this document. For enhanced readability, a high-resolution copy of this image is located in the root directory of the solution GitHub. It is recommended that the installation user downloads and reviews this image prior to proceeding.</p> <p>![Solution creation process](/figure3.png)</p> <p>![][2]</p> <p><strong>Figure 3.</strong> Solution creation process</p> <h3 id="standards-used-in-this-document"><a href="#standards-used-in-this-document" class="header-anchor">#</a> Standards used in this document</h3> <p>This document makes use of the following standard terms:</p> <ul><li><p>Installation user or installer – Individual or individuals responsible for carrying out the installation tasks to produce a functional Red Hat OpenShift Container Platform 4 solution on HPE Synergy.</p></li> <li><p>Installer machine or installer VM – The system that is capable of connecting to various components within the solution and is used to run most of the key commands. In this solution, this machine also serves as the Ansible Engine host. For more information, refer to the section [Installer machine] in this document for more details.</p></li> <li><p>Bootstrap node – The cluster requires the bootstrap machine to deploy the OpenShift Container Platform cluster on the three control plane machines. It can be removed after the cluster installation.</p></li></ul> <h1 id="solution-components"><a href="#solution-components" class="header-anchor">#</a> Solution components</h1> <h2 id="hardware"><a href="#hardware" class="header-anchor">#</a> Hardware</h2> <p>Figure 4 shows the physical configuration of the racks used in this solution.</p> <p>![Physical layout of the solution](/figure4.png)</p> <p>![][3]</p> <p><strong>Figure 4.</strong> Physical layout of the solution</p> <p>The configuration outlined in this document is based on the design guidance of an HPE Converged Architecture 750 Foundation model which offers an improved time to deployment and tested firmware recipe. The recipe can be retrieved at <a href="https://techlibrary.hpe.com/us/en/enterprise/integrated-systems/info-library/index.aspx?cat=convergedsystems#.XnB69KgzZPY" target="_blank" rel="noopener noreferrer">https://techlibrary.hpe.com/us/en/enterprise/integrated-systems/info-library/index.aspx?cat=convergedsystems#.XnB69KgzZPY<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>. It is strongly recommended that the installation user utilizes the latest available matrix. Hewlett Packard Enterprise has tested this solution with the latest firmware recipe available as of October 2019, including HPE OneView for Synergy 5.0.</p> <p>The installation user has the flexibility to customize the HPE components throughout this stack in accordance with the unique IT and workload requirements or to build the solution with individual components rather than using HPE CS750.</p> <p>Table 1 highlights the individual components and their quantities as deployed within the solution.</p> <p><strong>Table 1.</strong> Components utilized in the creation of this solution.</p> <p>| <strong>Component | Quantity | Description</strong> |</p> <p>| ------------------------------------- | -------- | ------------------------------------------------------------ |</p> <p>| HPE Synergy 12000 Frame | 3 | Three (3) HPE Synergy 12000 Frames house the infrastructure used for the solution |</p> <p>| HPE Virtual Connect 40Gb SE F8 Module | 2 | A total of two (2) HPE Virtual Connect 40Gb SE F8 Modules provide network connectivity into and out of the frames |</p> <p>| HPE Synergy 480 Gen10 Compute Module | 6 | Three (3) bare metal or virtualized management nodes and three (3) bare metal for worker nodes |</p> <p>| HPE Synergy D3940 Storage | 3 | Three (3) HPE Synergy D3940 12Gb SAS CTO Drive Enclosure with 40 SFF (2.5in) Drive Bays |</p> <p>| HPE FlexFabric 2-Slot Switch | 2 | Each switch contains one (1) each of the HPE 5945 modules listed as follows |</p> <p>| HPE 5945 24p SFP+ and 2p QSFP+ Module | 2 | One module per HPE FlexFabric 2-Slot Switch |</p> <p>| HPE 5945 8p QSFP+ Module | 2 | One module per HPE FlexFabric 2-Slot Switch |</p> <p>| HPE Synergy Composer | 2 | Core configuration and lifecycle management for the Synergy components |</p> <h2 id="software"><a href="#software" class="header-anchor">#</a> Software</h2> <p>Table 2 describes the versions of important software utilized in the creation of this solution. The installation user should ensure that they download or have access to this software. Ensure that the appropriate subscriptions and licensing are in place to use within the planned time frame.</p> <p><strong>Table 2.</strong> Major software versions used in the creation of this solution</p> <p>| <strong>Component</strong> | <strong>Version</strong> |</p> <p>| ------------------------------------ | ------------------------------------------------------------ |</p> <p>| Red Hat CoreOS | 4.3 |</p> <p>| Red Hat OpenShift Container Platform | 4.2 and 4.3 |</p> <p>| Red Hat Enterprise Linux | 7.6 |</p> <p><strong>NOTE</strong></p> <p>The latest sub-version of each component listed in Table 2 should be installed.</p> <p>When utilizing virtualized nodes, the software version used in the creation of this solution are shown in Table 3.</p> <p><strong>Table 3.</strong> Software versions used with virtualized implementations</p> <p>| <strong>Component</strong> | <strong>Version</strong> |</p> <p>| ------------------------------------ | ------------------------------------------------------------ |</p> <p>| VMware vSphere | ESXi 6.7 U2 (Build: 13981272) |</p> <p>| VMware vCenter Server Appliance | 6.7 Update 2c (Build: 14070457) |</p> <p>Software installed on the installer machine is shown in Table 4.</p> <p><strong>Table 4.</strong> Software installed on the installer machine</p> <p>| <strong>Component</strong> | <strong>Version</strong> |</p> <p>| ------------------------------------ | ------------------------------------------------------------ |</p> <p>| Ansible | 2.9 |</p> <p>| Python | 3.6 |</p> <p>| Java | 1.8 |</p> <p>| Openshift Container Platform packages | 4.3 |</p> <h2 id="services"><a href="#services" class="header-anchor">#</a> Services</h2> <p>This document is built with assumptions about services and network ports available within the implementation environment. This section discusses those assumptions.</p> <p>Table 5 disseminates the services required in this solution and provides a high-level explanation of their function.</p> <p><strong>Table 5.</strong> Services used in the creation of this solution.</p> <p>| <strong>Service</strong> | <strong>Description/Notes</strong> |</p> <p>| --------------------- | ------------------------------------------------------------ |</p> <p>| DNS | Provides name resolution on management and data center networks. |</p> <p>| DHCP | Provides IP address leases on PXE, management and usually for data center networks. |</p> <p>| NTP | Ensures consistent time across the solution stack. |</p> <p>| PXE | Enables booting of operating systems |</p> <h3 id="dns"><a href="#dns" class="header-anchor">#</a> DNS</h3> <p>Domain Name Services must be in place for the management and data center networks. Ensure that both forward and reverse lookups are working for all hosts.</p> <h3 id="dhcp"><a href="#dhcp" class="header-anchor">#</a> DHCP</h3> <p>DHCP should be present and able to provide IP address leases on the PXE, management, and data center networks.</p> <h3 id="ntp"><a href="#ntp" class="header-anchor">#</a> NTP</h3> <p>A Network Time Protocol (NTP) server should be available to hosts within the solution environment.</p> <h3 id="pxe"><a href="#pxe" class="header-anchor">#</a> PXE</h3> <p>Because all nodes in this solution are booted using PXE, a properly configured PXE server is essential.</p> <h3 id="network-port"><a href="#network-port" class="header-anchor">#</a> Network port</h3> <p>The port information listed in Table 6 allows cluster components to communicate with each other. This information can be retrieved from bootstrap, master, and worker nodes by running the following command.</p> <blockquote><p>```</p> <p># netstat –tupln</p> <p>```</p></blockquote> <p>Table 6 shows list of network ports used by the services under OpenShift Container Platform 4.</p> <p><strong>Table 6.</strong> List of network ports.</p> <p>| <strong>Protocol</strong> | <strong>Port Number/Range</strong> | <strong>Service Type</strong> | <strong>Other details</strong> |</p> <p>| ------------ | :----------------------------------------------------------: | ---------------------------------- | ------------------------------------------------------------ |</p> <p>| TCP | 80 | HTTP Traffic | The machines that run the Ingress router pods, compute, or worker by default. |</p> <p>| 443 | HTTPs traffic | | |</p> <p>| 2379-2380 | etcd server, peer and metrics ports ```` | | |</p> <p>| 6443 | Kubernetes API | The Bootstrap machine and masters. | |</p> <p>| 9000-9999 | Host level services, including the node exporter on ports 9100-9101 and the Cluster Version Operator on port 9099. | | |</p> <p>| 10249-10259 | The default ports that Kubernetes reserves | | |</p> <p>| 10256 | openshift-sdn | | |</p> <p>| 22623 | Machine Config Server | The Bootstrap machine and masters. | |</p> <p>| UDP | 4789 | VXLAN and GENEVE | |</p> <p>| 6081 | VXLAN and GENEVE | | |</p> <p>| 9000-9999 | Host level services, including the node exporter on ports 9100-9101. | | |</p> <p>| 30000-32767 | Kubernetes NodePort | | |</p> <p>For more information on the network port requirements for Red Hat OpenShift 4, refer to the documentation from Red Hat at</p> <p><a href="https://docs.openshift.com/container-platform/4.3/installing/installing_bare_metal/installing-bare-metal.html#installation-network-user-infra_installing-bare-metal" target="_blank" rel="noopener noreferrer">https://docs.openshift.com/container-platform/4.3/installing/installing_bare_metal/installing-bare-metal.html#installation-network-user-infra_installing-bare-metal<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.[]{#_OpenShift_Manifests_and .anchor}</p> <h1 id="preparing-the-execution-environment"><a href="#preparing-the-execution-environment" class="header-anchor">#</a> Preparing the execution environment</h1> <p>This section provides a detailed overview and steps to configure the components deployed for this solution.</p> <h2 id="non-root-user-access"><a href="#non-root-user-access" class="header-anchor">#</a> Non-root user access</h2> <p>The industry-wide security best practice to avoid the use of root user account for administration of Linux based servers. However, certain operations require root user privileges to perform tasks. In those cases, it is best to use the sudo command to obtain the necessary privilege escalation on a short-term basis. The sudo command allows programs and commands to be run with the security privileges of another user (Root is the default user) and can restrict the permissions to specific groups, users, and individual commands.</p> <p>The root user is not active by default in RHCOS. Instead, log in as the core user.</p> <p>Use the following steps to create a non-root user for the OpenShift installation process:</p> <ol><li><p>Login to the installer VM as root. Refer to the section [Installer machine] in this document for more details about the installer VM.</p></li> <li><p>Execute the following command to create a new user.</p></li></ol> <blockquote><p>```</p> <p># adduser openshift_admin</p> <p>```</p></blockquote> <ol><li>Execute the following command to set password for the new user.</li></ol> <blockquote><p>```</p> <p># passwd openshift_admin</p> <p>```</p></blockquote> <ol><li>Edit the sudoers file and add the entry of new user in the sudoers file using the following command.</li></ol> <blockquote><p>```</p> <p># visudo</p> <p>```</p></blockquote> <p>An example of sudoers file is as follows.</p> <blockquote><p>## Allow root to run any commands anywhere</p> <p>root ALL=(ALL) ALL</p> <p>openshift_admin ALL=(ALL) NOPASSWD:ALL</p> <p>## Allow members of group sudo to execute any command</p> <p>%sudo ALL=(ALL:ALL) NOPASSWD: ALL</p> <p>```</p></blockquote> <ol><li>Execute the following command to change the user.</li></ol> <blockquote><p>```</p> <p># su openshift_admin</p> <p>```</p></blockquote> <h2 id="installer-machine"><a href="#installer-machine" class="header-anchor">#</a> Installer machine</h2> <p>This document assumes that a server running Red Hat Enterprise Linux 7.6 exists within the deployment environment and is accessible to the installation user to be used as an installer machine. This server must have internet connectivity. In this solution, a virtual machine is used to act as an installer machine and the same host is utilized as an Ansible Engine host.</p> <p>Log in to the installer VM as non-root user and perform the following steps:</p> <ol><li>Register the host and attach the host pool with Red Hat by executing the following command.</li></ol> <p>```</p> <blockquote><p># sudo subscription-manager register –username=&lt;username&gt; --password=&lt;password&gt; --auto-attach</p></blockquote> <p>```</p> <ol><li>Disable all repositories and enable only the repositories required for the installer VM.</li></ol> <p>```</p> <p># sudo yum-config-manager --disable \*</p> <p># sudo subscription-manager repos --disable=&quot;*&quot; \</p> <p>--enable=&quot;rhel-7-server-rpms&quot; \</p> <p>--enable=&quot;rhel-7-server-extras-rpms&quot;</p> <p>```</p> <ol><li>Execute the following commands to download the hpe-solutions-openshift repository.</li></ol> <blockquote><p><em>```</em></p></blockquote> <p># cd /etc/ansible</p> <p># git clone <a href="https://github.com/HewlettPackard/hpe-solutions-openshift.git" target="_blank" rel="noopener noreferrer">https://github.com/HewlettPackard/hpe-solutions-openshift.git<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <blockquote><p>*``` *</p></blockquote> <ol><li>After the hpe-solutions-openshift repository is downloaded, navigate to the path <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/playbooks</em>. The scripts within this directory assists in configuring the prerequisites for the environment. The details of the scripts are as follows:</li></ol> <ul><li><p>python_env.sh – this script installs Python 3.</p></li> <li><p>ansible_env.sh – this script creates a Python 3 virtual environment and installs Ansible within the virtual environment.</p></li> <li><p>download_oneview_packages.sh – this script installs the prerequisite modules such as HPE oneview-ansible, HPE oneview-python and VMware pyVmomi within the virtual environment.</p></li></ul> <ol><li>Steps to configure the prerequisite environment are as follows:</li></ol> <ul><li><p>Change the directory to /etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/playbooks</p> <p>```</p> <p># cd /etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/playbooks</p> <p>```</p></li> <li><p>Execute the following command to setup prerequisite Python environment.</p> <p>```</p> <p># sudo sh python_env.sh</p> <p>```</p></li> <li><p>Execute the following command to enable Python3.</p> <p>```</p> <p># scl enable rh-python36 bash</p> <p>```</p></li> <li><p>Execute the following command to configure the Ansible environment.</p> <p>```</p> <p># sudo sh ansible_env.sh</p> <p>```</p></li> <li><p>Execute the following command to download the HPE OneView packages.</p> <p>```</p> <p># sudo sh download_oneview_packages.sh</p> <p>```</p></li> <li><p>Enable the virtual environment with the following command.</p> <p>```</p> <p># source ../ocp_venv/bin/activate</p> <p>```</p></li> <li><p>Execute the following command to set the environment variables.</p> <p>```</p> <p># export ANSIBLE_LIBRARY=/etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/library/oneview-ansible/library</p> <p># export ANSIBLE_MODULE_UTILS=/etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/library/oneview-ansible/library/module_utils</p> <p>```</p> <p>[]{#_Installer_configuration_to .anchor}</p></li></ul> <h3 id="openshift-inventory-file"><a href="#openshift-inventory-file" class="header-anchor">#</a> OpenShift inventory file</h3> <p>The files that are cloned from the GitHub site include a sample inventory file. The installation user should review this file (located on the installer VM at <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/infrastructure/hosts</em>) and ensure that the information within the file accurately reflects the information in their environment.</p> <p>Use an editor such as vim or nano to edit the inventory file.</p> <p>```</p> <p># vim /etc/ansible/hpe-solutions-openshift/synergy/scalable/infrastructure/hosts</p> <p>```</p> <p><strong>NOTE</strong></p> <p>The values provided in the variable files, inventory files, figures, and tables in this document are intended to be used for reference purposes. It is expected that the installation user updates them to suit the local environment.</p> <h3 id="ansible-vault"><a href="#ansible-vault" class="header-anchor">#</a> Ansible Vault</h3> <p>A preconfigured Ansible vault file (<em>secret.yml</em>) is provided as part of this solution which consists of sensitive information to support the host and virtual machine deployment.</p> <p>Run the following command on the installer VM to edit the vault to match the installation environment.</p> <p>```</p> <p># ansible-vault edit /etc/ansible/hpe-solutions-openshift/synergy/scalable/infrastructure/secret.yml</p> <p>```</p> <p>**NOTE **</p> <p>The default password for the Ansible vault file is <em>changeme.</em></p> <h2 id="kubernetes-manifests-and-ignition-files"><a href="#kubernetes-manifests-and-ignition-files" class="header-anchor">#</a> Kubernetes manifests and ignition files</h2> <p>Manifests and ignition files define the master node and worker node configurations and are key components of the Red Hat OpenShift Container Platform 4 installation.</p> <p>Before creating the manifest files and ignition files, it is necessary to download the Red Hat OpenShift 4 packages. Execute the following command on the installer VM to download the required packages.</p> <blockquote><p>```</p> <p># cd /etc/ansible/hpe-solutions-openshift/synergy/scalable/installer</p> <p># ansible-playbook playbooks/download_ocp_package.yml</p> <p>```</p></blockquote> <p>The OpenShift packages downloaded after executing the <em>download_ocp_package.yml</em> playbook can be found on the installer VM at <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/library/openshift_components</em>. To execute any OpenShift related adhoc commands, it is advised to execute them from within this folder.</p> <p>To create the manifest files and the ignition files, edit the <em>install-config.yaml</em> file provided in the directory <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/ignitions</em> to include the following details:</p> <ul><li><p>baseDomain - Base domain of the DNS which hosts Red Hat OpenShift Container Platform.</p></li> <li><p>name – Name for the OpenShift cluster. This is same as the new domain created in DNS.</p></li> <li><p>replicas – Update this field to reflect the corresponding number of master or worker instances required for the OpenShift cluster as per the installation environment requirements. It is recommended to have a minimum of 3 master nodes and 2 worker nodes per OpenShift cluster.</p></li> <li><p>clusterNetworks – This field is pre-populated by Red Hat. Update this field only if a custom cluster network is to be used.</p></li> <li><p>pullSecret – Update this field with the pull secret for the Red Hat account. Login to Red Hat account <a href="https://cloud.redhat.com/openshift/install/metal/user-provisioned" target="_blank" rel="noopener noreferrer">https://cloud.redhat.com/openshift/install/metal/user-provisioned<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> and retrieve the pull secret.</p></li> <li><p>sshKey – Update this field with the sshKey of the installer VM. Generate the SSH key with following command.</p></li></ul> <blockquote><p>```</p> <p># ssh-keygen</p> <p>```</p></blockquote> <p>An example <em>install-config.yaml</em> file appears below. Update the fields to suit the installation environment.</p> <blockquote><p>```</p> <p>apiVersion: v1</p> <p>baseDomain: &lt;name of the base domain&gt;</p> <p>compute:</p> <ul><li>hyperthreading: Enabled</li></ul> <p>name: worker</p> <p>replicas: 2</p> <p>controlPlane:</p> <p>hyperthreading: Enabled</p> <p>name: master</p> <p>replicas: 3</p> <p>metadata:</p> <p>name: &lt;name of the cluster, same as the new domain under the base domain created&gt;</p> <p>networking:</p> <p>clusterNetworks:</p> <ul><li>cidr: 12.128.0.0/14</li></ul> <p>hostPrefix: 23</p> <p>networkType: OpenShiftSDN</p> <p>serviceNetwork:</p> <ul><li>172.30.0.0/16</li></ul> <p>platform:</p> <p>none: {}</p> <p>pullSecret: ‘pull secret provided as per the Red Hat account’</p> <p>sshKey: ‘ ssh key of the installer VM ’</p> <p>```</p></blockquote> <p>Execute the following command on the installer VM to create the manifest files and the ignition files required to install Red Hat OpenShift.</p> <blockquote><p>```</p> <p># cd /etc/ansible/hpe-solutions-openshift/synergy/scalable/installer</p> <p># ansible-playbook playbooks/create_manifest_ignitions.yml</p> <p>```</p></blockquote> <p>The ignition files are generated on the installer VM within the folder <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/ignitions</em>.</p> <p><strong>NOTE</strong></p> <p>The ignition files have a time-out period of 24 hours and it is critical that the clusters are created within 24 hours of generating the ignition files. If 24 hours is passed, then regenerate the ignition files again by clearing up the files from the directory where the ignition files were saved.</p> <h2 id="pxe-server"><a href="#pxe-server" class="header-anchor">#</a> PXE Server</h2> <p>In this solution, a PXE Server is used for OS deployment and is configured on CentOS (version: CentOS Linux release 7.6.1810 (Core)). The PXE server uses the FTP service for file distribution but can be altered to support HTTP or NFS.</p> <p>This section highlights the steps to configure a PXE server:</p> <ol><li><p>Login to the CentOS server to be configured as a PXE server as a user that can run commands as root via sudo.</p></li> <li><p>Install packages such as DHCP, TFTP-server, vSFTPD (FTP server) and xinetd using the following command.</p></li></ol> <blockquote><p>```</p> <p># sudo yum install dhcp tftp tftp-server syslinux vsftpd xinetd</p> <p>```</p></blockquote> <ol><li>Update the DHCP configuration file at <em>/etc/dhcp/dhcpd.conf</em> with the MAC addresses, IP addresses, DNS, and routing details of the installation environment. Domain search is optional. A sample DHCP configuration file is shown as follows.</li></ol> <p>```</p> <blockquote><p>ddns-update-style interim;</p> <p>ignore client-updates;</p> <p>authoritative;</p> <p>allow booting;</p> <p>allow bootp;</p> <p># internal subnet for my DHCP Server</p> <p>subnet 20.0.x.x netmask 255.0.0.0 {</p> <p>range 20.0.x.x 20.0.x.x;</p> <p>deny unknown-clients;</p> <p>option domain-name-servers 20.x.x.x;</p> <p>option domain-name &quot;twentynet.local&quot;;</p> <p>option routers 20.x.x.x;</p> <p>option broadcast-address 20.255.255.255;</p> <p>default-lease-time 600;</p> <p>max-lease-time 7200;</p> <p>next-server 20.x.x.x;</p> <p>filename &quot;pxelinux.0&quot;;</p> <p>}</p> <p>#######################################</p> <p>host bootstrap {</p> <p>hardware ethernet 00:50:56:xx:98:df;</p> <p>fixed-address 20.0.x.x;</p> <p>}</p> <p>host master01 {</p> <p>hardware ethernet 00:50:56:95:xx:82;</p> <p>fixed-address 20.0.x.x;</p> <p>}</p> <p>host worker01 {</p> <p>hardware ethernet 00:50:56:xx🆎82;</p> <p>fixed-address 20.0.x.x;</p> <p>}</p></blockquote> <p>```</p> <ol><li>Trivial File Transfer Protocol (TFTP) is used to transfer files from data server to clients without any kind of authentication. TFTP is used for ignition file loading in PXE based environments. To configure the TFTP server, edit the configuration file <em>/etc/xinetd.d/tftp</em>. Change the parameter ‘disable = yes’ to ‘disable = no’ and leave the other parameters as is. To edit the <em>/etc/xinetd.d/tftp</em> file, execute the following command.</li></ol> <blockquote><p>```</p> <p># sudo vi  /etc/xinetd.d/tftp</p> <p>```</p></blockquote> <p>The TFTP configuration file is shown below.</p> <p>```</p> <p>service tftp</p> <p>{</p> <p>socket_type = dgram</p> <p>protocol = udp</p> <p>wait = yes</p> <p>user = root</p> <p>server = /usr/sbin/in.tftpd</p> <p>server_args = -s /var/lib/tftpboot</p> <p>disable = no</p> <p>per_source = 11</p> <p>cps = 100 2</p> <p>flags = IPv4</p> <p>}</p> <p>```</p> <blockquote><p>Network boot related files must be placed in the tftp root directory <em>/var/lib/tftpboot</em>. Run the following commands to copy the required network boot files to <em>/var/lib/tftpboot/</em>.</p></blockquote> <p>```</p> <p># sudo cp –v /usr/share/syslinux/pxelinux.0 /var/lib/tftpboot</p> <p># sudo cp –v /usr/share/syslinux/menu.c32 /var/lib/tftpboot</p> <p># sudo cp –v /usr/share/syslinux/memdisk /var/lib/tftpboot</p> <p># sudo cp –v /usr/share/syslinux/mboot.c32 /var/lib/tftpboot</p> <p># sudo cp –v /usr/share/syslinux/chain.c32 /var/lib/tftpboot</p> <p># sudo mkdir /var/lib/tftpboot/pxelinux.cfg</p> <p># sudo mkdir /var/lib/tftpboot/networkboot</p> <p>```</p> <ol><li>Copy the RHCOS 4 and RHEL 7.6 ISO files to the PXE server. Mount it to the <em>/mnt/</em> directory and then copy the contents of the ISO to the local FTP server using the following commands.</li></ol> <p>```</p> <p># sudo mount –o loop &lt;OS file name&gt; /mnt/</p> <p># cd /mnt/</p> <p># sudo cp –av * /var/ftp/pub/</p> <p>```</p> <ol><li>Copy the kernel file (vmlinuz) and initrd file from <em>/mnt</em> to <em>/var/lib/tftpboot/networkboot/</em> using the following commands.</li></ol> <p>```</p> <p># sudo cp /mnt/images/pxeboot/vmlinuz /var/lib/tftpboot/networkboot/</p> <p># sudo cp /mnt/images/pxeboot/initrd.img /var/lib/tftpboot/networkboot</p> <p>```</p> <ol><li>Unmount the ISO files using the following command.</li></ol> <p>```</p> <p># sudo unmount /mnt/</p> <p>```</p> <ol><li>For RHEL nodes, create and utilize a new kickstart file under the folder <em>/var/ftp/pub</em> with the name “<em>rhel7.cfg</em>” using the following command.</li></ol> <p>```</p> <p># sudo vi /var/ftp/pub/rhel7.cfg</p> <p>```</p> <p>An example kickstart file is shown below. The installation user should create a kickstart file to meet the requirements of their installation environment.</p> <p>```</p> <p>firewall --disabled</p> <p># Install OS instead of upgrade</p> <p>install</p> <p># Use FTP installation media</p> <p>url --url=&quot;ftp://&lt;FTP_server_IP_address&gt;/pub/rhel76/&quot;</p> <p># Root password</p> <p># root password can be plaintext as shown below</p> <p># rootpw –plaintext &lt;password&gt;</p> <p># root password is encrypted using the command “openssl passwd -1 &lt;password&gt;” and resultant output is provided for rootpw as shown below</p> <p>rootpw --iscrypted $6$uiq8l/7xEWsYXhrvaEgan4N21yhLa8K.U7UA12Th3PD11GOXvEcI40gp</p> <p># System authorization information</p> <p>auth useshadow passalgo=sha512</p> <p># Use graphical install</p> <p>graphical</p> <p>firstboot disable</p> <p># System keyboard, timezone, language</p> <p>keyboard us</p> <p>timezone Europe/Amsterdam</p> <p>lang en_US</p> <p># SELinux configuration</p> <p>selinux disabled</p> <p># Installation logging level</p> <p>logging level=info</p> <p># System bootloader configuration</p> <p>bootloader location=mbr</p> <p>clearpart --all --initlabel</p> <p>part swap --asprimary --fstype=&quot;swap&quot; --size=1</p> <p>part /boot --fstype xfs --size=300</p> <p>part pv.01 --size=1 --grow</p> <p>volgroup root_vg01 pv.01</p> <p>logvol / --fstype xfs --name=lv_01 --vgname=root_vg01 --size=1 --grow</p> <p>%packages</p> <p>@^minimal</p> <p>@core</p> <p>%end</p> <p>%addon com_redhat_kdump --disable --reserve-mb='auto'</p> <p>%end</p> <p>```</p> <ol><li>Create a PXE menu:</li></ol> <ul><li>Create a PXE menu file at the location <em>/var/lib/tftpboot/pxelinux.cfg/default</em> using the command.</li></ul> <blockquote><p>```</p> <p># sudo vi /var/lib/tftpboot/pxelinux.cfg/default</p> <p>```</p></blockquote> <ul><li><p>For each of the OS boot options, provide the following details:</p> <ul><li><p>MENU LABEL – Custom name of the respective menu label.</p></li> <li><p>KERNEL – Kernel details of the operating system.</p></li> <li><p>APPEND - Path of bootloader file along with path of cfg or ignition files (in case of RHCOS) or configuration file (in case of RHEL).</p></li></ul></li></ul> <blockquote><p>A sample PXE menu is shown below.</p> <p>```</p></blockquote> <p>default menu.c32</p> <p>prompt 0</p> <p>timeout 30</p> <p>MENU TITLE LinuxTechi.com PXE Menu</p> <p>LABEL rhel76</p> <p>MENU LABEL RHEL76-Buedata</p> <p>KERNEL /rhel76/vmlinuz</p> <p>APPEND initrd=/rhel76/initrd.img inst.repo=ftp://&lt;FTP_server_IP_address&gt;/pub/rhel76 ks=ftp://&lt;FTP_server_IP_address&gt;/pub/rhel76-hcp.cfg</p> <p>LABEL rhcos-bootstrap</p> <p>MENU LABEL Install RHCOS4.3 sec-Bootstrap</p> <p>KERNEL /networkboot/rhcos-4.3.0-x86_64-installer-kernel</p> <p>APPEND ip=dhcp rd.neednet=1 initrd=/networkboot/rhcos-4.3.0-x86_64-installer-initramfs.img console=tty0 console=ttyS0 coreos.inst=yes coreos.inst.install_dev=sda coreos.inst.image_url= ftp://&lt;FTP_server_IP_address&gt;/pub/rhcos-4.3.0-x86_64-metal-bios.raw.gz coreos.inst.ignition_url= ftp://&lt;FTP_server_IP_address&gt;/pub/sec/bootstrap.ign</p> <p>LABEL rhcos-master</p> <p>MENU LABEL Install RHCOS4.2 sec-Master</p> <p>KERNEL /networkboot/rhcos-4.3.0-x86_64-installer-kernel</p> <p>APPEND ip=dhcp rd.neednet=1 initrd=/networkboot/rhcos-4.3.0-x86_64-installer-initramfs.img console=tty0 console=ttyS0 coreos.inst=yes coreos.inst.install_dev=sda coreos.inst.image_url= ftp://&lt;FTP_server_IP_address&gt;/pub/rhcos-4.3.0-x86_64-metal-bios.raw.gz coreos.inst.ignition_url=ftp://&lt;FTP_server_IP_address&gt;/pub/sec/master.ign</p> <p>LABEL rhcos-worker</p> <p>MENU LABEL Install RHCOS4.2 sec-Worker</p> <p>KERNEL /networkboot/rhcos-4.3.0-x86_64-installer-kernel</p> <p>APPEND ip=dhcp rd.neednet=1 initrd=/networkboot/rhcos-4.3.0-x86_64-installer-initramfs.img console=tty0 console=ttyS0 coreos.inst=yes coreos.inst.install_dev=sda coreos.inst.image_url= ftp://&lt;FTP_server_IP_address&gt;/pub/rhcos-4.3.0-x86_64-metal-bios.raw.gz coreos.inst.ignition_url=ftp://&lt;FTP_server_IP_address&gt;/pub/sec/worker.ign</p> <p>```</p> <ol><li>Start and enable xinetd, dhcpd and vsftpd using the following commands.</li></ol> <p>```</p> <p># sudo systemctl start xinetd</p> <p># sudo systemctl enable xinetd</p> <p># sudo systemctl start dhcpd.service</p> <p># sudo systemctl enable dhcpd.service</p> <p># sudo systemctl start vsftpd</p> <p># sudo systemctl enable vsftpd</p> <p>```</p> <ol><li><p>Configure SELinux for FTP.</p> <p>```</p></li></ol> <blockquote><p># sudo setsebool –P allow_ftpd_full_access 1</p></blockquote> <p>```</p> <ol><li>Open ports in the firewall using the following firewall-cmd commands.</li></ol> <p>```</p> <blockquote><p># sudo firewall-cmd --add-service-ftp --permanent</p></blockquote> <p># sudo firewall-cmd --add-service-dhcp --permanent</p> <p># sudo firewall-cmd –reload</p> <p>```</p> <p><strong>NOTE</strong></p> <p>It is crucial to generate ignition files, copy them to the TFTP server, and update the path in the PXE default file. For more information about generating the ignition files, refer to the section [Kubernetes manifests and ignition files] in this document.</p> <h2 id="load-balancer"><a href="#load-balancer" class="header-anchor">#</a> Load balancer</h2> <p>Red Hat OpenShift Container Platform 4 uses an external load balancer to communicate from outside the cluster with services running inside the cluster. This section assumes that there is a load balancer available within the deployment environment and is available for use. This solution was developed using <strong>HA Proxy</strong>, an open source solution with one (1) virtual machine for load balancing functionality. This section covers its configuration. In a production environment, Hewlett Packard Enterprise recommends the use of enterprise load balancing such as F5 Networks Big-IP and its associated products.</p> <p>The following entries are made in the haproxy.cfg file.</p> <p>```</p> <p>Sample haproxy.cfg file</p> <p>#---------------------------------------------------------------------</p> <p># static backend for serving up images, stylesheets and such</p> <p>#---------------------------------------------------------------------</p> <p>#backend static</p> <p># balance roundrobin</p> <p># server static 127.0.0.1:4331 check</p> <p>#---------------------------------------------------------------------</p> <p># round robin balancing between the various backends</p> <p>#---------------------------------------------------------------------</p> <p>backend ocp4-kubernetes-api-server</p> <p>mode tcp</p> <p>balance source</p> <p>server bootstrap bootstrap.ocp.pxelocal.local:6443 check</p> <p>server master01 master01.ocp.pxelocal.local:6443 check</p> <p>server master02 master02.ocp.pxelocal.local:6443 check</p> <p>server master03 master03.ocp.pxelocal.local:6443 check</p> <p>backend ocp4-machine-config-server</p> <p>balance source</p> <p>mode tcp</p> <p>server bootstrap bootstrap.ocp.pxelocal.local:22623 check</p> <p>server master01 master01.ocp.pxelocal.local:22623 check</p> <p>server master02 master02.ocp.pxelocal.local:22623 check</p> <p>server master03 master03.ocp.pxelocal.local:22623 check</p> <p>backend ocp4-router-http</p> <p>balance source</p> <p>mode tcp</p> <p>server worker03 worker03.ocp.pxelocal.local:80 check</p> <p>server worker04 worker04.ocp.pxelocal.local:80 check</p> <p>server worker01 worker01.ocp.pxelocal.local:80 check</p> <p>server worker02 worker02.ocp.pxelocal.local:80 check</p> <p># if creating a cluster with only master nodes to begin with and later adding the worker nodes, master nodes should be added in this section instead of worker nodes. Once all the worker nodes are added into the cluster, this configuration needs to be updated with the worker nodes.</p> <p># server master01 master01.ocp.pxelocal.local:80 check</p> <p># server master02 master02.ocp.pxelocal.local:80 check</p> <p># server master03 master03.ocp.pxelocal.local:80 check</p> <p>backend ocp4-router-https</p> <p>balance source</p> <p>mode tcp</p> <p>server worker03 worker03.ocp.pxelocal.local:443 check</p> <p>server worker04 worker04.ocp.pxelocal.local:443 check</p> <p>server worker01 worker01.ocp.pxelocal.local:443 check</p> <p>server worker01 worker02.ocp.pxelocal.local:443 check</p> <p># if creating a cluster with only master nodes to begin with and later adding the worker nodes, master nodes should be added in this section instead of worker nodes. Once all the worker nodes are added into the cluster, this configuration needs to be updated with the worker nodes.</p> <p># server master01 master01.ocp.pxelocal.local:443 check</p> <p># server master02 master02.ocp.pxelocal.local:443 check</p> <p># server master03 master03.ocp.pxelocal.local:443 check</p> <p>```</p> <p><strong>NOTE</strong></p> <p>The load balancer configuration should contain values that are aligned to the installation environment.</p> <h2 id="user-provisioned-dns-requirements"><a href="#user-provisioned-dns-requirements" class="header-anchor">#</a> User-provisioned DNS requirements</h2> <p>This section covers the host entries that need to be made in the base domain to enable installation of Red Hat OpenShift Container Platform 4.</p> <p>Red Hat OpenShift Container Platform 4 uses three types of DNS records (A, CNAME, and SRV). The host names and their types are described in Table 7.</p> <p><strong>Table 7.</strong> DNS entries for Red Hat OpenShift Container Platform</p> <p><strong>| Hosts | DNS Record Types |</strong></p> <p>| --------------------------- | --------------------------- |</p> <p>| master_nodes | A |</p> <p>| worker_nodes | A |</p> <p>| bootstrap_nodes | A |</p> <p>| installer VM | A |</p> <p>| *, api, api-int, haproxy | A |</p> <p>| etcd | CNAME, SRV |</p> <p>To add the appropriate records, follow these steps:</p> <ol><li>‘A’ type resource record [ Host (A) ]. An A record specifies an IPv4 address. Example entries are shown in Table 8. Ensure that these entries are created for all the nodes in the installation environment.</li></ol> <p>**Table 8. ‘**A’ type DNS entry</p> <p>**|Host (A) | IP Address | Host name| **</p> <p>| --------------------------- | --------------------------- |</p> <p>|master_nodes |master_ip |master_name.cluster_name.baseDomain|</p> <p>|worker_nodes |worker_ip |worker_name.cluster_name.baseDomain|</p> <p>|bootstrap_nodes |bootstrap_ip |bootstrap_name.cluster_name.baseDomain|</p> <p>|installer VM |installer_ip |installer_name.cluster_name.baseDomain|</p> <p>|* |haproxy_ip |*.apps.cluster_name.baseDomain|</p> <p>|api |haproxy_ip |api.cluster_name.baseDomain|</p> <p>|api-int |haproxy_ip |api-int.cluster_name.baseDomain|</p> <p>|haproxy |haproxy_ip |haproxy_name.cluster_name.baseDomain|</p> <ol><li>Create a CNAME resource record [ Alias (CNAME) ] in DNS. Example entries are shown in Table 9. Ensure that these entries are created for all of the master nodes.</li></ol> <p><strong>Table 9.</strong> DNS entries for Red Hat OpenShift Container Platform</p> <p><strong>| Host (CNAME) | Target host name|</strong></p> <p>| --------------------------- | --------------------------- |</p> <p>| etcd-0 | <em>master01_name.cluster_name.baseDomain|</em></p> <p>| etcd-1 | <em>master02_name.cluster_name.baseDomain|</em></p> <p>| etcd-2 | <em>master03_name.cluster_name.baseDomain|</em></p> <p><strong>NOTE</strong></p> <p>Replace the italicized components in the examples above with the actual values that align to the installation environment.</p> <ol><li>For each master node, Red Hat OpenShift Container Platform also requires a Service Location (SRV) DNS record for the etcd server on that machine with priority 0, weight 10, and port 2380. The SRV record is used to identify computers that host specific services. Figure 5 shows the creation of an SRV record.</li></ol> <p>![Creating an SRV record](/figure5.png)</p> <p>![][4]</p> <p>Figure 5. Creating an SRV record</p> <h2 id="bootstrap-node"><a href="#bootstrap-node" class="header-anchor">#</a> Bootstrap node</h2> <p>A temporary bootstrap node is required for OpenShift cluster creation. This section assumes that a VMware vSphere host is present within the deployment environment and is associated with a VMware vCenter server. The host should be configured with appropriate storage and networking configurations.</p> <h3 id="playbooks-for-creating-the-bootstrap-node"><a href="#playbooks-for-creating-the-bootstrap-node" class="header-anchor">#</a> Playbooks for creating the bootstrap node</h3> <ol><li><strong>inputs.yml</strong>: This file contains input variables to create the bootstrap VM. Some of the variables pertaining to the VM configuration are provided with default values as per the Red Hat guidelines. It is expected that the installation user updates the values to suit their installation environment.</li></ol> <ul><li><p><strong>datacenter_name</strong>: name of the VMware data center.</p></li> <li><p><strong>cluster_name</strong>: name of the VMware cluster.</p></li> <li><p><strong>datastore_name</strong>: name of the VMware datastore.</p></li> <li><p><strong>network_name:</strong> name of the network associated with the vSphere host**.**</p></li> <li><p><strong>bootstrap_disk:</strong> disk size for the bootstrap node.</p></li> <li><p><strong>bootstrap_cpu:</strong> number of vCPUs for the bootstrap node.</p></li> <li><p><strong>bootstrap_name:</strong> custom name of the bootstrap node.</p></li></ul> <ol><li><p><strong>secret.yml</strong>: This is an Ansible vault file that contains sensitive information such as the VMware vCenter server IP address and credentials.</p></li> <li><p><strong>playbooks/deploy_vm.yml</strong>: This playbook is used to create the bootstrap VM.</p></li> <li><p><strong>roles/deploy_vm.yml</strong>: This is the Ansible role file that is required to create the bootstrap VM. Each role is associated with a set of tasks to accomplish the expected output and they are present in the tasks directory within the role.</p></li></ol> <p>Follow these steps to create the bootstrap node:</p> <ol><li><p>Login to the installer VM.</p></li> <li><p>Change the directory using the following command.</p></li></ol> <blockquote><p>```</p> <p># cd /etc/ansible/hpe-solutions-openshift/synergy/scalable/infrastructure</p> <p>```</p></blockquote> <ol><li>Update the <em>secret.yml</em> to provide the details of the VMware vCenter server using the following command. A sample input is provided, and it is expected that the installation user updates the configuration to suit the deployment environment.</li></ol> <blockquote><p>```</p> <p># ansible-vault edit secret.yml</p> <p>```</p> <p>```</p> <p># vcenter hostname/ip address and credentials</p> <p>vcenter_hostname: &lt;vcenter hostname&gt;</p> <p>vcenter_username: &lt;vcenter username&gt;</p> <p>vcenter_password: &lt;vcenter password&gt;</p> <p>```</p></blockquote> <ol><li>Update the <em>input.yml</em> file with the data center, cluster, and datastore information that will be used within the VMware vCenter server. This file provides default configuration information for the bootstrap node, if required. The configuration can be updated to suit the environment needs.</li></ol> <blockquote><p>```</p> <p># Variables for creating the bootstrap VM, as per the vSphere host configuration within the vCenter</p> <p>datacenter_name: &lt;name of data center within vcenter&gt;</p> <p>cluster_name: &lt;name of cluster within vcenter&gt;</p> <p>datastore_name: &lt;name of datastore within vcenter&gt;</p> <p>network_name: &lt;name of the network within vcenter&gt;</p> <p># Default values for creating the bootstrap VM</p> <p>bootstrap_disk: 150</p> <p>bootstrap_cpu: 4</p> <p>bootstrap_memory: 16400</p> <p>bootstrap_name: Bootstrap</p> <p>```</p></blockquote> <ol><li>After the <em>input.yml</em> and <em>secret.yml</em> files are updated with appropriate values, execute the playbook with the following command to create the bootstrap VM.</li></ol> <blockquote><p>```</p> <p># ansible-playbook –i hosts playbooks/deploy_vm.yml –ask-vault-pass</p> <p>```</p></blockquote> <p><strong>NOTE</strong></p> <p>It is essential that all the nodes within the deployment environment are synchronized for time using an NTP server. Failure to do so will result in an installation failure due to mismatches in certificates or other files with time dependencies.</p> <h1 id="physical-environment-configuration"><a href="#physical-environment-configuration" class="header-anchor">#</a> Physical environment configuration</h1> <h2 id="cabling-the-hpe-synergy-12000-frame-and-hpe-virtual-connect-40gb-se-f8-modules-for-hpe-synergy"><a href="#cabling-the-hpe-synergy-12000-frame-and-hpe-virtual-connect-40gb-se-f8-modules-for-hpe-synergy" class="header-anchor">#</a> Cabling the HPE Synergy 12000 Frame and HPE Virtual Connect 40Gb SE F8 Modules for HPE Synergy</h2> <p>This section shows the physical cabling between frames, Virtual Connect modules and solution switching. It is intended to provide an understanding of how the infrastructure was interconnected during testing and to serve as a guide on which the installation user can base their configuration.</p> <p>Figure 6 shows the cabling of the HPE Synergy Interconnects, HPE Synergy Frame Management, and Intelligent Resilient Fabric (IRF) connections. These connections handle east-west network communication as well as management traffic within the solution.</p> <p>![Inter-frame cabling](/figure6.png)</p> <p>![][5]</p> <p><strong>Figure 6.</strong> Cabling of the management and inter-frame communication links within the solution</p> <p>Figure 7 shows the cabling of HPE Synergy Frames to the network switches. The specific networks contained within the Bridge-Aggregation Groups (BAGG) are described in detail later in this document. At the lowest level, there are four (4) 40GbE connections dedicated to carry redundant, production network traffic to the first layer switch where it is further distributed.</p> <p>![Cabling of the interconnects to switching](/figure7.png)</p> <p>![][6]</p> <p><strong>Figure 7.</strong> Cabling of the HPE Synergy Interconnects to the HPE FlexFabric 5945 switches</p> <p>Table 10 explains the cabling of the Virtual Connect interconnect modules to the HPE FlexFabric 5945 switching.</p> <p><strong>Table 10.</strong> Networks used in this solution.</p> <p><strong>| Uplink Set | Synergy Source | Switch Destination</strong> |</p> <p>| ------------------- | ------------------- | ------------------ |</p> <p>| Network | Enclosure 1 Port Q3 | FortyGigE1/1/1 |</p> <p>| Enclosure 1 Port Q4 | FortyGigE2/1/1 | |</p> <p>| Enclosure 2 Port Q3 | FortyGigE1/1/2 | |</p> <p>| Enclosure 2 Port Q4 | FortyGigE2/1/2 | |</p> <h2 id="configuring-the-solution-switching"><a href="#configuring-the-solution-switching" class="header-anchor">#</a> Configuring the solution switching</h2> <p>The solution described in this document utilized HPE FlexFabric 5945 switches. The HPE FlexFabric 5945 switches are configured according to the configuration parameters found later in this section. Individual port configurations are described elsewhere in this section. The switches should be configured with an HPE Intelligent Resilient Framework (IRF). To understand the process of configuring IRF, refer the HPE FlexFabric 5945 Switch Series Installation Guide at <a href="https://support.hpe.com/hpsc/doc/public/display?sp4ts.oid=null&amp;docLocale=en_US&amp;docId=emr_na-c05212026" target="_blank" rel="noopener noreferrer">https://support.hpe.com/hpsc/doc/public/display?sp4ts.oid=null&amp;docLocale=en_US&amp;docId=emr_na-c05212026<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>. This guide may also be used to understand the initial installation of switching, creation of user accounts and access methods. The reminder of this section is built with the assumption that the switch has been installed, configured for IRF, hardened, and is accessible over SSH.</p> <p><strong>NOTE</strong></p> <p>The installation user might choose to utilize end of row switching to reduce switch and port counts in the context of the solution. If end of row switching is the approach, then this section should be used as a guidance on how to route network traffic outside of the HPE Synergy Frames.</p> <h3 id="physical-cabling"><a href="#physical-cabling" class="header-anchor">#</a> Physical cabling</h3> <p>Table 11 represents mapping of source ports to ports on the HPE FlexFabric 5945 switches.</p> <p><strong>Table 11.</strong> HPE FlexFabric 5945 port map</p> <p>| <strong>Source Port</strong> | <strong>Switch Port</strong> |</p> <p>| ----------------------------- | --------------- |</p> <p>| Virtual Connect Frame U30, Q3 | FortyGigE1/1/1 |</p> <p>| Virtual Connect Frame U30, Q4 | FortyGigE2/1/1 |</p> <p>| Virtual Connect Frame U30, Q5 | FortyGigE1/1/5 |</p> <p>| Virtual Connect Frame U30, Q6 | FortyGigE1/1/6 |</p> <p>| Virtual Connect Frame U40, Q3 | FortyGigE1/1/2 |</p> <p>| Virtual Connect Frame U40, Q4 | FortyGigE2/1/2 |</p> <p>| Virtual Connect Frame U40, Q5 | FortyGigE2/1/5 |</p> <p>| Virtual Connect Frame U40, Q6 | FortyGigE2/1/6 |</p> <p>| To Upstream Switching | Customer Choice |</p> <p>It is recommended that the installation user logs on to the switch post-configuration and provides a description for each of these ports.</p> <h3 id="network-definitions"><a href="#network-definitions" class="header-anchor">#</a> Network definitions</h3> <p>There are multiple networks defined at the switch layer in this solution:</p> <ul><li><p><strong>Synergy Management Network</strong> - This network is specific to the requirements of HPE Synergy.</p></li> <li><p><strong>Management Network –</strong> This network facilitates the management of hardware and software interfaced by IT.</p></li> <li><p><strong>Data Center Network</strong> – This network carries traffic from the overlay network used by the pods to external consumers of pod deployed services.</p></li></ul> <p>Table 12 defines the VLANs configured using HPE Synergy Composer in the creation of this solution. These networks should be defined at both the first layer switch and within Composer. This solution utilizes unique VLANs for the data center and solution management segments. Actual VLANs and network count will be determined by the requirements of your production environment.</p> <p><strong>Table 12.</strong> Networks used in this solution.</p> <p>| <strong>Network Function</strong> | <strong>VLAN Number</strong> | <strong>Bridge Aggregation Group</strong> |</p> <p>| ------------------- | ----------- | ------------------------ |</p> <p>| Synergy_Management | 193 | 111 |</p> <p>| Solution_Management | 1193 | 111 |</p> <p>| Data_Center | 2193 | 111 |</p> <ol><li>To add these networks to the switch, log on to the switch console over SSH and run the following commands.</li></ol> <blockquote><p>```</p> <p># sys</p> <p># vlan 193 1193 2193</p> <p>```</p></blockquote> <ol><li>For each of these VLANs, perform the following steps.</li></ol> <blockquote><p>```</p> <p># interface vlan-interface ####</p> <p># name VLAN Name per table above</p> <p># description Add text that describes the purpose of the VLAN</p> <p># quit</p> <p>```</p></blockquote> <p><strong>NOTE</strong></p> <p>It is strongly recommended to configure a dummy VLAN on the switches and assign unused ports to that VLAN.</p> <p>The switches should be configured with a bridge aggregation group (BAGG) for the different links to the HPE Synergy Frame connections. To configure the BAGG and ports as described in [Table 5], run the following commands.</p> <blockquote><p>```</p> <p># interface Bridge-Aggregation111</p> <p># link-aggregation mode dynamic</p> <p># description &lt;FrameNameU30&gt;-ICM</p> <p># quit</p> <p># interface range name &lt;FrameNameU30&gt;-ICM interface Bridge-Aggregation111</p> <p># quit</p> <p># interface range FortyGigE 1/1/1 to FortyGigE 1/1/2 FortyGigE 2/1/1 to FortyGigE 2/1/2</p> <p># port link-aggregation group 111</p> <p># quit</p> <p># interface range name &lt;FrameNameU30&gt;-ICM</p> <p># port link-type trunk</p> <p># undo port trunk permit vlan 1</p> <p># port trunk permit vlan 193 1193 2193</p> <p># quit</p> <p>```</p></blockquote> <ol><li>After the configuration of the switches is complete, save the state and apply it by typing <strong>save</strong> and follow the resulting prompts.</li></ol> <h2 id="hpe-synergy-480-gen10-compute-modules"><a href="#hpe-synergy-480-gen10-compute-modules" class="header-anchor">#</a> HPE Synergy 480 Gen10 Compute Modules</h2> <p>This section describes the connectivity of the HPE Synergy 480 Gen10 Compute Modules used in the creation of this solution. The HPE Synergy 480 Gen10 Compute Modules, regardless of function, were all configured identically. Table 13 describes the host configuration tested for this solution. Server configuration should be based on customer needs and the configuration used in the creation of this solution might not align with the requirements of any given production implementation.</p> <p><strong>Table 13.</strong> Host configuration.</p> <p>| <strong>Component</strong> | <strong>Quantity</strong> |</p> <p>| ------------------------------------------------------------ | --------------------- |</p> <p>| HPE Synergy 480/660 Gen10 Intel Xeon-Gold 6130 (2.1GHz/16-core/125W) FIO Processor Kit | 2 per server |</p> <p>| HPE 8GB (1x 8GB) Single Rank x8 DDR4-2666 CAS-19-19 Registered Smart Memory Kit | 20 per server |</p> <p>| HPE 16GB (1x 16GB) Single Rank x4 DDR4-2666 CAS-19-19 Registered Smart Memory Kit | 4 per server |</p> <p>| HPE Synergy 3820C 10/20Gb Converged Network Adapter | 1 per server |</p> <p>| HPE Smart Array P204i-c SR Gen10 12G SAS controller | 1 per server |</p> <p>| HPE 1.92TB SATA 6GB Mixed Use SFF (2.5in) 3yr Warranty Digitally Signed Firmware SSD | 2 per management host |</p> <h2 id="hpe-synergy-composer-2"><a href="#hpe-synergy-composer-2" class="header-anchor">#</a> HPE Synergy Composer 2</h2> <p>At the core of the management of the HPE Synergy environment is HPE Synergy Composer 2. A pair of HPE Synergy Composers are deployed across frames to provide redundant management of the environment for both initial deployment and changes over the lifecycle of the solution. HPE Synergy Composer 2 is used to configure the environment prior to the deployment of the operating systems and applications.</p> <p>This section walks the installation user through the process of installing and configuring the HPE Synergy Composer.</p> <h3 id="configure-the-hpe-synergy-composer-via-vnc"><a href="#configure-the-hpe-synergy-composer-via-vnc" class="header-anchor">#</a> Configure the HPE Synergy Composer via VNC</h3> <p>To configure HPE Synergy Composer with the user laptop, follow these steps:</p> <ol><li><p>Configure the laptop Ethernet port to the IP address 192.168.10.2/24*.* No gateway is required.</p></li> <li><p>Use a CAT5e cable to connect the laptop computer Ethernet port to laptop port on a front panel module of HPE Synergy Composer.</p></li> <li><p>Access the HPE Synergy Console using a web browser. Start a new browser session and enter <a href="http://192.168.10.1:5800" target="_blank" rel="noopener noreferrer">http://192.168.10.1:5800<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a><em>.</em></p></li> <li><p>Click <strong>Connect</strong> to start HPE OneView for Synergy from the HPE Synergy console.</p></li> <li><p>Click <strong>Hardware Setup</strong> to connect with <strong>Installation Technician</strong> user privileges.</p></li> <li><p>In the Appliance Network dialog box, fill in the following information</p></li></ol> <p>a.  Appliance host name: Enter <strong>a fully qualified name of the HPE Synergy Composer.</strong></p> <p>b.  Address assignment: <strong>Manual</strong></p> <p>c.  IP address: <strong>Enter an IP address on the management network</strong>.</p> <p>d.  Subnet mask or CIDR: <strong>Enter the subnet mask of the management network</strong>.</p> <p>e.  Gateway address: <strong>Enter the gateway for the management network</strong>.</p> <p>f.  Maintenance IP address 1: <strong>Enter a maintenance IP address on the management</strong> <strong>network</strong>.</p> <p>g.  Maintenance IP address 2: <strong>Enter a secondary maintenance IP</strong> <strong>address on the management network.</strong></p> <p>h.  Preferred DNS server: <strong>Enter the DNS server.</strong></p> <p>i.  IPv6 Address assignment: <strong>Unassign</strong></p> <ol><li><p>Click OK.</p></li> <li><p>When the hardware discovery process is complete, all HPE Synergy hardware including the Frames, Composer modules, Frame Link modules, Interconnect modules, Compute modules, and storage modules must be discovered and claimed by HPE OneView for Synergy.</p></li> <li><p>Review and correct any issues listed in the hardware setup checklist. The HPE Synergy 12000 Frame Setup and Installation Guide available at <a href="http://www.hpe.com/info/synergy-docs" target="_blank" rel="noopener noreferrer">http://www.hpe.com/info/synergy-docs<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> provides troubleshooting steps for common issues during hardware setup.</p></li> <li><p>Navigate to <strong>OneView</strong> -&gt; <strong>Settings</strong> and then click <strong>Appliance</strong>. Verify that the active and standby appliances show a status of <strong>Connected</strong>.</p></li></ol> <h3 id="configure-appliance-credentials"><a href="#configure-appliance-credentials" class="header-anchor">#</a> Configure appliance credentials</h3> <ol><li><p>Log in to the HPE OneView for Synergy Web Administration Portal, review and accept the License Agreement.</p></li> <li><p>On the HPE OneView Support Dialog box, verify that Authorized Service has a setting of Enabled. Click OK.</p></li> <li><p>Log in as <strong>Administrator</strong> with default password <strong>admin</strong>, set the new password to &lt;&lt;composer_administrator_password&gt;&gt; and click <strong>Ok</strong>.</p></li></ol> <h3 id="configure-solution-firmware"><a href="#configure-solution-firmware" class="header-anchor">#</a> Configure solution firmware</h3> <p>This solution adheres to the firmware recipe specified with the HPE Converged Solutions 750 specifications which can be found at CS750 Firmware and Software Compatibility Matrix. The solution used the latest firmware recipe available as of September of 2019 including HPE OneView for Synergy 5.0:</p> <ol><li><p>Select the <strong>OneView menu</strong> and select <strong>Settings.</strong></p></li> <li><p>Under Appliance, select <strong>Update Appliance</strong> and <strong>update Composer.</strong></p></li> <li><p>Once the update process completes, validate that both composer modules are connected and there is a <strong>green</strong> <strong>checkmark.</strong></p></li></ol> <h3 id="solution-configuration"><a href="#solution-configuration" class="header-anchor">#</a> Solution configuration</h3> <p>The installation user should utilize the Synergy Guided Setup to complete the following solution configuration details.</p> <h4 id="create-additional-users"><a href="#create-additional-users" class="header-anchor">#</a> Create additional users</h4> <p>It is recommended that you create a read-only user and an administrator account with a different username than administrator.</p> <h4 id="firmware"><a href="#firmware" class="header-anchor">#</a> Firmware</h4> <p>Upload a firmware bundle based on the HPE Converged Solutions 750 recipe. Once the bundle starts uploading, proceed to additional steps without disrupting the upload.</p> <h4 id="create-an-ip-pool-on-the-management-network"><a href="#create-an-ip-pool-on-the-management-network" class="header-anchor">#</a> Create an IP pool on the management network</h4> <p>Follow the guidance to create an IP pool on the management network. This IP pool will provide IP addresses to management IP’s and HPE device iLOs within the solution. Ensure that the pool is enabled prior to proceeding.</p> <h4 id="configure-ethernet-networks"><a href="#configure-ethernet-networks" class="header-anchor">#</a> Configure Ethernet networks</h4> <p>As explained in the [Network definitions] section of this document, the solution utilizes three (3) network segments. Refer to the Create networks section of the OneView Guided Setup wizard to define the networks shown in Table 14 at a minimum. Your VLAN values will generally differ from those described below.</p> <p><strong>Table 14.</strong> Networks defined within HPE Synergy Composer for this solution.</p> <p>| <strong>Network Name</strong> | <strong>VLAN Number</strong> | <strong>Purpose</strong> |</p> <p>| ----------------------| ----------- | --------------------------------------------------- |</p> <p>| Synergy_Management | 193 | Synergy management |</p> <p>| Management | Ethernet | 1193 | Solution management |</p> <p>| Data_Center | Ethernet | 2193 | Application, authentication and other user networks |</p> <p>The management network should be associated with the management network IP pool, which the user specified in the prior step. The installation user should create any additional required networks for the solution.</p> <h4 id="create-logical-interconnect-groups"><a href="#create-logical-interconnect-groups" class="header-anchor">#</a> Create Logical Interconnect Groups</h4> <p>Within Composer, use the Guided Setup to create a Logical Interconnect Group (LIG) with three (3) uplink sets defined. For this solution, the uplink sets are named Network. The uplink sets “Network” carries all other networks defined for the solution. Table 15 below defines the ports used to carry the uplink sets.</p> <p><strong>Table 15.</strong> Networks used in this solution</p> <p>| <strong>Uplink Set</strong> | <strong>Synergy Source</strong> |</p> <p>| --------------------------- | --------------------------- |</p> <p>| Network | Enclosure 1, Bay 3, Port Q3 |</p> <p>| | Enclosure 1, Bay 3, Port Q4 |</p> <p>| | Enclosure 2, Bay 6, Port Q3 |</p> <p>| | Enclosure 2, Bay 6, Port Q4 |</p> <h4 id="create-enclosure-group"><a href="#create-enclosure-group" class="header-anchor">#</a> Create Enclosure Group</h4> <ol><li><p>From the OneView Guided Setup, select Create <strong>enclosure group.</strong></p></li> <li><p>Provide a <strong>name</strong> and enter the <strong>number of frames</strong>.</p></li> <li><p>Select <strong>Use address pool</strong> and utilize the <strong>management pool</strong> defined earlier.</p></li> <li><p>Use the Logical Interconnect Group from the prior step in the creation of the Enclosure Group.</p></li> <li><p>Select <strong>Create</strong> when ready.</p></li></ol> <h4 id="create-logical-enclosure"><a href="#create-logical-enclosure" class="header-anchor">#</a> Create Logical Enclosure</h4> <p>Use the Guided Setup to create a logical enclosure making use of all three (3) enclosures. Select the firmware you uploaded earlier as a baseline. It can take some time for the firmware to update across the solution stack. Ensure that firmware complies with the baseline by selecting <strong>Actions</strong> and then <strong>Update</strong> <strong>Firmware</strong>. Click <strong>Cancel</strong> to exit.</p> <h3 id="configuring-solution-storage"><a href="#configuring-solution-storage" class="header-anchor">#</a> Configuring solution storage</h3> <p>The HPE Synergy D3940 Storage Module provides SSDs consumed by the Local Storage Operator to provide the persistent volume for the container workloads. If local disks are not installed within the HPE Synergy 480 Gen10 Compute Modules, the HPE Synergy D3940 Storage will also provide disks as boot volumes. Figure 8 describes the logical storage layout used in the solution. The HPE Synergy D3940 Storage Module provides SAS volumes.</p> <p>![Logical storage layout within the solution](/figure8.png)</p> <p>![][7]</p> <p><strong>Figure 8</strong>. Logical storage layout within the solution</p> <p>Table 16 lists all volumes used within the solution and highlights what storage provides the capacity and performance of each function.</p> <p><strong>Table 16.</strong> Volumes used in this solution</p> <p><strong>| Volume/Disk Function | Qty | Size | Source | Hosts | Shared/Dedicated</strong> |</p> <p>| --------------------------- | --- | --- | --- | --- | --------------------------- |</p> <p>| Local volume | 3 | 960GB | HPE Synergy D3940 storage | OpenShift worker nodes | dedicated |</p> <p>| Operating System (optional) | 6 | 300GB | HPE Synergy D3940 storage | All nodes | dedicated |</p> <h2 id="server-profiles"><a href="#server-profiles" class="header-anchor">#</a> Server Profiles</h2> <p>Server profiles are used to configure the personality of the compute resources. A server profile allows a set of configuration parameters, including firmware recipe, network and SAN connectivity, BIOS tuning, boot order configuration, local storage configuration, and more to be templatized. These templates are the key to delivering the “infrastructure as code” capabilities of the HPE Synergy platform. For the purpose of this solution, a template is created which can be leveraged for OpenShift master nodes and OpenShift worker nodes.</p> <h3 id="playbooks-for-creating-the-server-profile"><a href="#playbooks-for-creating-the-server-profile" class="header-anchor">#</a> Playbooks for creating the Server Profile</h3> <p>This repository contains Ansible scripts to automate the creation of server profiles in HPE OneView. The scripts are as follows:</p> <ol><li><p><strong>inputs.yml</strong>: This file contains input variables to create the server profile template and server profile.</p> <ul><li><p><strong>enclosure_group</strong>: name of the enclosure group as present in HPE OneView.</p></li> <li><p><strong>deployment_network_name</strong>: name of the network over which the OpenShift Container Platform solution must be deployed.</p></li> <li><p><strong>server_profile_template_name</strong>: custom name of the server profile template.</p></li></ul></li> <li><p><strong>hosts</strong>: This is the inventory file which will be used by the OpenShift installer VM to reference nodes for which the server profile needs to be created.</p> <ul><li><p><strong>type</strong>: type of the HPE Synergy server.</p></li> <li><p><strong>name</strong>: custom name for the server profile</p></li></ul></li> <li><p><strong>secret.yml</strong>: This is an Ansible vault file that consists of sensitive information, such as HPE OneView IP address, credentials, VMware vCenter server IP address and its credentials.</p></li> <li><p><strong>playbooks</strong>: This folder consists of playbooks to create the server profile template and server profile.</p></li> <li><p><strong>roles</strong>: This folder consists of Ansible roles to create the server profile template and server profile. Each role is associated with a set of tasks to accomplish the expected output and they are present in the tasks directory within the role.</p></li></ol> <p><strong>NOTE</strong></p> <p>The parameter names are case sensitive. Ensure that parameter names and functions are accurately recorded for the installation environment. Any variation in parameter names and functions will result in the failure of automated configuration steps.</p> <p>Update the inventory file found at <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/infrastructure</em> directory within the installer VM to include server hardware details. An example is provided below. The installation user should update the values to suit their environment.</p> <blockquote><p>```</p> <p>[server_profile_template]</p> <p>&quot;Frame01, bay 1&quot; type=&quot;SY 480 Gen10 1&quot;</p> <p>[server_profile]</p> <p>&quot;Frame03, bay 2&quot; type=&quot;SY 480 Gen10 1&quot; name=&quot;Master01&quot;</p> <p>&quot;Frame02, bay 3&quot; type=&quot;SY 480 Gen10 3&quot; name=&quot;Master02&quot;</p> <p>&quot;Frame01, bay 4&quot; type=&quot;SY 480 Gen10 2&quot; name=&quot;Master03&quot;</p> <p># if intended to use physical worker nodes, provide the server details in the following entries and uncomment them.</p> <p>#&quot;Frame01, bay 2&quot; type=&quot;SY 480 Gen10 1&quot; name=&quot;Worker01&quot;</p> <p>#&quot;Frame02, bay 3&quot; type=&quot;SY 480 Gen10 3&quot; name=&quot;Worker02&quot;</p> <p>#&quot;Frame03, bay 4&quot; type=&quot;SY 480 Gen10 2&quot; name=&quot;Worker03&quot;</p> <p>```</p></blockquote> <p>Update the <em>input.yml</em> file found at */etc/ansible/hpe-solutions-openshift/synergy/scalable/infrastructure. *</p> <blockquote><p>```</p> <p># Variables for creating the SPT and SP as per the OneView configuration</p> <p>enclosure_group: EG</p> <p>deployment_network_name: TwentyNet</p> <p>server_profile_template_name: openshift_spt</p> <p>```</p></blockquote> <p>Update the <em>secret.yml</em> found at <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/infrastructure.</em></p> <blockquote><p>```</p> <p># oneview and image streamer ip address and credentials.</p> <p>oneview_ip: x.x.x.x</p> <p>oneview_username: &lt;username&gt;</p> <p>oneview_password: &lt;password&gt;</p> <p>```</p></blockquote> <p><strong>WARNING</strong></p> <p>Before executing the playbooks, <em>deploy_server_profile_template.yml</em> and <em>deploy_server_profile.yml</em>. Ensure that the server hardware provided in the inventory file is powered off and does not have a server profile associated with it.</p> <p>After the inventory and variable files are updated with the appropriate values, execute the following commands on the installer VM to create the server profile template and server profile.</p> <p>```</p> <p># cd /etc/ansible/hpe-solutions-openshift/synergy/scalable/infrastructure</p> <p># ansible-playbook -i hosts playbooks/deploy_server_profile_template.yml --ask-vault-pass</p> <p># ansible-playbook -i hosts playbooks/deploy_server_profile.yml --ask-vault-pass</p> <p>```</p> <h3 id="hpe-synergy-storage"><a href="#hpe-synergy-storage" class="header-anchor">#</a> HPE Synergy storage</h3> <p>[]{#_Red_Hat_Local .anchor}To configure persistent volume for the OpenShift cluster, it is required to provide local storage to the worker nodes. This section describes how to add D3940 disk to the nodes.</p> <ol><li><p>Login to the <strong>HPE OneView</strong> console.</p></li> <li><p>Navigate to <strong>Server Profile</strong>.</p></li> <li><p>Select the server profile associated with the worker nodes and click <strong>Actions</strong> and select <strong>Edit</strong>.</p></li> <li><p>Navigate to the section <strong>Local Storage</strong> and select the <strong>SAS Storage Controller.</strong> Click <strong>Edit</strong>.</p></li> <li><p>Click <strong>Add Logical Drive</strong> and provide the values to the following parameters and then click <strong>OK</strong>. Add a minimum of 2 logical drives.</p> <p>a.  <strong>Name</strong>: Provide a name.</p> <p>b.  <strong>RAID Level</strong>: Select the applicable RAID level.</p> <p>c.  <strong>Drive type</strong>: Select the applicable drive type.</p> <p>d.  <strong>Erase on delete</strong>: Yes</p></li> <li><p>After the completion of the settings, click <strong>OK</strong> to apply the settings on the server profile.</p></li> <li><p>Repeat steps 1-6 for remaining worker nodes.</p></li></ol> <h1 id="physical-node-configuration"><a href="#physical-node-configuration" class="header-anchor">#</a> Physical node configuration</h1> <p>This section describes the configuration of the bare metal compute modules and is separated into sections that disseminate universal configuration parameters, pointers exclusively for virtualized nodes, and options used exclusively for bare metal nodes. The required configuration steps are outlined. These may be in the form of UI instruction pointers to code, or command line options. Use the CLI options or UI instruction pointers accordingly to reach the desired end state.</p> <h2 id="red-hat-openshift-master-nodes"><a href="#red-hat-openshift-master-nodes" class="header-anchor">#</a> Red Hat OpenShift master nodes</h2> <p>[]{#_Creating_Server_Profile .anchor}Refer to the section [Server Profiles] in this document to create a server profile for the bare metal master nodes.</p> <p>After the server profile is successfully created and attached to a compute module, refer to the following steps to install the operating system on the bootstrap node and the master nodes.</p> <ol><li><p>Ensure that the location of ignition files of the corresponding nodes is updated in the PXE configuration files.</p></li> <li><p>Ensure the MAC address of the network adapter in each server profile is updated with the corresponding IP address in the DHCP configuration file.</p></li> <li><p>Ensure that the load balancer server is up and running.</p></li> <li><p>From the <strong>HPE OneView</strong> interface, navigate to <strong>Server Profiles</strong> and select the appropriate server profile for the corresponding nodes. Select <strong>Actions</strong> &gt; <strong>Launch Console</strong>.</p></li> <li><p>While booting, select the appropriate OS label.</p></li> <li><p>Wait until the OS installation is complete.</p></li> <li><p>Verify the installation by logging on to the node from the installer VM using the following command.</p></li></ol> <blockquote><p>```</p> <p># ssh core@&lt; replace_with_node_fqdn_or_ip &gt;</p> <p>```</p></blockquote> <p><strong>NOTE</strong></p> <p>To utilize virtual machines as the OpenShift master nodes, refer to the sections [Virtual nodes configuration] and [Deploying virtual master nodes] in this document.</p> <h2 id="red-hat-openshift-worker-nodes"><a href="#red-hat-openshift-worker-nodes" class="header-anchor">#</a> Red Hat OpenShift worker nodes</h2> <p>Worker nodes for OpenShift 4 can run either RHCOS or RHEL 7.6. This section outlines the steps required to create worker nodes with either OS.</p> <p><strong>NOTE</strong></p> <p>To utilize virtual nodes as OpenShift worker nodes, refer the sections [Virtual nodes configuration][8] and [Deploying virtual worker nodes] in this document.</p> <h3 id="red-hat-openshift-worker-nodes-with-rhcos"><a href="#red-hat-openshift-worker-nodes-with-rhcos" class="header-anchor">#</a> Red Hat OpenShift worker nodes with RHCOS</h3> <p>Refer the section [Server Profiles] in this document to create the server profile for the worker nodes before proceeding with the subsequent section.</p> <p>After the server profile is successfully created and attached to a compute module, refer to the following steps to install the operating system on the worker nodes.</p> <ol><li><p>Ensure that the location of ignition files of the corresponding nodes is updated in the PXE configuration files.</p></li> <li><p>Ensure the MAC address of the network adapter in each server profile is updated with the corresponding IP address in the DHCP configuration file.</p></li> <li><p>Ensure that the load balancer server is up and running.</p></li> <li><p>From the <strong>HPE OneView</strong> interface, navigate to <strong>Server Profiles</strong> and select the appropriate server profile for the corresponding nodes. Select <strong>Actions</strong> &gt; <strong>Launch Console</strong>.</p></li> <li><p>While booting, select the appropriate OS label.</p></li> <li><p>Wait until the OS installation is complete.</p></li> <li><p>Verify the installation by logging on to the node from the installer VM using the following command.</p></li></ol> <blockquote><p>```</p> <p># ssh core@&lt; replace_with_node_fqdn_or_ip &gt;</p> <p>```</p></blockquote> <p>After the RHCOS worker nodes are up and running, refer the section [Red Hat OpenShift Container Platform deployment] to create the OpenShift 4 cluster.</p> <h3 id="red-hat-openshift-worker-nodes-with-rhel"><a href="#red-hat-openshift-worker-nodes-with-rhel" class="header-anchor">#</a> Red Hat OpenShift worker nodes with RHEL</h3> <p>Refer the section [Server Profiles] in this document to create the server profile for the worker nodes before proceeding with the subsequent section.</p> <p>After the server profile is successfully created and attached to a compute module, refer to the following steps to install the operating system on the worker nodes.</p> <ol><li><p>Ensure that the location of ignition files of the corresponding nodes is updated in the PXE configuration files.</p></li> <li><p>Ensure the MAC address of the network adapter in each server profile is updated with the corresponding IP address in the DHCP configuration file.</p></li> <li><p>Ensure that the load balancer server is up and running.</p></li> <li><p>From the <strong>HPE OneView</strong> interface, navigate to <strong>Server Profiles</strong> and select the appropriate Server Profile for the corresponding nodes. Select <strong>Actions</strong> &gt; <strong>Launch Console</strong>.</p></li> <li><p>While booting, select the appropriate OS label.</p></li> <li><p>Wait until the OS installation is complete.</p></li> <li><p>Verify the installation by logging on to the node from the installer VM using the following command.</p></li></ol> <blockquote><p>```</p> <p># ssh root@&lt; replace_with_node_fqdn_or_ip &gt;</p> <p>```</p></blockquote> <h4 id="preparing-worker-nodes-with-rhel"><a href="#preparing-worker-nodes-with-rhel" class="header-anchor">#</a> Preparing worker nodes with RHEL</h4> <p>Once the RHEL operating system is installed on the worker nodes, perform the following steps on that node:</p> <ol><li>Log in to the RHEL host, register and attach the host pool with Red Hat by running the following command.</li></ol> <blockquote><p>```</p> <p># subscription-manager register</p> <p>```</p></blockquote> <ol><li><p>Execute the following command to list the available subscriptions.</p> <p>```</p> <p># subscription-manager list --available --matches '*OpenShift*'</p> <p>```</p></li> <li><p>In the output for the command in step 2, find the pool ID for Red Hat OpenShift Container Platform subscription and attach it.</p></li></ol> <blockquote><p>```</p> <p># subscription-manager attach --pool=&lt;pool_id&gt;</p> <p>```</p></blockquote> <ol><li>Disable all repositories and enable only the repositories required by Red Hat OpenShift Container Platform 4.</li></ol> <blockquote><p>```</p> <p># yum-config-manager --disable \*</p> <p># subscription-manager repos --disable=&quot;*&quot;\</p> <p>--enable=&quot;rhel-7-server-rpms&quot; \</p> <p>--enable=&quot;rhel-7-server-extras-rpms&quot; \</p> <p>--enable=&quot;rhel-7-server-ose-4.3-rpms&quot;</p> <p>```</p></blockquote> <p>After the RHEL 7.6 nodes are ready, refer to the section [Adding RHEL 7.6 worker nodes] in this document to add them to the OpenShift 4 cluster.</p> <h1 id="virtual-nodes-configuration"><a href="#virtual-nodes-configuration" class="header-anchor">#</a> Virtual nodes configuration</h1> <p>This section describes the process to deploy virtualization hosts for OpenShift. This section outlines the steps required to configure virtual machine master and worker nodes. At a high level, these steps are as follows:</p> <ol><li><p>Deploying the vSphere hosts</p></li> <li><p>Creating the data center, cluster, and adding hosts into the cluster</p></li> <li><p>Creating a datastore in vCenter</p></li> <li><p>Create virtual master nodes</p></li> <li><p>Deploying virtual worker nodes</p></li></ol> <p><strong>NOTE</strong></p> <p>Hewlett Packard Enterprise utilized a consistent method for deployment that would allow for mixed deployments of virtual and physical master and worker nodes and built this solution on bare metal using the Red Hat OpenShift Container Platform user-provisioned infrastructure. For more details on the bare metal provisioner, refer to <a href="https://cloud.redhat.com/openshift/install/metal/user-provisioned" target="_blank" rel="noopener noreferrer">https://cloud.redhat.com/openshift/install/metal/user-provisioned<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>. If the intent is to have an overall virtual environment, it is recommended the installation user utilizes Red Hat’s virtual provisioning methods found at <a href="https://docs.openshift.com/container-platform/4.3/installing/installing_vsphere/installing-vsphere.html#installing-vsphere" target="_blank" rel="noopener noreferrer">https://docs.openshift.com/container-platform/4.3/installing/installing_vsphere/installing-vsphere.html#installing-vsphere<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h2 id="deploying-vsphere-hosts"><a href="#deploying-vsphere-hosts" class="header-anchor">#</a> Deploying vSphere hosts</h2> <p>Refer to the section [Server Profiles] in this document to create the server profile for the vSphere hosts.</p> <p>After the successful creation of the server profile, install the hypervisor. The following steps describes the process to install the hypervisor:</p> <ol><li><p>From the HPE OneView interface, navigate to Server Profiles and select ESXi-empty-volume Server Profile, Select <strong>Actions &gt; Launch Console.</strong></p></li> <li><p>From the Remote Console window, choose <strong>Virtual Drives -&gt; Image File CD-ROM/DVD</strong> from the <strong>iLO options</strong> menu bar.</p></li> <li><p>Navigate to the VMware ESXi 6.7 ISO file located on the installation system.</p></li></ol> <blockquote><p>Select the ISO file and click <strong>Open</strong>.</p></blockquote> <ol><li><p>If the server is in the powered off state, power switch on the server by selecting <strong>Power Switch -&gt; Momentary Press.</strong></p></li> <li><p>During boot, press <strong>F11</strong> Boot Menu and select iLO Virtual USB 3: iLO Virtual CD-ROM.</p></li> <li><p>When the VMware ESXi installation media has finished loading, proceed through the VMware user prompts. For storage device, select the 40GiB OS volume created on the HPE Image Streamer during server profile creation and <strong>set the root password.</strong></p></li> <li><p>Wait until the vSphere installation is complete.</p></li> <li><p>After the installation is complete, press <strong>F2</strong> to enter the vSphere host configuration page and update the IP address, gateway, DNS, hostname of the host and enable SSH.</p></li> <li><p>After the host is reachable, proceed with the next section.</p></li></ol> <h2 id="hpe-oneview-for-vmware-vcenter"><a href="#hpe-oneview-for-vmware-vcenter" class="header-anchor">#</a> HPE OneView for VMware vCenter</h2> <p>HPE OneView for VMware vCenter is a single, integrated plug-in application for VMware vCenter management. This enables the vSphere administrator to quickly obtain context-aware information about HPE Servers and HPE Storage in their VMware vSphere data center directly from within vCenter. This application enables the vSphere administrator to easily manage physical servers and storage, datastores, and virtual machines. By providing the ability to clearly view and directly manage the HPE Infrastructure from within the vCenter console, the productivity of VMware administrator increases. This also enhances the ability to ensure quality of service.</p> <p>For more details, refer to the HPE documentation at <a href="https://h20392.www2.hpe.com/portal/swdepot/displayProductInfo.do?productNumber=HPVPR" target="_blank" rel="noopener noreferrer">https://h20392.www2.hpe.com/portal/swdepot/displayProductInfo.do?productNumber=HPVPR<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h2 id="creating-the-data-center-cluster-and-adding-hosts-in-vmware-vcenter"><a href="#creating-the-data-center-cluster-and-adding-hosts-in-vmware-vcenter" class="header-anchor">#</a> Creating the Data center, Cluster and adding Hosts in VMware vCenter</h2> <p>This section assumes a VMware vCenter server is available within the installation environment. A data center is a structure in VMware vCenter which contains clusters, hosts, and datastore. To begin with, a data center needs to be created, followed by the clusters and adding hosts into the clusters.</p> <p>To create a data center, a cluster enabled with vSAN and DRS and adding hosts, the installation user will need to edit the vault file and the variables YAML file. Using an editor, open the file <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/vsphere/vcenter/roles/prepare_vcenter/vars/main.yml</em> to provide the names for data center, clusters and vSphere hostnames. A sample input file is listed and as follows. Installation user should modify this file to suit the environment.</p> <p>In the Ansible vault file (<em>secret.yml</em>) found at <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/vsphere/vcenter</em>, provide the vCenter and the vSphere host credentials.</p> <p>```</p> <p># vsphere hosts credentials</p> <p>vsphere_username: &lt;username&gt;</p> <p>vsphere_password: &lt;password&gt;</p> <p># vcenter hostname/ip address and credentials</p> <p>vcenter_hostname: x.x.x.x</p> <p>vcenter_username: &lt;username&gt;</p> <p>vcenter_password: &lt;password&gt;</p> <p>```</p> <p><strong>NOTE</strong></p> <p>This section assumes all the virtualization hosts have a common username and password. If it does not have a common username and password, it is up to the installation user to add the virtualization hosts within the appropriate cluster.</p> <p>Variables for running the playbook can be found at <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/vsphere/vcenter/roles/prepare_vcenter/vars/main.yml</em>.</p> <p>```</p> <p># custom name for data center to be created.</p> <p>datacenter_name: datacenter</p> <p># custom name of the compute clusters with the ESXi hosts for Management VMs.</p> <p>management_cluster_name: management-cluster</p> <p># hostname or IP address of the vsphere hosts utilized for the management nodes.</p> <p>vsphere_host_01: 10.0.x.x</p> <p>vsphere_host_02: 10.0.x.x</p> <p>vsphere_host_03: 10.0.x.x</p> <p>```</p> <p>After the variable files are updated with the appropriate values, execute the following command within the installer VM to create the data center, clusters, and add hosts into respective clusters.</p> <p>```</p> <p># cd /etc/ansible/hpe-solutions-openshift/synergy/scalable/vsphere/vcenter/</p> <p># ansible-playbook playbooks/prepare_vcenter.yml –ask-vault-pass</p> <p>```</p> <h2 id="creating-a-datastore-in-vcenter"><a href="#creating-a-datastore-in-vcenter" class="header-anchor">#</a> Creating a Datastore in vCenter</h2> <p>A datastore needs to be created in VMware vCenter from the volume carved out of HPE Storage SANs to store the VMs. The following are the steps to create a datastore in vCenter:</p> <ol><li><p>From the vSphere Web Client navigator, right-click the cluster, select <strong>Storage</strong> from the menu, and then select the <strong>New Datastore</strong>.</p></li> <li><p>From the Type page, select <strong>VMFS</strong> as the Datastore type and click <strong>Next</strong>.</p></li> <li><p>Enter the datastore name and if necessary, select the placement location for the datastore and click <strong>Next</strong>.</p></li> <li><p>Select the device to use for the datastore and click <strong>Next</strong>.</p></li> <li><p>From VMFS version page, select <strong>VMFS 6</strong> and click <strong>Next</strong>.</p></li> <li><p>Define the following configuration requirements for the datastore as per the installation environment and click <strong>Next</strong>.</p> <p>a.  Specify partition configuration</p> <p>b.  Datastore Size</p> <p>c.  Block Size</p> <p>d.  Space Reclamation Granularity</p> <p>e.  Space Reclamation Priority</p></li> <li><p>On the Ready to complete page, review the Datastore configuration and click <strong>Finish</strong>.</p></li></ol> <p><strong>NOTE</strong></p> <p>If you utilize virtual worker nodes, repeat this section to create a Datastore to store the worker virtual machines.</p> <h2 id="red-hat-openshift-container-platform-sizing"><a href="#red-hat-openshift-container-platform-sizing" class="header-anchor">#</a> Red Hat OpenShift Container Platform sizing</h2> <p>Red Hat OpenShift Container Platform sizing varies depending on the requirements of the organization and type of deployment. This section highlights the host sizing details recommended by Red Hat.</p> <p><strong>| Resource | Bootstrap node | Master node | Worker node |</strong></p> <p>| ------------ | -------- | ----------- | --------------------------------------------------- |</p> <p>| CPU | 4 | 4 | 4 |</p> <p>| Memory | 16GB | 16GB | 16GB |</p> <p>| Disk storage | 120GB | 120GB | 120GB |</p> <p>| Disk storage | 120GB | 120GB | 120GB</p> <p>Disk partitions on each of the nodes are as follows.</p> <ul><li><p>/var – 40GB</p></li> <li><p>/usr/local/bin – 1GB</p></li> <li><p>Temporary directory – 1GB</p></li></ul> <p><strong>NOTE</strong></p> <p>Sizing for worker nodes is ultimately dependent on the container workloads and their CPU, memory, and disk requirements.</p> <p>For more information about Red Hat OpenShift Container Platform sizing, refer to the Red Hat OpenShift Container Platform 4 product documentation at <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.3/html/scalability_and_performance/index" target="_blank" rel="noopener noreferrer">https://access.redhat.com/documentation/en-us/openshift_container_platform/4.3/html/scalability_and_performance/index<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h2 id="deploying-virtual-master-nodes"><a href="#deploying-virtual-master-nodes" class="header-anchor">#</a> Deploying virtual master nodes</h2> <p>This section outlines the steps to create the virtual machines used as the master nodes.</p> <p><strong>NOTE</strong></p> <p>This section utilized vSphere rules such as affinity and anti-affinity rules to ensure no two master nodes are present on the same vSphere host, hence it is essential to enable vMotion in all the vSphere hosts. If not enabled, select the vSphere host in the VMware vCenter server user interface, <strong>click -&gt; Configure -&gt; Networking -&gt; VMkernel adapters -&gt; Management Network -&gt; Edit</strong> and select the checkbox against vMotion to enable vMotion.</p> <p>To create the virtual machines for the OpenShift master nodes, edit the variables file. Use an editor such as Vim or Nano, open the file <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/vsphere/virtual_nodes /roles/deploy_vm/vars/main.yml.</em> The variable file contains information about the VMs, vCenter, hostnames, IP addresses, memory, and CPU. A sample variable file is provided and as follows. The installation user should modify the file to make it suitable for the target environment.</p> <blockquote><p>```</p> <p># Name of the Data center</p> <p>datacenter_name: &lt;data_center_name&gt;</p> <p># Name of the compute clusters with the ESXi hosts for Management VMs</p> <p>management_cluster_name: &lt;data_cluster_name&gt;</p> <p># Name of the Datastore to store the VMs</p> <p>management_datastore_name: &lt;datastore_name&gt;</p> <p># Name of the coreOS guest image</p> <p>guest_template: coreos64Guest</p> <p># Disk size in GB/GiB</p> <p>bootstrap_disk: 120</p> <p>master_disk: 120</p> <p>lb_disk: 50</p> <p># number of CPUs</p> <p>bootstrap_cpu: 4</p> <p>master_cpu: 4</p> <p>lb_cpu: 4</p> <p># Memory size in MB/MiB</p> <p>bootstrap_memory: 16400</p> <p>master_memory: 16400</p> <p>lb_memory: 16400</p> <p>gateway: &lt;replace_with_gateway_ip&gt;</p> <p>dns_server: &lt;replace_with_dns_server_ip&gt;</p> <p>domain: &lt;replace_with_domain_ip&gt;</p> <p># name of the master, bootstrap and lb nodes &lt; short names, not the FQDN &gt;</p> <p>bootstrap01_name: &lt;bootstrap01_host_name&gt;</p> <p>master01_name: &lt;master01_host_name&gt;</p> <p>master02_name: &lt;master02_host_name&gt;</p> <p>master03_name: &lt;master03_host_name&gt;</p> <p>lb01_name: &lt;lb01_host_name&gt;</p> <p>domain_name: &quot;&lt;sub_domain&gt;&quot;</p> <p># Network names for the management network</p> <p>datacenter_network_name: &quot;&lt;network_name&gt;&quot;</p> <p># vSphere affinity &amp; anti-affinity rules</p> <p>affinity_rule_name: &quot;vsphere-anti-affinty-rule&quot;</p> <p>anti_affinity_rule_name: &quot;vsphere-affinty-rule&quot;</p> <p>```</p></blockquote> <p>After the variable file is updated, execute the following command from the installer machine to deploy the specified VMs.</p> <blockquote><p>```</p> <p># cd /etc/ansible/hpe-solutions-openshift/synergy/scalable/vsphere/virtual_nodes</p> <p># ansible-playbook playbooks/deploy_vm.yml –ask-vault-pass</p> <p>```</p></blockquote> <p><em>deploy_vm.yml</em> playbooks create 3x VMs to be used as master nodes, 1x VM to be used as load balancer node and 1x VM to be used as a bootstrap node. All the master VMs will be deployed on different hosts whereas bootstrap &amp; haproxy VMs will be deployed on any single host.</p> <p>Wait for some time for vSphere rules to be applicable on VMs. vSphere rules can be viewed at <strong>vCenter server -&gt; Datacenter -&gt; Cluster -&gt; Configure -&gt; VM/Host Rules</strong>.The 3 master nodes are part of the <em>vsphere-anti-affinity-rule</em> and each master VM will reside on a different vSphere host. The bootstrap and load balancer VMs are part of the <em>vsphere-affinity-rule</em> and they are co-resident on one of 3 vSphere hosts.</p> <p>It is recommended to ensure the <em>Boot Delay</em> is long enough to enable OS installation via PXE server.</p> <p>After the virtual machines are successfully created, refer to the following steps to install the operating system on the bootstrap node and the master nodes:</p> <ol><li><p>Ensure that the location of ignition files of the corresponding nodes is updated in the PXE configuration files.</p></li> <li><p>Ensure the MAC address of the network adapter in VM is updated with the corresponding IP address in the DHCP configuration file.</p></li> <li><p>Ensure that the load balancer server is up and running.</p></li> <li><p>From the VMware vCenter Server, select the VM, and launch the VM Remote console.</p></li> <li><p>From the Remote Console window, power on the VM.</p></li> <li><p>While booting, select the appropriate OS label.</p></li> <li><p>Wait until the OS installation is complete.</p></li> <li><p>Verify the installation by logging on to the node from the installer VM using the following command.</p></li></ol> <blockquote><p>```</p> <p># ssh core@&lt; replace_with_node_fqdn_or_ip &gt;</p> <p>```</p></blockquote> <p>After the RHCOS master nodes are ready, refer to the section [Red Hat OpenShift Container Platform deployment] in this document to create the OpenShift 4 cluster.</p> <h2 id="deploying-virtual-worker-nodes"><a href="#deploying-virtual-worker-nodes" class="header-anchor">#</a> Deploying virtual worker nodes</h2> <p>This section outlines the steps to create virtual machines and configure them to be used as worker nodes.</p> <h3 id="creating-virtual-machines"><a href="#creating-virtual-machines" class="header-anchor">#</a> Creating virtual machines</h3> <p>This section outlines the steps to create virtual machines.</p> <ol><li><p>Login to vCenter using the Web Client and select an ESXi Host. Right-click the host and then click <strong>New Virtual Machine</strong> to open a <em>New Virtual Machine Wizard</em>.</p></li> <li><p>From <em>Select a creation type</em>, select <strong>Create a new virtual machine</strong> and click <strong>Next</strong>.</p></li> <li><p>Enter a unique <em>Name</em> for the VM and select the <strong>Datacenter</strong>. Click <strong>Next</strong>.</p></li> <li><p>Select the <strong>Cluster</strong> on which the VM can be deployed. Click <strong>Next</strong>.</p></li> <li><p>Select the <strong>Datastore</strong> on which the VM can be stored and click <strong>Next</strong>.</p></li> <li><p>On the Select compatibility page, choose <strong>ESXI 6.7 and later</strong> and click <strong>Next</strong>.</p></li> <li><p>On the Select a guest OS page, choose the Guest OS family as <strong>Linux</strong> and Guest OS Version as <strong>Red Hat Enterprise Linux 7 (64 bit)</strong> (in case of RHEL worker) and <strong>Red Hat CoreOS</strong> (in case of Red Hat CoreOS worker) and select <strong>Next</strong>.</p></li> <li><p>In the Customize hardware page, configure the Virtual Hardware with <strong>4 CPU</strong>, <strong>16 GB</strong> Memory, <strong>150 GB</strong> <strong>dual Hard Disk</strong> as per requirement and attach the Operating System from the datastore. Select the Connect at <strong>Power on</strong> option and click <strong>Next</strong>.</p></li> <li><p>Review the virtual machine configuration before deploying the virtual machine and click <strong>Finish</strong> to complete the New Virtual Machine wizard.</p></li></ol> <h3 id="red-hat-coreos-worker-nodes"><a href="#red-hat-coreos-worker-nodes" class="header-anchor">#</a> Red Hat CoreOS worker nodes</h3> <p>Refer to the section [Creating virtual machines] in the document to create virtual machines. After the virtual machines are successfully created, refer to the following steps to install the operating system on the worker nodes:</p> <ol><li><p>Ensure that the location of ignition files of the corresponding nodes is updated in the PXE configuration files.</p></li> <li><p>Ensure the MAC address of the network adapter in VM is updated with the corresponding IP address in the DHCP configuration file.</p></li> <li><p>Ensure that the load balancer server is up and running.</p></li> <li><p>From the VMware vCenter Server, select the VM and launch the VM Remote console.</p></li> <li><p>From the Remote Console window, power on the VM.</p></li> <li><p>While booting, select the appropriate OS label.</p></li> <li><p>Wait until the OS installation is complete.</p></li> <li><p>Verify the installation by logging on to the node from the installer VM using the following command.</p></li></ol> <blockquote><p>```</p> <p># ssh core@&lt; replace_with_node_fqdn_or_ip &gt;</p> <p>```</p></blockquote> <p>After the RHCOS worker nodes are up and running, refer to the section [Adding Red Hat CoreOS worker nodes] in the document to add them to OpenShift 4 cluster**.**</p> <h3 id="rhel-7-6-worker-nodes"><a href="#rhel-7-6-worker-nodes" class="header-anchor">#</a> RHEL 7.6 worker nodes</h3> <p>Refer to the section [Creating virtual machines] in this document to create virtual machines. After the virtual machines are successfully created, follow these steps to install the operating system on the worker nodes:</p> <ol><li><p>Ensure that the location of ignition files of the corresponding nodes is updated in the PXE configuration files.</p></li> <li><p>Ensure the MAC address of the network adapter in VM is updated with the corresponding IP address in the DHCP configuration file.</p></li> <li><p>Ensure that the load balancer server is up and running.</p></li> <li><p>From the VMware vCenter Server, select the VM, and launch the VM Remote console.</p></li> <li><p>From the Remote Console window, power on the VM.</p></li> <li><p>While booting, select the appropriate OS label.</p></li> <li><p>Wait until the OS installation is complete.</p></li> <li><p>Verify the installation by logging on to the node from the installer VM using the following command.</p></li></ol> <blockquote><p>```</p> <p># ssh root@&lt; node_fqdn or node_ip_address&gt;</p> <p>```</p></blockquote> <p>Once the RHEL 7.6 nodes are reachable, refer to the section [Preparing worker nodes with RHEL] in the document to prepare the RHEL worker nodes. After preparing the worker nodes, refer to the section [Adding RHEL 7.6 worker nodes] in the document to add them to the OpenShift 4 cluster.</p> <h1 id="red-hat-openshift-container-platform-deployment"><a href="#red-hat-openshift-container-platform-deployment" class="header-anchor">#</a> Red Hat OpenShift Container Platform deployment</h1> <p>This section describes the process to deploy Red Hat OpenShift Container Platform 4.</p> <h2 id="deploying-an-openshift-cluster"><a href="#deploying-an-openshift-cluster" class="header-anchor">#</a> Deploying an OpenShift Cluster</h2> <p>[]{#_Authentication .anchor}This section covers the steps to deploy a Red Hat OpenShift Container Platform 4 cluster with master nodes and worker nodes running RHCOS. After the operating system is installed on the nodes, verify the installation and then perform the following steps:</p> <ol><li><p>Login to the installer VM.</p></li> <li><p>Execute the following command to bootstrap the nodes.</p></li></ol> <blockquote><p>```</p> <p># /etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/library/openshift_components/openshift-install wait-for bootstrap-complete --log-level=debug</p> <p>```</p> <p><strong>RESULT</strong></p> <p>```</p> <p>DEBUG OpenShift Installer v4.3</p> <p>DEBUG Built from commit 425e4ff0037487e32571258640b39f56d5ee5572</p> <p>INFO Waiting up to 30m0s for the Kubernetes API at https://api.ocp.pxelocal.local:6443...</p> <p>INFO API v1.14.6+76aeb0c up</p> <p>INFO Waiting up to 30m0s for bootstrapping to complete...</p> <p>DEBUG Bootstrap status: complete</p> <p>INFO It is now safe to remove the bootstrap resources</p> <p>```</p></blockquote> <p><strong>NOTE</strong></p> <p>You can shut down or remove the bootstrap node after the completion of step 1 and step 2.</p> <ol><li>Run the following command to append the kubeconfig path to environment variables.</li></ol> <blockquote><p>```</p> <p># export KUBECONFIG=/etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/ignitions/auth/kubeconfig</p> <p>```</p></blockquote> <ol><li>Run the following command to append the oc utility path to environment variables.</li></ol> <blockquote><p>```</p> <p># export PATH=$PATH:/etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/library/openshift_components</p> <p>```</p></blockquote> <ol><li>Provide the PV storage for the registry. Execute the following command to set the image registry storage to an empty directory.</li></ol> <blockquote><p>```</p> <p># oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{&quot;spec&quot;:{&quot;storage&quot;:{&quot;emptyDir&quot;:{}}}}'</p> <p>```</p></blockquote> <ol><li>Execute the following command to complete the RedHat OpenShift Container Platform 4 cluster installation.</li></ol> <blockquote><p>```</p> <p># /etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/library/openshift_components/openshift-install wait-for install-complete --log-level=debug</p> <p>```</p></blockquote> <p>The result should appear like what appears below.</p> <blockquote><p>```</p> <p>DEBUG OpenShift Installer v4.3</p> <p>DEBUG Built from commit 6ed04f65b0f6a1e11f10afe658465ba8195ac459</p> <p>INFO Waiting up to 30m0s for the cluster at https://api.rrocp.pxelocal.local:6443 to initialize...</p> <p>DEBUG Still waiting for the cluster to initialize: Working towards 4.3: 99% complete</p> <p>DEBUG Still waiting for the cluster to initialize: Working towards 4.3: 99% complete, waiting on authentication, console, image-registry</p> <p>DEBUG Still waiting for the cluster to initialize: Working towards 4.3: 99% complete</p> <p>DEBUG Still waiting for the cluster to initialize: Working towards 4.3: 100% complete, waiting on image-registry</p> <p>DEBUG Still waiting for the cluster to initialize: Cluster operator image-registry is still updating</p> <p>DEBUG Still waiting for the cluster to initialize: Cluster operator image-registry is still updating</p> <p>DEBUG Cluster is initialized</p> <p>INFO Waiting up to 10m0s for the openshift-console route to be created...</p> <p>DEBUG Route found in openshift-console namespace: console</p> <p>DEBUG Route found in openshift-console namespace: downloads</p> <p>DEBUG OpenShift console route is created</p> <p>INFO Install complete!</p> <p>INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/root/ocp42-8/auth/kubeconfig'</p> <p>INFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp.pxelocal.local</p> <p>INFO Login to the console with user: kubeadmin, password: a6hKv-okLUA-Q9p3q-UXLc3</p> <p>```</p></blockquote> <p></p> <p><strong>NOTE</strong></p> <p>Make a note of the cluster URL and the username for future access.</p> <p>If the password is lost or forgotten, view the file kubeadmin-password located in the installer machine at <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/library/ignitions/auth/.</em></p> <p>Figure 9 shows the OpenShift Container Platform console view upon successful deployment.</p> <p>![OpenShift Web Console login screen](/figure9.png)</p> <p>![][9]</p> <p><strong>Figure 9.</strong> OpenShift Web Console login screen</p> <h2 id="adding-red-hat-coreos-worker-nodes"><a href="#adding-red-hat-coreos-worker-nodes" class="header-anchor">#</a> Adding Red Hat CoreOS worker nodes</h2> <p>This section covers the steps to add RHCOS worker nodes to an existing OpenShift Container Platform 4 cluster.</p> <h3 id="prerequisites"><a href="#prerequisites" class="header-anchor">#</a> Prerequisites</h3> <ol><li><p>A Red Hat OpenShift Container Platform 4 cluster is available within the deployment environment.</p></li> <li><p>A worker node ignition file is generated along with the bootstrap and master ignition files. Refer to the section [Kubernetes manifests and ignition files] in this document for details on generating manifest and ignition files.</p></li></ol> <blockquote><p><strong>NOTE</strong></p> <p>It is important to use the <em>worker.ign</em> ignition file generated along with the <em>master.ign</em> file used to create the OpenShift cluster. If this is not the case, then “certificate signing requests” in OpenShift will not recognize these worker nodes.</p></blockquote> <ol><li>A working node is available that needs to be attached to the existing OpenShift cluster and used as the worker node.</li></ol> <h3 id="procedure"><a href="#procedure" class="header-anchor">#</a> Procedure</h3> <ol><li><p>Login to the installer VM.</p></li> <li><p>Execute the following command to get the nodes associated with the OpenShift cluster.</p></li></ol> <blockquote><p>```</p> <p># export KUBECONFIG=/etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/ignitions/auth/kubeconfig</p> <p># export PATH=$PATH:/etc/ansible/hpe-solutions-openshift/synergy/scalable/installer/library/openshift_components</p> <p># oc get nodes</p> <p>```</p> <p>The output is as follows.</p> <p>```</p> <p>NAME STATUS ROLES AGE VERSION</p> <p>master1.ocp.twentynet.local Ready master,worker 5d v1.14.6+888f9c630</p> <p>master2.ocp.twentynet.local Ready master,worker 5d v1.14.6+888f9c630</p> <p>master3.ocp.twentynet.local Ready master,worker 5d v1.14.6+888f9c630</p> <p>```</p></blockquote> <ol><li>Since the worker node is booted with the existing worker ignition file, the node is recognized by the current OpenShift cluster. However, the certificate signing request is pending. Execute the following command to get the certificate signing requests.</li></ol> <blockquote><p>```</p> <p># oc get csr</p> <p>```</p> <p>The output is as follows.</p></blockquote> <p>```</p> <blockquote><p>NAME AGE REQUESTOR CONDITION</p> <p>csr-8pj6k 28m system:node:worker1.ocp.twentynet.local Pending</p> <p>csr-9pj7c 28m system:node:worker2.ocp.twentynet.local Pending</p> <p>csr-9pj1s 28m system:node:worker3.ocp.twentynet.local Pending</p> <p>```</p></blockquote> <ol><li>Execute the following command to approve all of the pending certificate signing requests and to add the worker nodes to the cluster.</li></ol> <blockquote><p>```</p> <p># oc get csr | awk '{print $1}'| while read line; do ./oc adm certificate approve $line; done</p> <p>```</p></blockquote> <ol><li>Verify that the certificate signing requests for the worker nodes are approved using the following command.</li></ol> <blockquote><p>```</p> <p># oc get csr</p> <p>```</p></blockquote> <p>The output is as follows.</p> <p>```</p> <p>NAME AGE REQUESTOR CONDITION</p> <p>csr-8pj6k 28m system:node:worker1.ocp.twentynet.local Approved,Issued</p> <p>csr-9pj7c 28m system:node:worker2.ocp.twentynet.local Approved,Issued</p> <p>csr-9pj1s 28m system:node:worker3.ocp.twentynet.local Approved,Issued</p> <p>```</p> <ol><li>Verify that the worker nodes are added to the cluster using the following command.</li></ol> <blockquote><p>```</p> <p># oc get nodes</p> <p>```</p> <p>The output is as follows.</p> <p>```</p> <p>NAME STATUS ROLES AGE VERSION</p> <p>master1.ocp.twentynet.local Ready master,worker 5d v1.14.6+888f9c630</p> <p>master2.ocp.twentynet.local Ready master,worker 5d v1.14.6+888f9c630</p> <p>master3.ocp.twentynet.local Ready master,worker 5d v1.14.6+888f9c630</p> <p>worker1.ocp.twentynet.local Ready worker 5d v1.14.6+888f9c630</p> <p>worker2.ocp.twentynet.local Ready worker 5d v1.14.6+888f9c630</p> <p>worker3.ocp.twentynet.local Ready worker 5d v1.14.6+888f9c630</p></blockquote> <p>```</p> <ol><li>Execute the following command to set the parameter <em>mastersSchedulable</em> parameter as <em>false</em>, so that master nodes will not be used to schedule pods.</li></ol> <blockquote><p>```</p> <p># oc edit scheduler</p> <p>```</p></blockquote> <p>The output is as follows.</p> <p></p> <blockquote><p>```</p> <p>apiVersion: config.openshift.io/v1</p> <p>kind: Scheduler</p> <p>metadata:</p> <p>creationTimestamp: &quot;2019-12-13T10:34:48Z&quot;</p> <p>generation: 2</p> <p>name: cluster</p> <p>resourceVersion: &quot;1748652&quot;</p> <p>selfLink: /apis/config.openshift.io/v1/schedulers/cluster</p> <p>uid: 30245db9-1d94-11ea-8066-000c29c3ee8e</p> <p>spec:</p> <p>**mastersSchedulable: false **</p> <p>policy:</p> <p>name: &quot;&quot;</p> <p>status: {}</p> <p>```</p></blockquote> <ol><li>Execute the following command to verify the master roles have been reset.</li></ol> <blockquote><p>```</p> <p># oc get nodes</p> <p>```</p> <p>The output is as follows.</p> <p>```</p> <p>NAME STATUS ROLES AGE VERSION</p> <p>master1.ocp.twentynet.local Ready master 5d v1.14.6+888f9c630</p> <p>master2.ocp.twentynet.local Ready master 5d v1.14.6+888f9c630</p> <p>master3.ocp.twentynet.local Ready master 5d v1.14.6+888f9c630</p> <p>worker1.ocp.twentynet.local Ready worker 5d v1.14.6+888f9c630</p> <p>worker2.ocp.twentynet.local Ready worker 5d v1.14.6+888f9c630</p> <p>worker3.ocp.twentynet.local Ready worker 5d v1.14.6+888f9c630</p></blockquote> <p>```</p> <h2 id="adding-rhel-7-6-worker-nodes"><a href="#adding-rhel-7-6-worker-nodes" class="header-anchor">#</a> Adding RHEL 7.6 worker nodes</h2> <p>This section covers the steps to add RHEL 7.6 worker nodes to an existing Red Hat OpenShift Container Platform 4 cluster.</p> <h3 id="prerequisites-2"><a href="#prerequisites-2" class="header-anchor">#</a> Prerequisites</h3> <ol><li><p>Ensure the required packages are installed and any necessary configuration is performed on the installer VM. Refer to the section [Installer machine] of this document for details on the prerequisites and configuration steps.</p></li> <li><p>RHEL nodes are prepared for installation. Refer to the section [Preparing worker nodes with RHEL] of this document for details on preparing the RHEL 7.6 worker nodes.</p></li></ol> <h3 id="procedure-2"><a href="#procedure-2" class="header-anchor">#</a> Procedure</h3> <p>Perform the following steps on the installer VM:</p> <ol><li>Download the Red Hat OpenShift Container Platform 4 Ansible package to enable the addition of RHEL 7.6 worker nodes to an existing OpenShift cluster.</li></ol> <blockquote><p>```</p> <p># sudo yum install openshift-ansible openshift-clients jq</p> <p>```</p></blockquote> <ol><li>Create an Ansible inventory file exclusively for adding RHEL worker nodes that is named <em>&lt;path&gt;/inventory/hosts</em> that defines your compute machine nodes and required variables as listed.</li></ol> <blockquote><p>```</p></blockquote> <p>[all:vars]</p> <p># Username that runs the Ansible tasks on the remote compute machines</p> <p># ansible_user=root</p> <p># If you do not specify root for the ansible_user,</p> <p># you must set ansible_become to True and assign the user sudo permissions.</p> <p>ansible_become=True</p> <p># Path to the kubeconfig file for your cluster</p> <p>openshift_kubeconfig_path=&quot;~/.kube/config&quot;</p> <p># FQDN of each RHEL machine to add to the cluster</p> <p>[new_workers]</p> <p>worker-0.example.com</p> <p>worker-1.example.com</p> <p>```</p> <ol><li>Execute the following commands to run the playbook which adds RHEL 7.6 worker nodes to the existing cluster.</li></ol> <blockquote><p>```</p> <p># cd /usr/share/ansible/openshift-ansible</p> <p># ansible-playbook -i &lt;path to inventory hosts file&gt; playbooks/scaleup.yml</p> <p>```</p></blockquote> <h3 id="approving-the-certificate-signing-requests-for-your-machines"><a href="#approving-the-certificate-signing-requests-for-your-machines" class="header-anchor">#</a> Approving the certificate signing requests for your machines</h3> <p>When new machines are added to the cluster, pending certificate signing requests (CSRs) are generated for each machine that is added. Confirm these CSRs are approved. You can also approve the CSR if necessary.</p> <h4 id="prerequisites-3"><a href="#prerequisites-3" class="header-anchor">#</a> Prerequisites</h4> <ul><li>jq package is installed.</li></ul> <h3 id="procedure-3"><a href="#procedure-3" class="header-anchor">#</a> Procedure</h3> <ol><li>Confirm that the cluster recognizes the machines by executing the following command. The output lists all of the machines that have been created.</li></ol> <blockquote><p>```</p> <p># oc get nodes</p> <p>NAME STATUS ROLES AGE VERSION</p> <p>master-0 Ready master 63m v1.14.6+c4799753c</p> <p>master-1 Ready master 63m v1.14.6+c4799753c</p> <p>master-2 Ready master 64m v1.14.6+c4799753c</p> <p>worker-0 NotReady worker 76s v1.14.6+c4799753c</p> <p>worker-1 NotReady worker 70s v1.14.6+c4799753c</p></blockquote> <p>```</p> <ol><li>Review the pending CSRs and ensure that there is a client and server request with Pending or Approved status for each machine added to the cluster.</li></ol> <blockquote><p>```</p> <p># oc get csr</p> <p>NAME AGE REQUESTOR CONDITION</p> <p>csr-8b2br 15m system:serviceaccount:openshift-machine-config-operator:node-bootstrapper Pending</p> <p>csr-8vnps 15m system:serviceaccount:openshift-machine-config-operator:node-bootstrapper Pending</p> <p>csr-bfd72 5m26s system:node:ip-10-0-50-126.us-east-2.compute.internal Pending</p> <p>csr-c57lv 5m26s system:node:ip-10-0-95-157.us-east-2.compute.internal Pending</p> <p>```</p></blockquote> <p>Since the CSRs rotate automatically, it is important to approve the CSRs within an hour of adding the machines to the cluster. If the CSRs are not approved within an hour, the certificates will rotate, and more than two certificates will be present for each node. All certificates must be approved. After the initial CSRs are approved, the subsequent node client CSRs are automatically approved by the cluster kube-controller-manager.</p> <p>To approve CSRs individually, run the following command for each valid CSR. In this example, &lt;csr_name&gt; is the name of a CSR from the list of current CSRs.</p> <p>```</p> <p># oc adm certificate approve &lt;csr_name&gt;</p> <p>```</p> <p>If all CSRs are valid, approve all of them by running the following command.</p> <p>```</p> <blockquote><p># oc get csr -ojson | jq -r '.items[] | select(.status == {} ) | .metadata.name' | xargs oc adm certificate approve</p> <p>```</p></blockquote> <h1 id="red-hat-local-storage-operator"><a href="#red-hat-local-storage-operator" class="header-anchor">#</a> Red Hat Local Storage Operator</h1> <p>This section describes how to configure the Local Storage Operator on Red Hat OpenShift Container Platform 4.</p> <h2 id="physical-environment"><a href="#physical-environment" class="header-anchor">#</a> Physical environment</h2> <p>Ensure a minimum of 2 unused disks (a minimum of 100GB and 250GB) are present on each worker node prior to implementing local persistent storage.</p> <p><strong>NOTE</strong></p> <p>Use the following command to find partition details.</p> <p>```</p> <blockquote><p># lsblk</p> <p>```</p></blockquote> <p>The output appears similar to what is shown below.</p> <blockquote><p>```</p> <p>NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT</p> <p>loop0 7:0 0 250G 0 loop</p> <p>sda 8:0 0 150G 0 disk</p> <p>├─sda1 8:1 0 1M 0 part</p> <p>├─sda2 8:2 0 1G 0 part /boot</p> <p>└─sda3 8:3 0 149G 0 part /sysroot</p> <p>sdb 8:16 0 100G 0 disk</p> <p>sdc 8:32 0 250G 0 disk</p></blockquote> <p>```</p> <h2 id="deploying-local-storage-operator"><a href="#deploying-local-storage-operator" class="header-anchor">#</a> Deploying local storage operator</h2> <ol><li><p>Login to the <strong>OpenShift Web Console</strong>.</p></li> <li><p>Navigate to <strong>Administration</strong> -&gt; <strong>Namespaces</strong> and click <strong>Create Namespace</strong>.</p></li> <li><p>From the <strong>Create Namespace</strong> page, provide the following values and then click <strong>Create</strong>.</p> <p>a.  <strong>Name</strong>: local-storage</p> <p>b.  Select the <strong>No restrictions</strong> option for Default Network Policy.</p></li> <li><p>Navigate to <strong>Operators</strong> -&gt; <strong>OperatorHub</strong>.</p></li> <li><p>From the list of operators, select <strong>Local</strong> <strong>Storage</strong> <strong>Operator</strong>.</p></li> <li><p>On the Local Storage Operator page, click <strong>Install</strong>.</p></li> <li><p>In the <strong>Create Operator Subscription</strong> page, provide the following details and click **Subscribe. **</p> <p>a.  <strong>Update Channel</strong>: 4.3</p> <p>b.  <strong>Approval Strategy</strong>: Select Manual or Automatic as per the installation environment requirement.</p></li> <li><p>Wait for the operator to be installed. The status of the installation can be monitored from the <em>Installed Operators</em> page.</p></li> <li><p>A filesystem is essential to create local file storage. Login to the installer VM, copy and update the following <em>local-storage-filesystem.yaml</em> file with values for the following parameters:</p> <p>a.  <strong>namespace:</strong> local-storage</p> <p>b.  <strong>values:</strong> Fully qualified domain name of the worker nodes</p> <p>c.  <strong>devicePaths:</strong> Partition of the worker nodes to be used to create the filesystem</p> <div class="language- extra-class"><pre><code>A sample *local-storage-filesystem.yaml* is shown below.

\`\`\`
</code></pre></div></li></ol> <blockquote><p>apiVersion: &quot;local.storage.openshift.io/v1&quot;</p> <p>kind: &quot;LocalVolume&quot;</p> <p>metadata:</p> <p>name: &quot;local-disks-fs1&quot;</p> <p>namespace: &quot;local-storage&quot;</p> <p>spec:</p> <p>nodeSelector:</p> <p>nodeSelectorTerms:</p> <ul><li><p>matchExpressions:</p></li> <li><p>key: kubernetes.io/hostname</p></li></ul> <p>operator: In</p> <p>values:</p> <p><strong>- worker1.ocp.twentynet.local</strong></p> <p><strong>- worker2.ocp.twentynet.local</strong></p> <p><strong>- worker3.ocp.twentynet.local</strong></p> <p>storageClassDevices:</p> <ul><li>storageClassName: &quot;local-sc&quot;</li></ul> <p>volumeMode: Filesystem</p> <p>fsType: xfs</p> <p><strong>devicePaths</strong>:</p> <p><strong>- /dev/sdb</strong></p> <p>```</p></blockquote> <ol><li>Execute the following command to create the local storage filesystem.</li></ol> <blockquote><p>```</p> <p># oc create –f local-storage-filesystem.yaml</p> <p>```</p></blockquote> <p>The output is as follows.</p> <blockquote><p>```</p> <p>localvolume.local.storage.openshift.io/local-disks-fs created</p> <p>```</p></blockquote> <ol><li><p>A disk is essential to create local block storage. Within the installer VM, copy and update the <em>local-storage-block.yaml</em> file with values for the following parameters.</p> <p>a.  <strong>namespace:</strong> local-storage</p> <p>b.  <strong>values:</strong> Fully qualified domain name of the worker nodes</p> <p>c.  <strong>devicePaths:</strong> partition of the worker nodes to be used to create the block</p></li></ol> <p>A sample <em>local-storage-block.yaml</em> file is as follows.</p> <blockquote><p>```</p> <p>apiVersion: &quot;local.storage.openshift.io/v1&quot;</p> <p>kind: &quot;LocalVolume&quot;</p> <p>metadata:</p> <p>name: &quot;local-disks-fs1&quot;</p> <p><strong>namespace: &quot;local-storage&quot;</strong></p> <p>spec:</p> <p>nodeSelector:</p> <p>nodeSelectorTerms:</p> <ul><li><p>matchExpressions:</p></li> <li><p>key: kubernetes.io/hostname</p></li></ul> <p>operator: In</p> <p>values:</p> <p><strong>- worker1.ocp.twentynet.local</strong></p> <p><strong>- worker2.ocp.twentynet.local</strong></p> <p><strong>- worker3.ocp.twentynet.local</strong></p> <p>storageClassDevices:</p> <ul><li>storageClassName: &quot;local-sc&quot;</li></ul> <p>volumeMode: Filesystem</p> <p>fsType: xfs</p> <p><strong>devicePaths:</strong></p> <p><strong>- /dev/sdb</strong></p> <p>```</p></blockquote> <ol><li>Execute the following command to create the resource.</li></ol> <blockquote><p>```</p> <p># oc create –f local-storage-block.yaml</p> <p>```</p></blockquote> <p>The output is as follows.</p> <blockquote><p>```</p> <p>localvolume.local.storage.openshift.io/local-disks created</p> <p>```</p></blockquote> <ol><li>Execute the following command to verify that all of the pods are up and running and that the Storage Class and PVs exist.</li></ol> <blockquote><p>```</p> <p># oc get pod –n local-storage</p> <p>```</p></blockquote> <p>The output is similar to the following.</p> <blockquote><p>```</p> <p>NAME READY STATUS RESTARTS AGE</p> <p>local-disks-fs-local-diskmaker-6258p 1/1 Running 0 6d19h</p> <p>local-disks-fs-local-diskmaker-8rzgc 1/1 Running 0 6d19h</p> <p>local-disks-fs-local-diskmaker-c6vch 1/1 Running 0 6d19h</p> <p>local-disks-fs-local-provisioner-598f5 1/1 Running 0 6d19h</p> <p>local-disks-fs-local-provisioner-5g4vp 1/1 Running 0 6d19h</p> <p>local-disks-fs-local-provisioner-sxh5x 1/1 Running 0 6d19h</p> <p>local-disks-fs1-local-diskmaker-5p4s7 1/1 Running 0 4d17h</p> <p>local-disks-fs1-local-diskmaker-9jh6r 1/1 Running 0 4d17h</p> <p>local-disks-fs1-local-diskmaker-z62vd 1/1 Running 0 4d17h</p> <p>local-disks-fs1-local-provisioner-gjkn4 1/1 Running 0 4d17h</p> <p>local-disks-fs1-local-provisioner-kmdzt 1/1 Running 0 4d17h</p> <p>local-disks-fs1-local-provisioner-r28th 1/1 Running 0 4d17h</p> <p>local-disks-local-diskmaker-5wpr9 1/1 Running 0 6d19h</p> <p>local-disks-local-diskmaker-cg952 1/1 Running 0 6d19h</p> <p>local-disks-local-diskmaker-s7wvr 1/1 Running 0 6d19h</p> <p>local-disks-local-provisioner-dvg58 1/1 Running 0 6d19h</p> <p>local-disks-local-provisioner-lkl6p 1/1 Running 0 6d19h</p> <p>local-disks-local-provisioner-z4nj6 1/1 Running 0 6d19h</p> <p>local-storage-operator-688c6cf7fb-k95kq 1/1 Running 0 5d19h</p> <p>mongodb-1-9rrv4 1/1 Running 0 3d21h</p> <p>mongodb-1-deploy 0/1 Completed 0 3d21h</p> <p>mysql-ocs-fs-0 1/2 CrashLoopBackOff 252 21h</p> <p>mysql-ocs-fs-1 1/2 CrashLoopBackOff 253 21h</p> <p>```</p></blockquote> <p><strong>NOTE</strong></p> <p>For more information on the Red Hat Local Storage, refer to Red Hat documentation at <a href="https://docs.openshift.com/container-platform/4.2/storage/persistent_storage/persistent-storage-local.html" target="_blank" rel="noopener noreferrer">https://docs.openshift.com/container-platform/4.2/storage/persistent_storage/persistent-storage-local.html<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h1 id="securing-redhat-openshift-container-platform-using-sysdig-secure-and-sysdig-monitor"><a href="#securing-redhat-openshift-container-platform-using-sysdig-secure-and-sysdig-monitor" class="header-anchor">#</a> Securing RedHat OpenShift Container Platform using Sysdig Secure and Sysdig Monitor</h1> <h2 id="introduction-2"><a href="#introduction-2" class="header-anchor">#</a> Introduction</h2> <p>This section describes how to install the Sysdig agents in an automated way on Red Hat OpenShift Container Platform 4 running on HPE Synergy Composable Infrastructure. Sysdig agents can be installed on a wide array of Linux hosts. The assumption is that the installation user will run the Sysdig agent as a pod which then enables the Sysdig agent to automatically detect and monitor Red Hat OpenShift Container Platform 4.</p> <p></p> <p>To install Sysdig agents on the Red Hat OpenShift Container Platform 4 nodes, use the repository located on the HPE OpenShift Solutions GitHub at</p> <p>[<em>https://github.com/hewlettpackard/hpe-solutions-openshift</em>]. This repository contains</p> <p>Ansible plays and scripts to automate installation. Sysdig security components are stored at the location <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/platform/security-sysdig</em>.</p> <p></p> <h2 id="playbooks-for-configuring-sysdig-for-openshift-container-platform"><a href="#playbooks-for-configuring-sysdig-for-openshift-container-platform" class="header-anchor">#</a> Playbooks for configuring Sysdig for OpenShift Container Platform</h2> <p>To install the Sysdig agents in an automated way on Red Hat OpenShift Container Platform 4, use the repository located at the HPE OpenShift Solutions GitHub at <a href="https://github.com/hewlettpackard/hpe-solutions-openshift" target="_blank" rel="noopener noreferrer">https://github.com/hewlettpackard/hpe-solutions-openshift<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>. This repository contains the following:</p> <ul><li><p><strong>Playbooks:</strong> This folder contains the playbooks required for Sysdig agent installation.</p></li> <li><p><strong>Roles:</strong> This folder contains a role called &quot;sysdig-agent-deploy-ocp&quot; which is responsible for performing the actions required for Sysdig agent integration.</p></li> <li><p><strong>Hosts:</strong> This is the inventory file which will be used by the installer VM to reference hosts during sysdig agent deployment. Provide the installer VM IP or fully qualified domain name in this file.</p></li> <li><p><strong>site.yaml</strong>: This file defines the entire workflow for Sysdig integration.</p></li></ul> <p></p> <h3 id="prerequisites-4"><a href="#prerequisites-4" class="header-anchor">#</a> Prerequisites</h3> <p>To successfully deploy Sysdig agents on the nodes, the following prerequisites must be met:</p> <ul><li><p>Red Hat OpenShift Container Platform 4 is up and running.</p></li> <li><p>Worker nodes in the Red Hat OpenShift Container Platform deployment could be virtual or physical and must be running RHCOS/RHEL operating system.</p></li> <li><p>The installation user has SaaS-based access to Sysdig Secure and Sysdig Monitor for the purpose of container security.</p></li> <li><p>The installation user has admin privileges for Sysdig Secure and Sysdig Monitor.</p></li> <li><p>Sysdig agents with version 9.5.0 are deployed on OpenShift Container Platform.</p></li> <li><p>The installation user has a valid access token that is given by Sysdig and is specific to their credentials on Sysdig Monitor and Sysdig Secure.</p></li></ul> <h3 id="custom-attributes-or-variable-files-and-plays"><a href="#custom-attributes-or-variable-files-and-plays" class="header-anchor">#</a> Custom attributes or variable files and plays</h3> <p>Each playbook has a role associated with it. Each role has a set of tasks under the &quot;<em>task</em>&quot; folder and variables under the &quot;<em>var</em>&quot; folder. These variable values must be defined by the installation user according to the installation environment prior to running the plays:</p> <ul><li><p><strong>sysdig-agent-deploy-ocp/vars/main.yml</strong>: Used during Sysdig agent deployment to OpenShift. This file contains Sysdig related variables.</p></li> <li><p><strong>sysdig-agent-deploy-ocp/tasks/main.yml</strong>: This file contains the actual Sysdig agent installation steps.</p></li> <li><p><strong>sysdig-agent-deploy-ocp/files/sysdig-agent-configmap.yaml</strong>: This file is provided by Sysdig and handles the Sysdig software related configuration.</p></li> <li><p><strong>sysdig-agent-deploy-ocp/files/sysdig-agent-daemonset-redhat-openshift.yaml</strong>: This file is provided by Sysdig and handles the Sysdig daemon related configuration.</p></li></ul> <p></p> <h3 id="executing-the-playbooks"><a href="#executing-the-playbooks" class="header-anchor">#</a> Executing the playbooks</h3> <p>This section describes the steps that need to be performed to use the playbooks:</p> <ol><li><p>Login to the installer VM.</p></li> <li><p>Browse the cloned directory and navigate to the following sub-directory.</p></li></ol> <blockquote><p>```</p> <p># cd etc/ansible/hpe-solutions-openshift/synergy/scalable/platform/security-sysdig</p> <p>```</p></blockquote> <ol><li>Update the variables in the following files.</li></ol> <p>a.  Provide the OpenShift Installer VM fully qualified domain name or IP address under [master].</p> <p>b.  Edit the file roles/<em>sysdig-agent-deploy-ocp/vars/main.yml</em></p> <div class="language- extra-class"><pre><code>i.  Provide a value for the project name for Sysdig integration with OpenShift container Platform under the “projectname” variable.  

ii. Provide the Sysdig access key/token value. This value is retrieved from the user setting by logging into Sysdig Secure or Sysdig Monitor GUI in “accesskeyval” variable.  
</code></pre></div><p>c.  Edit the file /<em>roles/sysdig-agent-deploy-ocp/files/sysdig-agent-configmap.yaml</em></p> <div class="language- extra-class"><pre><code>i.  Enter “OpenShift” as the cluster type.  

ii. Enter the Sysdig Collector address and port. Check with Sysdig on which collector is accessible in the installation environment and over which port.  

iii. Hewlett Packard Enterprise recommends access of Sysdig collector over Secure Socket Layer (SSL) port. For both the “ssl” and “ssl certificate validate” keys, set the value as “true”.  

iv. Set the variable related to the underlying Kubernetes deployment (OpenShift in this solution), to true.  

v.  Enter the cluster name of the OpenShift cluster. 
</code></pre></div> <ol><li>Run the play using following command.</li></ol> <blockquote><p>```</p> <p># ansible-playbook -i hosts site.yml</p> <p>```</p></blockquote> <ol><li>To verify the deployment, log in to the installer VM and execute the following command.</li></ol> <blockquote><p>```</p> <p># oc get pods</p> <p>```</p> <p>The output of this command displays the Sysdig agent names running on each of the nodes within the OpenShift cluster as shown in Figure 10. If you see a pod with a pending status, then there might be a possibility that the underlying OpenShift node is not functional.</p></blockquote> <p>![Sysdig agents running on OpenShift nodes](/figure10.png)</p> <p>![][10]<br>
**Figure 10. **Sysdig agents running on OpenShift nodes</p> <ol><li><p>Execute the following command to verify the number of pods that are currently up and running in the OpenShift Container Platform deployment.</p> <p>```</p> <p># oc get nodes</p> <p>```</p></li></ol> <p>The output is shown in Figure 11.</p> <p>![OpenShift nodes information](/figure11.png)</p> <p>![][11]**Figure 11. **OpenShift nodes information</p> <p></p> <ol><li><p>Login to Sysdig Secure web interface at [https://sysdig.com/] as a user with administrative privileges.</p></li> <li><p>From the Sysdig Secure web interface, click the icon named **POLICY EVENTS **and you will see the web interface for **Policy Events **tab. On the **Policy Events **tab, click the **Groupings **drop-down list and select <strong>Entire Infrastructure</strong>. The user with administrative privileges should be able to see all the OpenShift nodes as in <strong>Figure 12</strong>.</p></li></ol> <p>![ OpenShift cluster in Sysdig Secure](/figure12.png)</p> <p>![][12]</p> <p>**Figure 12. **OpenShift cluster in Sysdig Secure</p> <ol><li><p>From the Sysdig Monitor web interface, click <strong>Explore</strong> icon and you will see the web interface for the <em>Explore</em>** **tab. On the <em>Explore</em> tab, click the **Data Source <strong>(two rectangles) drop-down menu and select the data source named <em>Sysdig Agents</em></strong> **from the drop-down list.</p></li> <li><p>Open the <em>Groupings</em>** **drop-down list and select <strong>Clusters and Nodes</strong>. The user with administrative privileges should be able to see all the OpenShift nodes as shown in <strong>Figure 13</strong>.</p></li></ol> <p>![OpenShift cluster in Sysdig Monitor](/figure13.png)</p> <p>![][13]</p> <p><strong>Figure 13</strong>.** **OpenShift cluster in Sysdig Monitor</p> <p><strong>NOTE</strong></p> <p>For an explanation of host requirements for agent installation, refer to, <a href="https://docs.sysdig.com/en/host-requirements-for-agent-installation.html" target="_blank" rel="noopener noreferrer">https://docs.sysdig.com/en/host-requirements-for-agent-installation.html<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p>It is recommended to use port 6443 to transfer data over Secure Socket Layer/ Transport Layer Security (SSL/TLS) protocol. Sysdig agents transfer data to Sysdig Cloud over HTTPS that encrypts and decrypts the requests and the responses returned by the Sysdig Cloud.</p> <ol><li>From the Sysdig Monitor web interface, click <strong>EXPLORE</strong> icon. On the <em>Explore</em> tab, click the **Data Source <strong>(two rectangles) drop-down menu and select the data source named <em>Sysdig Agents</em></strong> **from the drop-down list. Open the **Groupings **drop-down list and select <strong>Deployment and Pods</strong>. A user with administrative privileges should be able to see all the agents and their details as shown in <strong>Figure 14</strong>.</li></ol> <p>![Sysdig agents running on the OpenShift cluster ](/figure14.png)</p> <p>![][14]</p> <p>**Figure 14. **Sysdig agents running on the OpenShift cluster</p> <p><strong>NOTE</strong></p> <p>This section is based on Sysdig SaaS deployment approach. It is recommended that the installation user waits for some time until the OpenShift node statistics are visible in the Sysdig Secure and Sysdig Monitor user interface.</p> <h1 id="physical-worker-node-labeling-in-openshift"><a href="#physical-worker-node-labeling-in-openshift" class="header-anchor">#</a> Physical worker node labeling in OpenShift</h1> <h2 id="introduction-3"><a href="#introduction-3" class="header-anchor">#</a> Introduction</h2> <p>Discovering the node properties and advertising them through node labels can be used to control workload placement in an OpenShift cluster. With OpenShift running on HPE server platforms, organizations can automate the discovery of hardware properties and use that information to schedule workloads that benefit from the different capabilities that the underlying hardware provides. Using HPE iLO and its REST or Redfish API- based discovery capabilities (proliantutils), the following properties can be discovered about the nodes:</p> <ul><li><p>Presence of GPUs</p></li> <li><p>Underlying RAID configurations</p></li> <li><p>Presence of disks by type</p></li> <li><p>Persistent-memory availability</p></li> <li><p>Status of CPU virtualization features</p></li> <li><p>SR-IOV capabilities</p></li> <li><p>CPU architecture</p></li> <li><p>CPU core count</p></li> <li><p>Platform information including model, iLO and BIOS versions</p></li> <li><p>Memory capacity</p></li> <li><p>UEFI Security settings</p></li> <li><p>Health status of compute, storage, and network components</p></li></ul> <p>After these properties are discovered for the physical worker nodes, OpenShift node labeling can be applied to group nodes based on the underlying features of the nodes. By default, every node will have its node name as a label.</p> <p>The following properties can be used to label nodes:</p> <ol><li>Overall health status of the node.</li></ol> <blockquote><p>If current status of &quot;BIOS, Fans, Temperature Sensors, Battery, Processor, Memory, Network, and Storage&quot; is ok, node health status is labeled as &quot;Ok&quot;. Otherwise it will appear as &quot;Degraded&quot;.</p></blockquote> <ol><li>Overall security status of the node.</li></ol> <blockquote><p>If the current status of the following BIOS configuration items (which are important for security) are as expected, then security status of the node is &quot;Ok&quot;. Otherwise, they will be labeled as &quot;Degraded&quot;.</p> <p><strong>NOTE</strong></p> <p>Based on the HPE Gen10 Security Reference Guide, the recommended values for the chosen parameters are as follows.</p></blockquote> <ul><li><p>Secure Boot: Enabled</p></li> <li><p>Asset tag: Locked</p></li> <li><p>UEFI Shell Script Verification: Enabled</p></li> <li><p>UEFI Shell Startup: Disabled</p></li> <li><p>Processor AES: Enabled</p></li></ul> <blockquote><p>For more information, refer HPE Gen10 Security Reference Guide at <a href="https://support.hpe.com/hpesc/public/docDisplay?docId=a00018320en_us" target="_blank" rel="noopener noreferrer">https://support.hpe.com/hpesc/public/docDisplay?docId=a00018320en_us<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p></blockquote> <ol><li>Custom Labeling.</li></ol> <blockquote><p>User defined labels (key, value) are assigned to desired physical worker nodes. Users can use these Python scripts to retrieve the properties of the underlying hardware and then decide on required labels that should be assigned to each physical worker nodes.</p></blockquote> <h2 id="sample-scripts-for-labeling-worker-nodes"><a href="#sample-scripts-for-labeling-worker-nodes" class="header-anchor">#</a> Sample scripts for labeling worker nodes</h2> <p>To label the physical worker node in Red Hat OpenShift Container Platform, use the repository located at the HPE OpenShift Solutions GitHub at <a href="https://github.com/hewlettpackard/hpe-solutions-openshift" target="_blank" rel="noopener noreferrer">https://github.com/hewlettpackard/hpe-solutions-openshift<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>. This repository contains Python scripts to automate the labeling of physical worker nodes by discovering the physical node properties in a Red Hat OpenShift Container Platform 4 deployment and advertising them through node labels. Node labels can be targeted for deployment using node selectors which can be set at a project level (can be used to restrict which nodes a project gets access to) or pod level.</p> <h3 id="contents-of-the-repository"><a href="#contents-of-the-repository" class="header-anchor">#</a> Contents of the repository</h3> <ol><li><strong>config.json</strong>: This file contains variables holding information about the OpenShift specific environment variables.</li></ol> <ul><li><p><strong>kubeconfig_path</strong>: Specifies the path of kubeconfig and this path is used by the &quot;oc&quot; command at runtime.</p></li> <li><p><strong>oc_command_path</strong>: Specifies the oc command path and is used to run the &quot;oc&quot; command.</p></li></ul> <ol><li><strong>hosts.json</strong>: This is the inventory file which will be used by OpenShift installer VM to reference physical worker nodes and user-defined labels.</li></ol> <ul><li><p><strong>host_fqdn</strong>: Specifies the physical worker node fully qualified domain name or IP.</p></li> <li><p><strong>ilo_ip</strong>: iLO IP of the physical worker node.</p></li> <li><p><strong>username</strong>: Username used to log in to the iLO of the physical worker node.</p></li> <li><p><strong>password</strong>: Password to log in to the iLO of the physical worker node.</p></li> <li><p><strong>custom_label_required</strong>: The value is &quot;yes&quot; if the user wishes to use custom labels. Otherwise, it should be set to &quot;no&quot;.</p></li> <li><p><strong>custom_labels</strong>: Specify the custom labels key and value.</p></li></ul> <ol><li><p><strong>json_parser.py</strong>: This file contains the logic to derive value of any standalone key or nested keys from the json file.</p></li> <li><p><strong>physical_node_labelling.py</strong>: This file contains the logic to derive the physical hardware properties and label of the Red Hat OpenShift Container Platform physical worker nodes based on properties and user-defined label names. To extract hardware properties, Python module &quot;proliantutils&quot; is used in this script.</p></li></ol> <h3 id="prerequisites-5"><a href="#prerequisites-5" class="header-anchor">#</a> Prerequisites</h3> <ol><li><p>Red Hat OpenShift Container Platform 4 is up and running.</p></li> <li><p>There must be at least one physical worker node in the Red Hat OpenShift Container Platform 4 deployment.</p></li></ol> <h3 id="executing-the-playbooks-2"><a href="#executing-the-playbooks-2" class="header-anchor">#</a> Executing the playbooks</h3> <ol><li><p>Login to the installer VM.</p></li> <li><p>Activate the Python 3 virtual environment as mentioned in the section [Installer machine] in this document.</p></li> <li><p>Proliantutils is a set of utility libraries for interfacing and managing various components (like iLO) for HPE ProLiant Servers. Execute the following command to install proliantutils.</p></li></ol> <blockquote><p>```</p> <p># sudo pip install proliantutils==2.9.2</p> <p>```</p></blockquote> <ol><li>Verify the version of proliantutils using the following command.</li></ol> <blockquote><p>```</p> <p># sudo pip freeze | grep proliantutils</p> <p>```</p></blockquote> <p>The output is as follows.</p> <blockquote><p>```</p> <p>You are using pip version 9.0.1, however version 20.0.2 is available.</p> <p>You should consider upgrading via the 'pip install --upgrade pip' command.</p> <p>proliantutils==2.9.2</p> <p>```</p></blockquote> <ol><li>Execute the following command to install sushy module.</li></ol> <blockquote><p>```</p> <p># sudo pip install sushy==3.0.0</p> <p>```</p></blockquote> <ol><li>Verify the version of sushy.</li></ol> <blockquote><p>```</p> <p># sudo pip freeze | grep sushy</p> <p>Output: 3.0.0</p> <p>```</p></blockquote> <ol><li>Execute the following command and navigate to the directory physical-worker labeling.</li></ol> <blockquote><p>```</p> <p># cd etc/ansible/hpe-solutions-openshift/synergy/scalable/platform/physical-workerlabeling</p> <p>```</p></blockquote> <ol><li>Update the <em>config.json</em> and <em>hosts.json</em> files with appropriate values.</li></ol> <blockquote><p>```</p> <p># sudo vi config.json</p> <p># sudo vi hosts.json</p> <p>```</p></blockquote> <ol><li>After the files mentioned in step 8 are updated, execute the script using the following command. The installation user will see the output similar to the following.</li></ol> <blockquote><p>```</p> <p># python physical_node_labelling.py</p> <p>```</p></blockquote> <p>The output is as follows.</p> <blockquote><p>```</p> <p>1: Get the physical worker node details.</p> <p>2: Get current health status of the physical worker node</p> <p>3: Get security parameters of the physical worker node</p> <p>4: Label the physical worker with health status</p> <p>5: Label the physical worker with security status</p> <p>6: Custom labels</p> <p>7: Display current labels on the node</p> <p>8: Quit</p> <p>Enter the choice number:</p> <p>```</p></blockquote> <ol><li>Select option 1 to retrieve the physical worker node details. The output is similar to the following.</li></ol> <blockquote><p>```</p> <p>Enter the choice number: 1</p> <p>{'server1': {'host_fqdn': 'ocs1.jocp.twentynet.local', 'ilo_ip': '10.0.x.x',</p> <p>'username': 'admin', 'password': 'admin123', 'custom_label_required': 'yes',</p> <p>'custom_labels': {'label1': {'label_name': 'trial', 'label_val': 'trail123'},'label2':</p> <p>{'label_name': 'trial2', 'label_val': 'trial456'}}}}</p> <p>```</p></blockquote> <ol><li>Select option 2 to retrieve the current health status of the physical worker node. The output is similar to the following.</li></ol> <blockquote><p>```</p> <p>Enter the choice number: 2</p> <p>{'ocs1.jocp.twentynet.local': 'OK'}</p> <p>```</p></blockquote> <ol><li>Select option 3 to retrieve the security parameters of the physical worker node. The output is similar to the following.</li></ol> <blockquote><p>```</p> <p>Enter the choice number: 3</p> <p>{'ocs1.jocp.twentynet.local': 'Degraded'}</p> <p>```</p></blockquote> <ol><li>Select option 4 to label the physical worker with its current hardware health status. The output is similar to the following.</li></ol> <blockquote><p>```</p> <p>Enter the choice number: 4</p> <p>NAME STATUS ROLES AGE VERSION LABELS</p> <p>ocs1.jocp.twentynet.local Ready worker 6d4h v1.16.2</p> <p>beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,health=OK,kubernet</p> <p>es.io/arch=amd64,kubernetes.io/hostname=ocs1.jocp.twentynet.local,kubernetes</p> <p>.io/os=linux,node-</p> <p>role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos,qa_shree1=1,qa_shree</p> <p>2=2,security=Degraded,shreeman=new_label2,trial2=trial456,trial=trail123</p> <p>Verified - Label health=OK is added to the node ocs1.jocp.twentynet.local</p> <p>b''</p> <p>```</p></blockquote> <ol><li>Select option 5 to label the physical worker with security status. The output is similar to the following.</li></ol> <blockquote><p>```</p> <p>Enter the choice number: 5</p> <p>NAME STATUS ROLES AGE VERSION LABELS</p> <p>ocs1.jocp.twentynet.local Ready worker 6d4h v1.16.2</p> <p>beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,health=OK,kubernet</p> <p>es.io/arch=amd64,kubernetes.io/hostname=ocs1.jocp.twentynet.local,kubernetes</p> <p>.io/os=linux,node-</p> <p>role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos,qa_shree1=1,qa_shree</p> <p>2=2,security=Degraded,shreeman=new_label2,trial2=trial456,trial=trail123</p> <p>Verified - Label security=Degraded is added to the node</p> <p>ocs1.jocp.twentynet.local</p> <p>b''</p> <p>```</p></blockquote> <ol><li>Select option 6 to define custom labels. The output is similar to the following.</li></ol> <blockquote><p>```</p> <p>Enter the choice number: 6</p> <p>NAME STATUS ROLES AGE VERSION LABELS</p> <p>ocs1.jocp.twentynet.local Ready worker 6d4h v1.16.2</p> <p>beta.kubernetes.io/arch=amd64,beta.k</p> <p>ubernetes.io/os=linux,health=OK,kubernetes.io/arch=amd64,kubernetes.io/hostn</p> <p>ame=ocs1.jocp.twentynet</p> <p>.local,kubernetes.io/os=linux,node-</p> <p>role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos,qa_shre</p> <p>e1=1,qa_shree2=2,security=Degraded,shreeman=new_label2,trial2=trial456,trial</p> <p>=trail123</p> <p>Verified - Label trial=trail123 is added the node ocs1.jocp.twentynet.local</p> <p>NAME STATUS ROLES AGE VERSION LABELS</p> <p>ocs1.jocp.twentynet.local Ready worker 6d4h v1.16.2</p> <p>beta.kubernetes.io/arch=amd64,beta.k</p> <p>ubernetes.io/os=linux,health=OK,kubernetes.io/arch=amd64,kubernetes.io/hostn</p> <p>ame=ocs1.jocp.twentynet</p> <p>.local,kubernetes.io/os=linux,node-</p> <p>role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos,qa_shre</p> <p>e1=1,qa_shree2=2,security=Degraded,shreeman=new_label2,trial2=trial456,trial</p> <p>=trail123</p> <p>Verified - Label trial2=trial456 is added the node ocs1.jocp.twentynet.local</p> <p>b''</p> <p>```</p></blockquote> <ol><li>Select option 7 to display current labels on the node. The output is similar to the following.</li></ol> <blockquote><p>```</p> <p>Enter the choice number: 7</p> <p>NAME STATUS ROLES AGE VERSION LABELS</p> <p>ocs1.jocp.twentynet.local Ready worker 6d4h v1.16.2</p> <p>beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,health=OK,kubernet</p> <p>es.io/arch=amd64,kubernetes.io/hostname=ocs1.jocp.twentynet.local,kubernetes</p> <p>.io/os=linux,node-</p> <p>role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos,qa_shree1=1,qa_shree</p> <p>2=2,security=Degraded,shreeman=new_label2,trial2=trial456,trial=trail123</p> <p>```</p></blockquote> <ol><li>Select option 8 to quit the script. The output is shown as follows.</li></ol> <blockquote><p>```</p> <p>Enter the choice number: 8</p> <p>Exiting!!</p> <p>```</p></blockquote> <h3 id="verify-scheduling-of-pods-using-nodeselector"><a href="#verify-scheduling-of-pods-using-nodeselector" class="header-anchor">#</a> Verify scheduling of pods using NodeSelector</h3> <ol><li>Execute the following command to get all the nodes in the Red Hat OpenShift Container Platform installation.</li></ol> <blockquote><p>```</p> <p># oc get nodes</p> <p>```</p> <p>The output is similar to the following.</p> <p>```</p> <p>NAME STATUS ROLES AGE VERSION</p> <p>master1.jocp.twentynet.local Ready master 37d v1.16.2</p> <p>master2.jocp.twentynet.local Ready master 37d v1.16.2</p> <p>master3.jocp.twentynet.local Ready master 37d v1.16.2</p> <p>worker1.jocp.twentynet.local Ready worker 34d v1.16.2</p> <p>worker2.jocp.twentynet.local Ready worker 31d v1.16.2</p> <p>worker3.jocp.twentynet.local Ready worker 31d v1.16.2</p> <p>worker4.jocp.twentynet.local Ready worker 16d v1.16.2</p> <p>worker5.jocp.twentynet.local Ready worker 16d v1.16.2</p> <p>ocs1.jocp.twentynet.local Ready worker 16d v1.16.2</p> <p>```</p> <p><strong>NOTE</strong></p> <p>Node “ocs1.jocp.twentynet.local” is the physical worker node and other worker nodes are virtual machines.</p></blockquote> <ol><li><p>Execute the following command to create a new project.</p> <p>```</p> <p># oc new-project test4</p> <p>```</p></li> <li><p>Execute the following command to create a new application in the newly created project “test4”.</p></li></ol> <blockquote><p>```</p> <p># oc new-app django-psql-example</p> <p>```</p></blockquote> <ol><li><p>Execute the following command to get the pods for the newly created application.</p> <p>```</p> <p># oc get pods</p> <p>```</p></li></ol> <blockquote><p>The output is similar to the following.</p></blockquote> <p>```</p> <p>NAME READY STATUS RESTARTS AGE</p> <p>postgresql-1-bgzjm 0/1 Running 0 15s</p> <p>postgresql-1-deploy 1/1 Running 0 23s</p> <p>```</p> <ol><li><p>Execute the following command to describe the application pod.</p> <p>```</p> <p># oc describe pod postgresql-1-bgzjm</p> <p>```</p></li></ol> <blockquote><p>The output is similar to the following.</p></blockquote> <p>```</p> <p>Name: postgresql-1-bgzjm</p> <p>Namespace: test</p> <p>Priority: 0</p> <p>Node: worker1.jocp.twentynet.local/20.0.x.x</p> <p>```</p> <ol><li><p>Execute the following command to check the deployment configuration.</p> <p>```</p> <p># oc get dc</p> <p>```</p></li></ol> <blockquote><p>The output is similar to the following.</p></blockquote> <p>```</p> <p>NAME REVISION DESIRED CURRENT TRIGGERED BY</p> <p>django-psql-example 0 1 0 config,image(django-psql-example:latest)</p> <p>postgresql 1 1 1 config,image(postgresql:10)</p> <p>```</p> <ol><li><p>Patch the deployment configuration with the “NodeSelector” as “health=OK”. This labeling will notify the scheduler about the application associated with this deployment configuration and is expected to run on a node with a label of “health=OK”. The command is as shown:</p> <p>```</p> <p># oc patch dc postgresql –p '{&quot;spec&quot;: {&quot;template&quot;:{&quot;spec&quot;:{&quot;nodeSelector&quot;:{&quot;health&quot;: &quot;OK&quot;}}}}}'</p> <p>deploymentconfig.apps.openshift.io/postgresql patched</p> <p>```</p></li> <li><p>Execute the following command to validate new pods that are created for the same application after the deployment configuration is updated with node selector information.</p></li></ol> <blockquote><p>```</p> <p># oc get pods</p> <p>```</p> <p>The output is similar to the following.</p></blockquote> <p>```</p> <p>NAME READY STATUS RESTARTS AGE</p> <p>postgresql-1-deploy 0/1 Completed 0 5m7s</p> <p>postgresql-2-deploy 0/1 Completed 0 56s</p> <p>postgresql-2-vvqcf 1/1 Running 0 45s</p> <p>```</p> <ol><li><p>Execute the following command to describe the newly created application pod to validate to see if it is deployed on the node with the desired label.</p> <p>```</p> <p># oc describe pod postgresql-2-vvqcf</p> <p>```</p></li></ol> <blockquote><p>The output is similar to the following. It shows that the application is deployed on the node <em>“ocs1.jocp.twentynet.local”</em> which was labeled as “health=OK” in step 11 of the section [Executing the playbooks] in this document.</p></blockquote> <p>```</p> <p>Name: postgresql-2-vvqcf</p> <p>Namespace: test</p> <p>Priority: 0</p> <p>Node: ocs1.jocp.twentynet.local /20.0.x.x</p> <p>Node-Selectors: health=OK</p> <p>```</p> <p><strong>NOTE</strong></p> <p>These scripts have been tested on Red Hat OpenShift Container Platform 4.3 with the following configuration parameters:</p> <ul><li><p>Worker nodes are running RHCOS as the operating system</p></li> <li><p>Installer VM OS Version: RHEL 7.6</p></li> <li><p>Python: 3.6.9</p></li> <li><p>proliantutils: 2.9.2</p></li> <li><p>sushy: 3.0.0</p></li></ul> <h1 id="openshift-operators"><a href="#openshift-operators" class="header-anchor">#</a> OpenShift Operators</h1> <h2 id="introduction-4"><a href="#introduction-4" class="header-anchor">#</a> Introduction</h2> <p>Operators are pieces of software that ease the operational complexity of running other pieces of software. They act like an extension of the software vendor’s engineering team, watching over a Kubernetes environment (such as OpenShift Container Platform) and using the current state to make decisions in real time. Any container implementation requires certain operators that need to be enabled for use by the end user. This requirement is satisfied by using a combination of Python and Shell scripts and their execution for automating the installation of these operators.</p> <p>HPE has deployed the following operators in an automated fashion:</p> <ul><li><p>Kiali Operator</p></li> <li><p>Jaeger Operator</p></li> <li><p>Red Hat OpenShift Service Mesh</p></li> <li><p>Prometheus Operator</p></li> <li><p>Grafana Operator</p></li> <li><p>Elasticsearch Operator</p></li> <li><p>Logstash Operator</p></li> <li><p>Kibana Operator</p></li></ul> <h2 id="scripts-for-configuring-the-operators"><a href="#scripts-for-configuring-the-operators" class="header-anchor">#</a> Scripts for configuring the operators</h2> <p>This section provides details on the scripts developed to automate the installation of operators on the OpenShift Container Platform cluster. The scripts to install operators can be found in the installer VM at /<em>etc/ansible/hpe-solutions-openshift/synergy/scalable/platform/operator_install</em>.</p> <ul><li><p><strong>operator_install.py</strong> - main python script which installs the required operators.</p></li> <li><p><strong>elasticsearch.sh</strong> - shell script to install the elasticsearch operator.</p></li> <li><p><strong>kibana.sh</strong> - shell script to install the kibana operator.</p></li> <li><p><strong>logstash.sh</strong> - shell script to install the logstash operator.</p></li> <li><p><strong>userinput.json</strong> - input json file filled by the installation user.</p></li> <li><p><strong>pingtest.py</strong> - script used within operator_install.py to ping IP's along with a port number.</p></li> <li><p><strong>enableyumrepos.sh</strong> – shell script to enable yum installation of the required packages.</p></li></ul> <h2 id="installing-the-operators-on-the-openshift-container-platform-cluster"><a href="#installing-the-operators-on-the-openshift-container-platform-cluster" class="header-anchor">#</a> Installing the operators on the OpenShift Container Platform cluster</h2> <ol><li><p>Login to the installer VM.</p></li> <li><p>Execute the following command to install Java.</p></li></ol> <blockquote><p>```</p> <p># sudo yum install java</p> <p>```</p></blockquote> <ol><li>Execute the following command to install Python requests module.</li></ol> <blockquote><p>```</p> <p># sudo pip install requests</p> <p>```</p></blockquote> <ol><li>Update the <em>userinput.json</em> file found at <em>/etc/ansible/hpe-solutions-openshift/synergy/scalable/platform/operator_install</em> with the following setup configuration details:</li></ol> <ul><li><p>OpenShift Container Platform cluster username and password</p></li> <li><p>OpenShift Server (api.domain.base_domain) and port number (OpenShift Container Platform runs on port 6443 by default)</p></li> <li><p>OpenShift Installer VM IP address</p></li> <li><p>OpenShift Project Name</p></li> <li><p>OpenShift OC client tool path (leave it empty if a path has been set)</p></li></ul> <blockquote><p><strong>NOTE</strong></p> <p>The operator list, channel details, operator source and install plans can be used if the user wants to install additional operators. For the default installation of the required operators, the user should not modify these fields.</p></blockquote> <ol><li><p>Execute the following command to install the operators discussed in the [Introduction] of this document.</p> <p>```</p> <p># /etc/ansible/hpe-solutions-openshift/synergy/scalable/platform/operator_install</p> <p># python –W ignore operator_install.py –i userinput.json</p> <p>```</p></li></ol> <p>Figure 15 shows the output after the script to create the operators is executed.</p> <p>![ Operator installation script output](/figure15.png)</p> <p>![C:\Users\vutkur\Desktop\OpenShift REST API's\cli output.JPG]</p> <p><strong>Figure 15</strong> Operator installation script output</p> <h2 id="validation-of-the-operator-installation"><a href="#validation-of-the-operator-installation" class="header-anchor">#</a> Validation of the Operator Installation</h2> <p>The required operators will be created after the execution of the script and they will be reflected in the OpenShift console. This section outlines the steps to verify the operators created through script and are reflected in the GUI:</p> <ol><li><p>Login to the <strong>OpenShift Console</strong> as the user with administrative privileges.</p></li> <li><p>Navigate to <strong>Operators</strong> -&gt; <strong>Installed</strong> <strong>Operators</strong> -&gt; select your project name.</p></li> <li><p>The operators will be displayed in the screen as shown in Figure 16.</p></li></ol> <p>![Installed Operators within OpenShift console](/figure16.png)</p> <p>![C:\Users\vutkur\Desktop\OpenShift REST API's\installed operators.JPG]</p> <p><strong>Figure</strong> <strong>16</strong> Installed Operators within OpenShift console</p> <p><strong>NOTE</strong></p> <p>The kibana dashboard can be accessed by visiting the installer VM IP address and the port number 5601 for instance, .</p> <h1 id="validating-openshift-container-platform-deployment"><a href="#validating-openshift-container-platform-deployment" class="header-anchor">#</a> Validating OpenShift Container Platform deployment</h1> <p>After the cluster is up and running with OpenShift Local Storage Operator, the cluster configuration is validated by deploying a MongoDB pod with persistent volume and Yahoo Cloud Service Benchmarking (YCSB). This section covers the steps to validate the OpenShift Container Platform deployment.</p> <h2 id="deploying-mongodb-application"><a href="#deploying-mongodb-application" class="header-anchor">#</a> Deploying MongoDB application</h2> <ol><li><p>Login to the installer VM.</p></li> <li><p>Download the Red Hat scripts specific to the MongoDB application at <a href="https://github.com/red-hat-storage/SAWORF" target="_blank" rel="noopener noreferrer">https://github.com/red-hat-storage/SAWORF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> using the following command.</p> <p>```</p> <p># git clone <a href="https://github.com/red-hat-storage/SAWORF.git" target="_blank" rel="noopener noreferrer">https://github.com/red-hat-storage/SAWORF.git<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>```</p></li> <li><p>From within the red-hat-storage repository, navigate to the folder SAWORF/OCS3/MongoDB/blog2.</p></li> <li><p>Update the <em>create_and_load.sh</em> script with <strong>glusterfs</strong> in place of <strong>local-sc</strong> content. Example is shown as follows.</p></li></ol> <blockquote><p>```</p> <p>mongodb_ip=$(oc get svc -n ${PROJECT_NAME} | grep -v <strong>glusterfs</strong> | grep mongodb | awk '{print $3}'</p> <p>```</p></blockquote> <ol><li><p>Create MongoDB and YCSB pods and load the sample data.</p> <p>Update the following command with appropriate values for the command line parameters and execute the command to create the MongoDB and YCSB pods and also to load the sample data.</p></li></ol> <blockquote><p>```</p> <p>./create_and_load_mongodb $PROJECT_NAME $OCP_TEMPLATE $MONGODB_MEMORY_LIMIT $PV_SIZE $MONGODB_VERSION $YCSB_WORKLOAD $YCSB_DISTRIBUTION $YCSB_RECORDCOUNT $YCSB_OPERATIONCOUNT $YCSB_THREADS $LOG_DIR</p> <p>```</p></blockquote> <p>Example command is shown as follows.</p> <blockquote><p>```</p> <p># ./create_and_load_mongodb dbtest mongodb-persistent 4Gi 10Gi 3.6 workloadb uniform 4000 4000 8 root /mnt/data/</p> <p>```</p></blockquote> <p>The output should look similar to the following.</p> <p>```</p> <p>--&gt; Deploying template &quot;openshift/mongodb-persistent&quot; to project dbtest</p> <p>MongoDB</p> <hr> <blockquote><p>MongoDB database service, with persistent storage. For more information about using this template, including OpenShift considerations, see documentation in the upstream repository: https://github.com/sclorg/mongodb-container.</p></blockquote> <p>NOTE: Scaling to more than one replica is not supported. You must have persistent volumes available in your cluster to use this template.</p> <p>The following service(s) have been created in your project: mongodb.</p> <p>Username: redhat</p> <p>Password: redhat</p> <p>Database Name: redhatdb</p> <p>Connection URL: mongodb://redhat:redhat@mongodb/redhatdb</p> <p>For more information about using this template, including OpenShift considerations, see documentation in the upstream repository: https://github.com/sclorg/mongodb-container.</p> <p>With parameters:</p> <p>* Memory Limit=4Gi</p> <p>* Namespace=openshift</p> <p>* Database Service Name=mongodb</p> <p>* MongoDB Connection Username=redhat</p> <p>* MongoDB Connection Password=redhat</p> <p>* MongoDB Database Name=redhatdb</p> <p>* MongoDB Admin Password=redhat</p> <p>* Volume Capacity=10Gi</p> <p>* Version of MongoDB Image=3.6</p> <p>--&gt; Creating resources ...</p> <p>secret &quot;mongodb&quot; created</p> <p>service &quot;mongodb&quot; created</p> <p>error: persistentvolumeclaims &quot;mongodb&quot; already exists</p> <p>deploymentconfig.apps.openshift.io &quot;mongodb&quot; created</p> <p>--&gt; Failed</p> <p>pod/ycsb-pod created</p> <p>```</p> <ol><li>Execute the following command to run the check_db_size script.</li></ol> <blockquote><p>```</p> <p># ./check_db_size $PROJECT_NAME</p> <p>```</p></blockquote> <p>The output should look similar to the following.</p> <p>```</p> <p>MongoDB shell version v3.6.12</p> <p>connecting to: mongodb://172.x.x.x:27017/redhatdb?gssapiServiceName=mongodb</p> <p>Implicit session: session {&quot;id&quot; : UUID(&quot;c0a76ddc-ea0b-4fc-88fd-045d0f98b2&quot;) }</p> <p>MongoDB server version: 3.6.3</p> <p>{</p> <p>&quot;db&quot; : &quot;redhatdb&quot;,</p> <p>&quot;collections&quot; : 1,</p> <p>&quot;views&quot; : 0,</p> <p>&quot;objects&quot; : 4000,</p> <p>&quot;avgObjSize&quot; : 1167.877,</p> <p>&quot;dataSize&quot; : 0.004350680857896805,</p> <p>&quot;storageSize&quot; : 0.00446319580078125,</p> <p>&quot;numExtents&quot; : 0,</p> <p>&quot;indexes&quot; : 1,</p> <p>&quot;indexSize&quot; : 0.0001068115234375,</p> <p>&quot;fsUsedSize&quot; : 1.0311393737792969,</p> <p>&quot;fsTotalSize&quot; : 99.951171875,</p> <p>&quot;ok&quot; : 1</p> <p>}</p> <p>```</p> <h2 id="verifying-mongodb-pod-deployment"><a href="#verifying-mongodb-pod-deployment" class="header-anchor">#</a> Verifying MongoDB pod deployment</h2> <ol><li>Execute the following command to verify the persistent volume associated with MongoDB pods.</li></ol> <blockquote><p>```</p> <p># oc get pv|grep mongodb</p> <p>```</p></blockquote> <p>The output should look similar to the following.</p> <p>```</p> <p>local-pv-e7f10f65 100Gi RWO Delete Bound dbtest/mongodb local-sc 26h</p> <p>```</p> <ol><li>Execute the following command to verify the persistent volume claim associated with MongoDB pods.</li></ol> <blockquote><p>```</p> <p># oc get pvc</p> <p>```</p></blockquote> <p>The output should look similar to the following.</p> <p>```</p> <p>local-pv-e7f10f65 100Gi RWO Delete Bound dbtest/mongodb local-sc 26h</p> <p>```</p> <ol><li>Execute the following command to ensure MongoDB and YCSB pods are up and running.</li></ol> <blockquote><p>```</p> <p># oc get pod</p> <p>```</p></blockquote> <p>The output should look similar to the following.</p> <p>```</p> <p>NAME               READY   STATUS      RESTARTS   AGE</p> <p>mongodb-1-deploy   0/1     Completed   0          3m40s</p> <p>mongodb-1-skbwq    1/1     Running     0          3m36s</p> <p>ycsb-pod           1/1     Running     0          3m41s</p> <p>```</p> <p><strong>NOTE</strong></p> <p>For more information about deploying MongoDB application along with YCSB, refer to the Red Hat documentation at <a href="https://www.redhat.com/en/blog/multitenant-deployment-mongodb-using-openshift-container-storage-and-using-ycsb-test-performance" target="_blank" rel="noopener noreferrer">https://www.redhat.com/en/blog/multitenant-deployment-mongodb-using-openshift-container-storage-and-using-ycsb-test-performance<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h1 id="resources-and-additional-links"><a href="#resources-and-additional-links" class="header-anchor">#</a> Resources and additional links</h1> <p>Red Hat, <a href="https://www.redhat.com" target="_blank" rel="noopener noreferrer">https://www.redhat.com<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>Red Hat OpenShift Container Platform 4.3 Documentation, <a href="https://docs.openshift.com/container-platform/4.3/welcome/index.html" target="_blank" rel="noopener noreferrer">https://docs.openshift.com/container-platform/4.3/welcome/index.html<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>HPE Synergy, <a href="https://www.hpe.com/info/synergy" target="_blank" rel="noopener noreferrer">https://www.hpe.com/info/synergy<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>HPE Solutions for OpenShift GitHub, <a href="https://github.com/hewlettpackard/hpe-solutions-openshift" target="_blank" rel="noopener noreferrer">https://github.com/hewlettpackard/hpe-solutions-openshift<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>HPE FlexFabric 5945 switching, [https://www.hpe.com/us/en/product-catalog/networking/networking-switches/pip.hpe-flexfabric-5945-switch-series.1009148840.html]</p> <p>HPE Workload Aware Security for Linux, <a href="https://h20392.www2.hpe.com/portal/swdepot/displayProductInfo.do?productNumber=WASL" target="_blank" rel="noopener noreferrer">https://h20392.www2.hpe.com/portal/swdepot/displayProductInfo.do?productNumber=WASL<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>hpe.com/contact/feedback.</p> <p>© Copyright 2020 Hewlett Packard Enterprise Development LP. The information contained herein is subject to change without notice. The only warranties for Hewlett Packard Enterprise products and services are set forth in the express warranty statements accompanying such products and services. Nothing herein should be construed as constituting an additional warranty. Hewlett Packard Enterprise shall not be liable for technical or editorial errors or omissions contained herein.</p> <p>Red Hat ® Red Hat Enterprise Linux ® Red Hat OpenShift ® are registered trademarks of Red Hat, Inc. in the United States and other countries. Intel ® and Xeon ® are trademarks of Intel Corporation in the U.S. and other countries. VMware ® VMware vSphere ® VMware vCenter ® are registered trademarks of VMware, Inc. in the United States and/or other jurisdictions.</p> <p>OCP 3921, version 1.0 April 2020</p> <p>[]: media/image1.png{width=&quot;6.960265748031496in&quot; height=&quot;5.117600612423447in&quot;}
[1]: media/image2.png{width=&quot;7.033112423447069in&quot; height=&quot;4.2975710848643915in&quot;}
[Red Hat OpenShift worker nodes with RHEL]: #red-hat-openshift-worker-nodes-with-rhel
[2]: media/image3.png{width=&quot;7.138888888888889in&quot; height=&quot;5.3763221784776904in&quot;}
[Installer machine]: #installer-machine
[3]: media/image4.png{width=&quot;6.537878390201225in&quot; height=&quot;6.110840988626422in&quot;}
[Kubernetes manifests and ignition files]: #kubernetes-manifests-and-ignition-files
[4]: media/image5.png{width=&quot;4.166183289588801in&quot; height=&quot;4.594936570428697in&quot;}
[5]: media/image6.png{width=&quot;6.470149825021872in&quot; height=&quot;6.66690179352581in&quot;}
[6]: media/image7.jpg{width=&quot;6.5in&quot; height=&quot;6.593520341207349in&quot;}
[Table 5]: #services
[Network definitions]: #network-definitions
[7]: media/image8.png{width=&quot;6.5in&quot; height=&quot;2.321474190726159in&quot;}
[Server Profiles]: #server-profiles
[Virtual nodes configuration]: #virtual-nodes-configuration
[Deploying virtual master nodes]: #deploying-virtual-master-nodes
[8]: #validating-openshift-container-platform-deployment
[Deploying virtual worker nodes]: #deploying-virtual-worker-nodes
[Red Hat OpenShift Container Platform deployment]: #red-hat-openshift-container-platform-deployment
[Adding RHEL 7.6 worker nodes]: #adding-rhel-7.6-worker-nodes
[Creating virtual machines]: #creating-virtual-machines
[Adding Red Hat CoreOS worker nodes]: #adding-red-hat-coreos-worker-nodes
[Preparing worker nodes with RHEL]: #preparing-worker-nodes-with-rhel
[9]: media/image9.png{width=&quot;6.5in&quot; height=&quot;3.5625984251968505in&quot;}
[<em>https://github.com/hewlettpackard/hpe-solutions-openshift</em>]: https://github.com/hewlettpackard/hpe-solutions-openshift
[10]: media/image10.png{width=&quot;4.879464129483814in&quot; height=&quot;1.4647058180227472in&quot;}
[11]: media/image11.png{width=&quot;6.5in&quot; height=&quot;1.5243055555555556in&quot;}
[https://sysdig.com/]: https://sysdig.com//
[12]: media/image12.png{width=&quot;3.7916666666666665in&quot; height=&quot;3.9375in&quot;}
[13]: media/image13.png{width=&quot;3.8504746281714786in&quot; height=&quot;3.664705818022747in&quot;}
[14]: media/image14.png{width=&quot;5.656725721784777in&quot; height=&quot;2.2941174540682416in&quot;}
[Executing the playbooks]: #executing-the-playbooks-1
[Introduction]: #introduction-3
[C:\Users\vutkur\Desktop\OpenShift REST API's\cli output.JPG]: media/image15.jpg{width=&quot;4.6059317585301836in&quot; height=&quot;4.599266185476815in&quot;}
[C:\Users\vutkur\Desktop\OpenShift REST API's\installed operators.JPG]: media/image16.jpeg{width=&quot;6.5in&quot; height=&quot;2.841666666666667in&quot;}
[https://www.hpe.com/us/en/product-catalog/networking/networking-switches/pip.hpe-flexfabric-5945-switch-series.1009148840.html]: https://www.hpe.com/us/en/product-catalog/networking/networking-switches/pip.hpe-flexfabric-5940-switch-series.1009148840.html</p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/hpe-solutions-openshift/assets/js/app.adad66b3.js" defer></script><script src="/hpe-solutions-openshift/assets/js/2.ac0f675e.js" defer></script><script src="/hpe-solutions-openshift/assets/js/20.826352b1.js" defer></script>
  </body>
</html>
