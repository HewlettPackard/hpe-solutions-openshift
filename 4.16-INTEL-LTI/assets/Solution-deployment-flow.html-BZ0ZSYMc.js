import{_ as s,c as n,a,o as t}from"./app-DvQHlCAu.js";const i="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/Aspose.Words.d1fc7ddd-52aa-4c6c-b4c8-7a1fb8d3e951.009-DMTTqSuU.png",l="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/Aspose.Words.d1fc7ddd-52aa-4c6c-b4c8-7a1fb8d3e951.010-CLFQUtK8.png",o="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu1-3dAf4yS0.png",r="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu2-BRYknmuI.png",p="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu3-dSCQrl-Z.png",d="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu4-CuELQlyD.png",c="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu5-CNCTf-_W.png",h="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu6-CSM3HyuQ.png",u="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu7-DSIWk-Az.png",m="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/table-1-D5GbiuEF.png",v="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu8-p3urvUOH.png",b="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu9-Q6PzMuFF.png",g="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu10-DB5S1VlJ.png",y="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu11-BK3--8P2.png",f="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu12-DsSSaod4.png",k="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu13-oAnDEcEK.png",w="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu14-Cii3vjrN.png",x="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu15-Wgvd3Zew.png",_="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu16-CynaM0TV.png",I="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu17-C-FQpJjr.png",T="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu18-CZ6RUP8I.png",P="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu19-CCkSdGEN.png",O="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu20-Cf2o9zzr.png",R="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu21-BioVF2xO.png",S="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/figure-gpu22-CKqfep6J.png",C="/hpe-solutions-openshift/4.16-INTEL-LTI/assets/f81-DHkuqY8F.png",E={};function H(L,e){return t(),n("div",null,e[0]||(e[0]=[a('<h1 id="solution-deployment-workflow" tabindex="-1"><a class="header-anchor" href="#solution-deployment-workflow"><span><strong>Solution Deployment Workflow</strong></span></a></h1><p>The following figure shows the high-level workflow of the installation process:</p><p><img src="'+i+`" alt=""></p><p><strong>FIGURE 7.</strong> End to End Solution Deployment Workflow</p><p>The following steps provide an overview of each step that needs to be performed for deploying the HPE ProLiant NGS-optimized solution for Red Hat OpenShift Container Platform 4.16:</p><p>The way you interact with the installation program differs depending on your installation type.</p><ul><li>For clusters with installer-provisioned infrastructure, you delegate the infrastructure bootstrapping and provisioning to the installation program instead of doing it yourself. The installation program creates all of the networking, machines, and operating systems that are required to support the cluster.</li><li>If you provision and manage the infrastructure for your cluster, you must provide all of the cluster infrastructure and resources, including the bootstrap machine, networking, load balancing, storage, and individual cluster machines.</li></ul><ol><li><strong>Set up iPXE, TFTP, and DHCP for RHCOS</strong></li></ol><p>In this step, the iPXE server is leveraged to boot the machine. The iPXE and TFTP server is set up to boot RHCOS. The PXE boot process is the initial stage for deploying the solution and configuring DHCP is an integral part of this process. This configuration can be done using the sudo access.</p><p>For more information on configuring the iPXE set up, see the <a href="https://github.com/HewlettPackard/hpe-solutions-openshift/blob/master/DL-LTI-Openshift/playbooks/deploy_ipxe_ocp.yml" target="_blank" rel="noopener noreferrer">Deploy iPXE guide</a>.</p><ol start="2"><li><strong>Configure a load balancer for RHOCP 4 nodes</strong></li></ol><p>In the multi-node RHOCP cluster deployment, the load balancer is mandatory. For this solution, Hewlett Packard Enterprise has leveraged the required traffic for HAProxy load balancing. This configuration can be done using the sudo access. For commercial load balancer such as F5 Big-IP or any other RHOCP 4 supported load balancer, visit the manufacture website.</p><ol start="3"><li><strong>Configure BindDNS</strong></li></ol><p>In the User-Provisioned Infrastructure (UPI), DNS records are required for each machine. These records resolve the hostnames for all other machines in a RHOCP cluster. This component can also be configured using the sudo access for Linux-based DNS solution. It provides details on configuring the sudo to allow non-root users to execute root level commands.</p><ol start="4"><li><strong>Configure firewall ports</strong></li></ol><p>In the User-Provisioned Infrastructure (UPI), the network connectivity between machines allows cluster components to communicate within the RHOCP cluster. Hence, the required ports must be open between RHOCP cluster nodes. This component can also be configured using the sudo access for Linux-based firewall. For third-party firewall solutions, visit the manufacture website. It provides details on configuring the sudo to allow non-root users to execute root level commands.</p><p>For more information, see the <a href="https://docs.openshift.com/container-platform/4.16/installing/installing_bare_metal/installing-bare-metal-network-customizations.html" target="_blank" rel="noopener noreferrer">Installing a user-provisioned bare metal cluster with network customizations</a> and <a href="https://docs.openshift.com/container-platform/4.16/installing/installing_bare_metal/installing-bare-metal-network-customizations.html#installation-network-user-infra_installing-bare-metal-network-customizations" target="_blank" rel="noopener noreferrer">Networking requirements for user-provisioned infrastructure</a> sections in the <em>OpenShift Container Platform 4.16 documentation</em>.</p><ol start="5"><li><strong>Start RHOCP 4 User-Provisioned Infrastructure setup</strong></li></ol><p>The User-Provisioned Infrastructure (UPI) begins with installing a bastion host. This setup uses RHEL 9.4 virtual machine as a bastion host. This bastion host is used for deployment and management of the RHOCP 4 clusters. The setup and configuration of this step can be completed using the sudo user access.</p><p>For more information, see the <a href="https://docs.openshift.com/container-platform/4.16/installing/installing_bare_metal/installing-bare-metal.html#ssh-agent-using_installing-bare-metal" target="_blank" rel="noopener noreferrer">Generating a key pair for cluster node SSH access</a> section in the <em>OpenShift Container Platform 4.16 documentation</em>.</p><ol start="6"><li><strong>Download RHOCP 4 software version and images</strong></li></ol><p>To download the RHOCP 4 image, see the <a href="https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.16/latest/" target="_blank" rel="noopener noreferrer">RHCOS image mirror</a> page. Check the access token for your cluster and install it on the bastion host. The bastion host is used for deploying and managing the RHOCP 4 clusters. The setup and configuration of this step can be completed using the sudo user access.</p><p>For more information, see the <a href="https://docs.openshift.com/container-platform/4.16/installing/installing_bare_metal/installing-bare-metal.html#installation-obtaining-installer_installing-bare-metal" target="_blank" rel="noopener noreferrer">Obtaining the installation program</a> section in the <em>OpenShift Container Platform 4.16 documentation</em>.</p><ol start="7"><li><strong>Create ignition config files</strong></li></ol><p>This step begins with the creation of the install-config.yaml in a new folder. Use the Red Hat OpenShift installer tool to convert the YAML file to the ignition config file, which is required to install the RHOCP 4. During this process, system modification is not done on the bastion host or the provisioning server. This setup can be completed using the sudo access.</p><p>For more information, see the <a href="https://docs.openshift.com/container-platform/4.16/installing/installing_bare_metal/installing-bare-metal.html#installation-initializing-manual_installing-bare-metal" target="_blank" rel="noopener noreferrer">Manually creating the installation configuration file</a> section in the <em>OpenShift Container Platform 4.16 documentation</em>.</p><ol start="8"><li><strong>Upload ignition config files to the web</strong></li></ol><p>In this step, the ignition config files are uploaded to an internal website that allows anonymous access to the iPXE boot process. Update the iPXE default file to point to the website location of the ignition file. The action required in this step can be done using the sudo user.</p><p>For more information, see the <a href="https://docs.openshift.com/container-platform/4.16/installing/installing_bare_metal/installing-bare-metal.html#installation-user-infra-machines-pxe_installing-bare-metal" target="_blank" rel="noopener noreferrer">Installing RHCOS by using PXE or iPXE booting</a> section in the <em>OpenShift Container Platform 4.16 documentation</em>.</p><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>KVM is an open-source virtualization technology that converts your Linux machine into a type-1 bare-metal hypervisor and allows you to run multiple Virtual Machines (VMs) or guest VMs on Red Hat Linux.</p><p>For more information, see the <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_virtualization/getting-started-with-virtualization-in-rhel-9_configuring-and-managing-virtualization" target="_blank" rel="noopener noreferrer">Getting started with virtualization</a> section in the <em>Red Hat Enterprise Linux 8 documentation</em>.</p></div><ol start="9"><li><strong>Deploy bootstrap node</strong></li></ol><p>The bootstrap node is a temporary node that is used to bring up the RHOCP cluster. After the cluster is up, this machine can be decommissioned, and the hardware can be reused. The iPXE boot process must use bootstrapping information as a part of the iPXE boot parameter to install the RHCOS on this node.</p><ol start="10"><li><strong>Deploy master node</strong></li></ol><p>The master node uses the iPXE image for RHCOS after the bootstrap node. The iPXE boot process must use the master.ign information as a part of the iPXE boot parameter to install the RHCOS on this node. The root user is not active by default in RHCOS. Since the root login is not available, log in as the core user.</p><ol start="11"><li><strong>Create the cluster</strong></li></ol><p>The four nodes, one bootstrap and three master nodes boot up and are available at the login prompt for RHCOS. To complete the bootstrap process, log in as a sudo user on the bastion host or provision server and use the Red Hat OpenShift installer tool.</p><p>For more information, see the <a href="https://docs.openshift.com/container-platform/4.16/installing/installing_bare_metal/installing-bare-metal.html#installation-installing-bare-metal_installing-bare-metal" target="_blank" rel="noopener noreferrer">Waiting for the bootstrap process to complete</a> section in the <em>OpenShift Container Platform 4.16 documentation</em>.</p><ol start="12"><li><strong>Log in to the cluster</strong></li></ol><p>After the bootstrap process has completed successfully, login to the cluster. The kubeconfig file is present in the auth directory where the ignition files are created on the bastion host. Export the cluster kubeconfig file and log in to your cluster as a default system user. The kubeconfig file contains information about the cluster that is used by the CLI to connect a client to the correct cluster and API server. This file is specific to a cluster and is created during the RHOCP installation. After logging in, approve the pending Certificate Signing Requests (CSRs) for the nodes.</p><p>For more information, see the <a href="https://docs.openshift.com/container-platform/4.16/installing/installing_bare_metal/installing-bare-metal.html#installation-approve-csrs_installing-bare-metal" target="_blank" rel="noopener noreferrer">Approving the certificate signing requests for your machines</a> section in the <em>OpenShift Container Platform 4.16 documentation</em>.</p><ol start="13"><li><strong>Configure operators</strong></li></ol><p>After the control plane initializes, you must immediately configure operators that are not available. It ensures their availability (for example, image-registry).</p><p>For more information, see the <a href="https://docs.openshift.com/container-platform/4.16/installing/installing_bare_metal/installing-bare-metal.html#installation-registry-storage-config_installing-bare-metal" target="_blank" rel="noopener noreferrer">Image registry storage configuration</a> section in the <em>OpenShift Container Platform 4.16 documentation</em>. To complete this step, you can also log in as a sudo user on the bastion host or provision server.</p><ol start="14"><li><strong>Add Worker nodes</strong></li></ol><p>In the RHOCP, you can add RHEL worker nodes to a User-Provisioned Infrastructure cluster or an installation-provisioned infrastructure cluster on the x86_64 architecture. For more information, see the <a href="https://docs.openshift.com/container-platform/4.16/machine_management/adding-rhel-compute.html" target="_blank" rel="noopener noreferrer">Adding RHEL compute machines to an OpenShift Container Platform cluster</a> section in the <em>OpenShift Container Platform 4.16 documentation</em>.</p><h2 id="preparing-the-execution-environment-for-rhocp-worker-node" tabindex="-1"><a class="header-anchor" href="#preparing-the-execution-environment-for-rhocp-worker-node"><span>Preparing the execution environment for RHOCP worker node</span></a></h2><p><strong>Prerequisites:</strong></p><ul><li>RedHat Enterprise Linux 9.4 must be <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/performing_a_standard_rhel_9_installation/index" target="_blank" rel="noopener noreferrer">installed and registered</a> on your host machine</li><li>Configure BOND</li></ul><p><strong>Setting up RHEL 9.4 installer machine</strong></p><p>This section assumes the following considerations for our deployment environment:</p><ul><li>A server running Red Hat Enterprise Linux (RHEL) 9.4 exists within the deployment environment and is accessible to the installation user to be used as an installer machine. This server must have internet connectivity.</li><li>A virtual machine is used to act as an installer machine and the same host is utilized as an Ansible Engine host. We are using one of the worker machines as an installer machine to execute Ansible Playbook.</li></ul><p><strong>Prerequisites to execute ansible playbook:</strong></p><p>RHEL 9.4 installer machine must have the following configurations:</p><ol><li>The installer machine must have at least 500 GB disk space (especially in the &quot;/&quot; partition), 4 CPU cores and 16 GB RAM.</li><li>RHEL 9.4 installer machine must be subscribed with valid Red Hat credentials. To register the installer machine for the Red Hat subscription, run the following command:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ <span class="token function">sudo</span> subscription-manager register <span class="token parameter variable">--username</span><span class="token operator">=</span><span class="token operator">&lt;</span>username<span class="token operator">&gt;</span> <span class="token parameter variable">--password</span><span class="token operator">=</span><span class="token operator">&lt;</span>password<span class="token operator">&gt;</span> --auto-attach </span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="3"><li>Sync time with NTP server.</li><li>SSH key pair must be available on the installer machine. If the SSH key is not available, generate a new SSH key pair with the following command:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ssh-keygen </span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><strong>To set up the installer machine:</strong></p><ol start="5"><li>Create and activate a Python3 virtual environment for deploying this solution with the following commands:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ python3 <span class="token parameter variable">-m</span> venv <span class="token operator">&lt;</span>virtual_environment_name<span class="token operator">&gt;</span> </span>
<span class="line"></span>
<span class="line">$ <span class="token builtin class-name">source</span> <span class="token operator">&lt;</span>virtual_environment_name<span class="token operator">&gt;</span>/bin/activate </span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol start="6"><li>Download the OpenShift repositories using the following commands in the Ansible Engine:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ <span class="token function">mkdir</span> /opt </span>
<span class="line"></span>
<span class="line">$ <span class="token builtin class-name">cd</span> /opt </span>
<span class="line"></span>
<span class="line">$ yum <span class="token function">install</span> <span class="token parameter variable">-y</span> <span class="token function">git</span> </span>
<span class="line"></span>
<span class="line">$ <span class="token function">git</span> clone <span class="token operator">&lt;</span>https://github.com/HewlettPackard/hpe-solutions-openshift.git<span class="token operator">&gt;</span> </span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol start="7"><li>Setup the installer machine to configure the nginx, development tools, and other python packages required for LTI installation. Navigate to the $BASE_DIR directory and run the following command:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ <span class="token builtin class-name">cd</span> <span class="token variable">$BASE_DIR</span></span>
<span class="line">$ <span class="token function">sh</span> setup.sh </span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><div class="hint-container tip"><p class="hint-container-title">Note</p><p>$BASE_DIR refers to <strong>/opt/hpe-solutions-openshift/DL-LTI-Openshift/</strong></p></div><ol start="8"><li><p>As part of setup.sh script it will create nginx service, so user must download and copy Rhel 9.4 DVD ISO to <strong>/usr/share/nginx/html/</strong></p></li><li><p>Minimum Storage requirement for management servers</p></li></ol><table><thead><tr><th><strong>Management Servers</strong></th><th><strong>Host OS disk</strong></th><th><strong>Storage Pool disk</strong></th></tr></thead><tbody><tr><td>Server 1</td><td>2 x 1.6 TB</td><td>2 x 1.6 TB</td></tr><tr><td>Server 2</td><td>2 x 1.6 TB</td><td>2 x 1.6 TB</td></tr><tr><td>Server 3</td><td>2 x 1.6 TB</td><td>2 x 1.6 TB</td></tr></tbody></table><p>Host OS disk – raid1 for redundancy</p><ol start="10"><li><strong>Creating and deleting logical drives</strong></li></ol><p>Create and delete logical drives on the head nodes following below steps.</p><p><strong>Input File Update:-</strong></p><ol><li><p>User has to update the input.yaml file in $BASE_DIR/create_delete_logicaldrives directory to execute the logical drive script.</p></li><li><p>User needs to update all the details in the input.yaml file which include:-</p></li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line"></span>
<span class="line">		ILOServers:</span>
<span class="line"></span>
<span class="line">			- ILOIP: <span class="token number">172.28</span>.*.*</span>
<span class="line"></span>
<span class="line">				ILOuser: admin</span>
<span class="line"></span>
<span class="line">				ILOPassword: Password</span>
<span class="line"></span>
<span class="line">				controller: <span class="token number">12</span>  </span>
<span class="line"></span>
<span class="line">				RAID: Raid1</span>
<span class="line"></span>
<span class="line">				PhysicalDrives: 1I:1:1,1I:1:2  </span>
<span class="line"></span>
<span class="line">			- ILOIP: <span class="token number">172.28</span>.*.*</span>
<span class="line"></span>
<span class="line">				ILOuser: admin</span>
<span class="line"></span>
<span class="line">				ILOPassword: Password</span>
<span class="line"></span>
<span class="line">				controller: <span class="token number">1</span></span>
<span class="line"></span>
<span class="line">				RAID: Raid1</span>
<span class="line"></span>
<span class="line">				PhysicalDrives: 1I:3:1,1I:3:2</span>
<span class="line"></span>
<span class="line">			- ILOIP: <span class="token number">172.28</span>.*.*</span>
<span class="line"></span>
<span class="line">				ILOuser: admin</span>
<span class="line"></span>
<span class="line">				ILOPassword: Password</span>
<span class="line"></span>
<span class="line">				controller: <span class="token number">11</span></span>
<span class="line"></span>
<span class="line">				RAID: Raid1</span>
<span class="line"></span>
<span class="line">				PhysicalDrives: 1I:3:1,1I:3:2   </span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>Note:-</strong></p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line"><span class="token number">1</span>. To <span class="token function">find</span> controller <span class="token function">id</span> login to the respective ILO -<span class="token operator">&gt;</span> System Information -<span class="token operator">&gt;</span> Storage tab where inside Location <span class="token function">find</span> the **slot number** as the controller id. </span>
<span class="line"></span>
<span class="line">			 <span class="token comment"># Example - Slot = 12 </span></span>
<span class="line"></span>
<span class="line"><span class="token number">2</span>. To <span class="token function">find</span> the PhysicalDrives login to the respective ILO -<span class="token operator">&gt;</span> System Information -<span class="token operator">&gt;</span> Storage tab inside Unconfigured Drives where under Location you can deduce PhysicalDrives based on these information:</span>
<span class="line"></span>
<span class="line">		 <span class="token comment"># Slot: 12:Port=1I:Box=1:Bay=1</span></span>
<span class="line"></span>
<span class="line">		<span class="token comment"># Example - 1I:1:1 (&#39;Port:Box:Bay&#39;)</span></span>
<span class="line"></span>
<span class="line">		<span class="token comment"># Slot: 12:Port=1I:Box=1:Bay=2</span></span>
<span class="line"></span>
<span class="line">		<span class="token comment"># Example - 1I:1:2 (&#39;Port:Box:Bay&#39;) </span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>Playbook Execution:-</strong></p><p>To delete all the existing logical drives in the server in case if any and to create new logical drives named &#39;RHEL Boot Volume&#39; in respective ILO servers run the site.yml playbook inside create_delete_logicaldrives directory with the below mentioned command</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">	$ ansible-playbook site.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>We can provide the input variables in any one of the below methods</p><p>Method 1. <strong>Input.py : Automation way of taking input</strong></p><p>Through the input.py, go to the directory /opt/hpe-solutions-openshift/DL-LTI-Openshift/ and run the below command.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">   python input.py</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>Here it will prompt for values which are not obtained from SCID json files.</p><p>A sample input collection through input.py is as follows.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">	Enter server serial number <span class="token keyword">for</span> the first <span class="token function">head</span> <span class="token function">node</span> server <span class="token punctuation">(</span> Example: 2M2210020X <span class="token punctuation">)</span></span>
<span class="line">	2M205107TH</span>
<span class="line">	Enter ILO address <span class="token keyword">for</span> the first <span class="token function">head</span> <span class="token function">node</span> server <span class="token punctuation">(</span> Example: <span class="token number">192.28</span>.201.5 <span class="token punctuation">)</span></span>
<span class="line">	<span class="token number">172.28</span>.201.13</span>
<span class="line">	Enter ILO username <span class="token keyword">for</span> the first <span class="token function">head</span> <span class="token function">node</span> server <span class="token punctuation">(</span> Example: admin <span class="token punctuation">)</span></span>
<span class="line">	admin</span>
<span class="line">	Enter ILO password <span class="token keyword">for</span> the first <span class="token function">head</span> <span class="token function">node</span> server <span class="token punctuation">(</span> Example: Password <span class="token punctuation">)</span></span>
<span class="line">	Password</span>
<span class="line">	Enter Host FQDN <span class="token keyword">for</span> the first <span class="token function">head</span> <span class="token function">node</span> server <span class="token punctuation">(</span> Example: kvm1.xyz.local <span class="token punctuation">)</span></span>
<span class="line">	headnode1.isv.local</span>
<span class="line">	etc <span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>.&#39;</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>After execution of input.py, it will generate input.yaml and hosts file in the same location.</p><p>Method 2. <strong>Input.yaml: Manually editing input file</strong></p><p>Go to the directory $BASE_DIR(<strong>/opt/hpe-solutions-openshift/DL-LTI-Openshift/</strong>), here we will have input.yaml and hosts files.</p><ol><li>A preconfigured Ansible vault file (input.yaml) is provided as a part of this solution, which consists of sensitive information to support the host and virtual machine deployment.</li></ol><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">cd $BASE_DIR</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>Run the following commands on the installer VM to edit the vault to match the installation environment.</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">ansible-vault edit input.yaml</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>The default password for the Ansible vault file is <strong>changeme</strong></p><p>Sample input_sample.yml can be found in the $BASE_DIR along with description of each input variable.</p></div><p>A sample input.yaml file is as follows with a few filled parameters.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line"> - Server_serial_number: 2M20510XXX</span>
<span class="line">   ILO_Address: <span class="token number">172.28</span>.*.*</span>
<span class="line">   ILO_Username: admin</span>
<span class="line">   ILO_Password: *****</span>
<span class="line">   Hostname: headnode3.XX.XX                <span class="token comment">#ex. headnode3.isv.local</span></span>
<span class="line">   Host_Username: root</span>
<span class="line">   Host_Password: ******</span>
<span class="line">   HWADDR1: XX:XX:XX:XX:XX:XX             <span class="token comment">#mac address for server physical interface1 </span></span>
<span class="line">   HWADDR2: XX:XX:XX:XX:XX:XX             <span class="token comment">#mac address for server physical interface2</span></span>
<span class="line">   Host_OS_disk: sda</span>
<span class="line">   Host_VlanID: <span class="token number">230</span></span>
<span class="line">   Host_IP: <span class="token number">172.28</span>.*.*</span>
<span class="line">   Host_Netmask: <span class="token number">255</span>.*.*.*</span>
<span class="line">   Host_Prefix: XX                          <span class="token comment">#ex. 8,24,32</span></span>
<span class="line">   Host_Gateway: <span class="token number">172.28</span>.*.*</span>
<span class="line">   Host_DNS: <span class="token number">172.28</span>.*.*</span>
<span class="line">   Host_Domain: XX.XX                       <span class="token comment">#ex. isv.local</span></span>
<span class="line">   corporate_proxy: <span class="token number">172.28</span>.*.*              <span class="token comment">#provide corporate proxy, ex. proxy.houston.hpecorp.net</span></span>
<span class="line">   corporate_proxy_port: XX               <span class="token comment">#corporate proxy port no, ex. 8080</span></span>
<span class="line"></span>
<span class="line">config:</span>
<span class="line">   HTTP_server_base_url: http://172.28.*.*/  <span class="token comment">#Installer IP address</span></span>
<span class="line">   HTTP_file_path: /usr/share/nginx/html/    </span>
<span class="line">   OS_type: rhel9</span>
<span class="line">   OS_image_name: rhel-9.4-x86_64-dvd.iso    <span class="token comment"># ISO image should be present in /usr/share/nginx/html/</span></span>
<span class="line">   base_kickstart_filepath: /opt/hpe-solutions-openshift/DL-LTI-Openshift/playbooks/roles/rhel9_os_deployment/tasks/ks_rhel9.cfg</span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>A sample <strong>hosts</strong> files is as follows</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">		   <span class="token string">&#39;[kvm_nodes]</span>
<span class="line">			172.28.*.*</span>
<span class="line">			172.28.*.*</span>
<span class="line">			172.28.*.*</span>
<span class="line"></span>
<span class="line">			[ansible_host]</span>
<span class="line">			172.28.*.*</span>
<span class="line"></span>
<span class="line">			[rhel9_installerVM]</span>
<span class="line">			172.28.*.*</span>
<span class="line"></span>
<span class="line">			[binddns_masters]</span>
<span class="line">			172.28.*.*</span>
<span class="line"></span>
<span class="line">			[binddns_slaves]</span>
<span class="line">			172.28.*.*</span>
<span class="line">			172.28.*.*</span>
<span class="line"></span>
<span class="line">			[masters_info]</span>
<span class="line">			master1 ip=172.28.*.* hostname=headnode1</span>
<span class="line"></span>
<span class="line">			[slaves_info]</span>
<span class="line">			slave1 ip=172.28.*.* hostname=headnode2</span>
<span class="line">			slave2 ip=172.28.*.* hostname=headnode3&#39;</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><hr><h2 id="deploying-rhocp-cluster-using-ansible-playbooks" tabindex="-1"><a class="header-anchor" href="#deploying-rhocp-cluster-using-ansible-playbooks"><span><strong>Deploying RHOCP cluster using Ansible playbooks</strong></span></a></h2><p>The Lite Touch Installation (LTI) package includes Ansible playbooks with scripts to deploy RHOCP cluster. You can use one of the following two methods to deploy RHOCP cluster:</p><ul><li><strong>Run a consolidated playbook:</strong> This method includes a single playbook for deploying the entire solution. This site.yml playbook contains a script that performs all the tasks starting from the OS deployment until the RHOCP cluster is successfully installed and running. To run LTI using a consolidated playbook:</li></ul><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts site.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>The default password for the Ansible vault file is <strong>changeme</strong></p></div><ul><li><strong>Run individual playbooks:</strong> This method includes multiple playbooks with scripts that enable you to deploy specific parts of the solution depending on your requirements. The playbooks in this method must be executed in a specific sequence to deploy the solution. The following table includes the purpose of each playbook required for the deployment:</li></ul><p><strong>TABLE 8.</strong> RHOCP cluster deployment using Ansible playbooks</p><table><thead><tr><th style="text-align:left;">Playbook</th><th style="text-align:left;">Description</th></tr></thead><tbody><tr><td style="text-align:left;">rhel9_os_deployment.yml</td><td style="text-align:left;">This playbook contains the script to deploy RHEL 9.4 OS on BareMetal servers.</td></tr><tr><td style="text-align:left;">copy_ssh_headnode.yml</td><td style="text-align:left;">This playbook contains the script to copy the SSH public key from the installer machine to the head nodes.</td></tr><tr><td style="text-align:left;">prepare_rhel_hosts.yml</td><td style="text-align:left;">This playbook contains the script to prepare nodes for the RHOCP head nodes.</td></tr><tr><td style="text-align:left;">ntp.yml</td><td style="text-align:left;">This playbook contains the script to create NTP setup to enable time synchronization on head nodes.</td></tr><tr><td style="text-align:left;">binddns.yml</td><td style="text-align:left;">This playbook contains the script to deploy Bind DNS on three head nodes and acts as active-passive cluster configuration.</td></tr><tr><td style="text-align:left;">haproxy.yml</td><td style="text-align:left;">This playbook contains the script to deploy HAProxy on the head nodes and acts as active-active cluster configuration.</td></tr><tr><td style="text-align:left;">squid_proxy.yml</td><td style="text-align:left;">This playbook contains the script to deploy the Squid proxy on the head nodes to get web access.</td></tr><tr><td style="text-align:left;">storage_pool.yml</td><td style="text-align:left;">This playbook contains the script to create the storage pools on the head nodes.</td></tr><tr><td style="text-align:left;">rhel9_installerVM.yml</td><td style="text-align:left;">This playbook contains the script to create a RHEL 9 installer machine, which will also be used as an installer at a later stage.</td></tr><tr><td style="text-align:left;">copy_ssh_installerVM.yml</td><td style="text-align:left;">This playbook contains the script to copy the SSH public key to the RHEL 9 installer machine.</td></tr><tr><td style="text-align:left;">prepare_rhel9_installer.yml</td><td style="text-align:left;">This playbook contains the script to prepare the RHEL 9 installer.</td></tr><tr><td style="text-align:left;">copy_scripts.yml</td><td style="text-align:left;">This playbooks contains the script to copy ansible code to rhel9 installer and headnodes.</td></tr><tr><td style="text-align:left;">download_ocp_packages.yml</td><td style="text-align:left;">This playbook contains the script to download the required RHOCP packages.</td></tr><tr><td style="text-align:left;">generate_manifest.yml</td><td style="text-align:left;">This playbook contains the script to generate the manifest files.</td></tr><tr><td style="text-align:left;">copy_ocp_tool.yml</td><td style="text-align:left;">This playbook contains the script to copy the RHOCP tools from the current installer to the head nodes and RHEL 9 installer.</td></tr><tr><td style="text-align:left;">deploy_ipxe_ocp.yml</td><td style="text-align:left;">This playbook contains the script to deploy the iPXE server on the head nodes.</td></tr><tr><td style="text-align:left;">ocp_vm.yml</td><td style="text-align:left;">This playbook contains the script to create bootstrap and master nodes.</td></tr></tbody></table><p>To run individual playbooks:</p><ul><li>Do one of the following:</li></ul><ol><li>Edit site.yml file and add a comment for all the playbooks you do not want to execute.</li></ol><p>For example, add the following comments in the site.yml file to deploy RHEL 9.4 OS:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">- import_playbook: playbooks/rhel9_os_deployment.yml</span>
<span class="line">- import_playbook: playbooks/copy_ssh_headnode.yml</span>
<span class="line">- import_playbook: playbooks/prepare_rhel_hosts.yml</span>
<span class="line">- import_playbook: playbooks/ntp.yml</span>
<span class="line">- import_playbook: playbooks/binddns.yml</span>
<span class="line">- import_playbook: playbooks/haproxy.yml</span>
<span class="line">- import_playbook: playbooks/squid_proxy.yml</span>
<span class="line">- import_playbook: playbooks/storage_pool.yml</span>
<span class="line">- import_playbook: playbooks/rhel9_installerVM.yml</span>
<span class="line">- import_playbook: playbooks/copy_ssh_installerVM.yml</span>
<span class="line">- import_playbook: playbooks/prepare_rhel9_installer.yml</span>
<span class="line">- import_playbook: playbooks/download_ocp_packages.yml</span>
<span class="line">- import_playbook: playbooks/generate_manifest.yml</span>
<span class="line">- import_playbook: playbooks/copy_ocp_tool.yml</span>
<span class="line">- import_playbook: playbooks/deploy_ipxe_ocp.yml</span>
<span class="line">- import_playbook: playbooks/ocp_vm.yml</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol start="2"><li>Run the individual YAML files using the following command:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/<span class="token operator">&lt;</span>yaml_filename<span class="token operator">&gt;</span>.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>For example, run the following YAML file to deploy RHEL 9.4 OS:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/rhel9_os_deployment.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>For more information on executing individual playbooks, see the consecutive sections.</p><h4 id="deploying-rhel-9-os-on-baremetal-servers" tabindex="-1"><a class="header-anchor" href="#deploying-rhel-9-os-on-baremetal-servers"><span>Deploying RHEL 9 OS on baremetal servers</span></a></h4><p>This section describes how to run the playbook that contains the script for deploying RHEL 9.4 OS on BareMetal servers. To deploy RHEL 9.4 OS on the head nodes:</p><ol><li>Navigate to the $BASE_DIR(<strong>/opt/hpe-solutions-openshift/DL-LTI-Openshift/</strong>) directory on the installer.</li><li>Run the following playbook:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/rhel9_os_deployment.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="copying-ssh-key-to-head-nodes" tabindex="-1"><a class="header-anchor" href="#copying-ssh-key-to-head-nodes"><span>Copying SSH key to head nodes</span></a></h4><p>Once the OS is installed on the head nodes, copy the ssh key from the installer machine to the head nodes. It uses playbook that contains the script to copy the SSH public key from the installer machine to the head nodes.</p><p>To copy the SSH key to the head node run the following playbook:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/copy_ssh_headnode.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="setting-up-head-nodes" tabindex="-1"><a class="header-anchor" href="#setting-up-head-nodes"><span>Setting up head nodes</span></a></h4><p>This section describes how to run the playbook that contains the script to prepare nodes for the RHOCP head nodes.</p><p>To register the head nodes to Red Hat subscription and download and install KVM Virtualization packages run the following playbook:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/prepare_rhel_hosts.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="setting-up-ntp-server-on-head-nodes" tabindex="-1"><a class="header-anchor" href="#setting-up-ntp-server-on-head-nodes"><span><strong>Setting up NTP server on head nodes</strong></span></a></h4><p>This section describes how to run the playbook that contains the script to set up NTP server and enable time synchronization on all head nodes.</p><p>To set up NTP server on head nodes run the following playbook:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/ntp.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="deploying-bind-dns-on-head-nodes" tabindex="-1"><a class="header-anchor" href="#deploying-bind-dns-on-head-nodes"><span><strong>Deploying Bind DNS on head nodes</strong></span></a></h4><p>This section describes how to deploy Bind DNS service on all three head nodes for active-passive cluster configuration.</p><p>To deploy Bind DNS service on head nodes run the following playbook:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/binddns.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="deploying-haproxy-on-head-nodes" tabindex="-1"><a class="header-anchor" href="#deploying-haproxy-on-head-nodes"><span><strong>Deploying HAProxy on head nodes</strong></span></a></h4><p>The RHOCP 4.16 uses an external load balancer to communicate from outside the cluster with services running inside the cluster. This section describes how to deploy HAProxy on all three head nodes for active-active cluster configuration.</p><p>To deploy HAProxy server configuration on head nodes run the following playbook:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/haproxy.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="deploying-squid-proxy-on-head-nodes" tabindex="-1"><a class="header-anchor" href="#deploying-squid-proxy-on-head-nodes"><span><strong>Deploying Squid proxy on head nodes</strong></span></a></h4><p>Squid is a proxy server that caches content to reduce bandwidth and load web pages more quickly. This section describes how to set up Squid as a proxy for HTTP, HTTPS, and FTP protocol, as well as authentication and restricting access. It uses a playbook that contains the script to deploy the Squid proxy on the head nodes to get web access.</p><p>To deploy Squid proxy server on head nodes run the following playbook:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/squid_proxy.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="creating-storage-pools-on-head-nodes" tabindex="-1"><a class="header-anchor" href="#creating-storage-pools-on-head-nodes"><span><strong>Creating storage pools on head nodes</strong></span></a></h4><p>This section describes how to use the storage_pool.yml playbook that contains the script to create the storage pools on the head nodes.</p><p>To create the storage pools run the following playbook:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/storage_pool.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="creating-rhel-9-installer-machine" tabindex="-1"><a class="header-anchor" href="#creating-rhel-9-installer-machine"><span><strong>Creating RHEL 9 installer machine</strong></span></a></h4><p>This section describes how to create a RHEL 9 installer machine using the rhel9_installerVM.yml playbook. This installer machine is also used as an installer for deploying the RHOCP cluster and adding RHEL 9.4 worker nodes.</p><p>To create a RHEL 9 installer machine run the following playbook:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/rhel9_installerVM.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="copying-ssh-key-to-rhel-9-installer-machine" tabindex="-1"><a class="header-anchor" href="#copying-ssh-key-to-rhel-9-installer-machine"><span>Copying SSH key to RHEL 9 installer machine</span></a></h4><p>This section describes how to copy the SSH public key to the RHEL 9 installer machine using the copy_ssh_installerVM.yml playbook.</p><p>To copy the SSH public key to the RHEL 9 installer machine run the following playbook:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/copy_ssh_installerVM.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="setting-up-rhel-9-installer" tabindex="-1"><a class="header-anchor" href="#setting-up-rhel-9-installer"><span>Setting up RHEL 9 installer</span></a></h4><p>This section describes how to set up the RHEL 9 installer using the prepare_rhel9_installer.yml playbook.</p><p>To set up the RHEL 9 installer run the following playbook:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/prepare_rhel9_installer.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="downloading-rhocp-packages" tabindex="-1"><a class="header-anchor" href="#downloading-rhocp-packages"><span><strong>Downloading RHOCP packages</strong></span></a></h4><p>This section provides details about downloading the required RHOCP 4.16 packages using a playbook.</p><p>To download RHOCP 4.16 packages:</p><p>Download the required packages on the installer VM with the following playbook:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/download_ocp_packages.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="generating-kubernetes-manifest-files" tabindex="-1"><a class="header-anchor" href="#generating-kubernetes-manifest-files"><span><strong>Generating Kubernetes manifest files</strong></span></a></h4><p>The manifests and ignition files define the master node and worker node configuration and are key components of the RHOCP 4.16 installation. This section describes how to use the generate_manifest.yml playbook that contains the script to generate the manifest files.</p><p>To generate Kubernetes manifest files run the following playbook:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/generate_manifest.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="copying-rhocp-tools" tabindex="-1"><a class="header-anchor" href="#copying-rhocp-tools"><span>Copying RHOCP tools</span></a></h4><p>This section describes how to copy the RHOCP tools from the present installer to head nodes and RHEL 9 installer using the copy_ocp_tool.yml playbook.</p><p>To copy the RHOCP tools to the head nodes and RHEL 9 installer run the following playbook:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/copy_ocp_tool.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="deploying-ipxe-server-on-head-nodes" tabindex="-1"><a class="header-anchor" href="#deploying-ipxe-server-on-head-nodes"><span>Deploying iPXE server on head nodes</span></a></h4><p>This section describes how to deploy the iPXE server on the head nodes using the deploy_ipxe_ocp.yml playbook.</p><p>To deploy the iPXE server run the following playbook:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/deploy_ipxe_ocp.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="creating-bootstrap-and-master-nodes" tabindex="-1"><a class="header-anchor" href="#creating-bootstrap-and-master-nodes"><span>Creating bootstrap and master nodes</span></a></h4><p>This section describes how to create bootstrap and master nodes using the scripts in the ocp_vm.yml playbook.</p><p>To create bootstrap and master VMs on Kernel-based Virtual Machine (KVM):</p><p>Run the following playbook:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/ocp_vm.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="deploying-rhocp-cluster" tabindex="-1"><a class="header-anchor" href="#deploying-rhocp-cluster"><span><strong>Deploying RHOCP cluster</strong></span></a></h4><p>Once the playbooks are executed successfully and the Bootstrap and master nodes are deployed with the RHCOS, deploy the RHOCP cluster.</p><p>To deploy the RHOCP cluster:</p><ol><li>Login to the installer VM.</li></ol><p>This installer VM was created as a KVM VM on one of the head nodes using the rhel9_installerVM.yml playbook. For more information, see the <a href="#creating-rhel-9-installer-machine">Creating RHEL 9 installer machine</a> section.</p><ol start="2"><li>Add the kubeconfig path in the environment variables using the following command:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ <span class="token builtin class-name">export</span> <span class="token assign-left variable">KUBECONFIG</span><span class="token operator">=</span>/opt/hpe-solutions-openshift/DL-LTI-Openshift/playbooks/roles/generate_ignition_files/ignitions/auth/kubeconfig</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="3"><li>Run the following command:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ openshift-install wait-for bootstrap-complete <span class="token parameter variable">--dir</span><span class="token operator">=</span>/opt/hpe-solutions-openshift/DL-LTI-Openshift/playbooks/roles/generate_ignition_files/ignitions --log-level debug</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="4"><li>Complete the RHOCP 4.16 cluster installation with the following command:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line"><span class="token variable">$openshift</span>-install wait-for install-complete <span class="token parameter variable">--dir</span><span class="token operator">=</span>/opt/hpe-solutions-openshift/DL-LTI-Openshift/playbooks/roles/generate_ignition_files/ignitions --log-level<span class="token operator">=</span>debug</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>The following output is displayed:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">DEBUG OpenShift Installer v4.16</span>
<span class="line"></span>
<span class="line">DEBUG Built from commit 6ed04f65b0f6a1e11f10afe658465ba8195ac459 </span>
<span class="line"></span>
<span class="line">INFO Waiting up to 30m0s <span class="token keyword">for</span> the cluster at https://api.rrocp.pxelocal.local:6443 to initialize<span class="token punctuation">..</span>. </span>
<span class="line"></span>
<span class="line">DEBUG Still waiting <span class="token keyword">for</span> the cluster to initialize: Working towards <span class="token number">4.16</span>: <span class="token number">99</span>% complete </span>
<span class="line"></span>
<span class="line">DEBUG Still waiting <span class="token keyword">for</span> the cluster to initialize: Working towards <span class="token number">4.16</span>: <span class="token number">99</span>% complete, waiting on authentication, console,image-registry </span>
<span class="line"></span>
<span class="line">DEBUG Still waiting <span class="token keyword">for</span> the cluster to initialize: Working towards <span class="token number">4.16</span>: <span class="token number">99</span>% complete </span>
<span class="line"></span>
<span class="line">DEBUG Still waiting <span class="token keyword">for</span> the cluster to initialize: Working towards <span class="token number">4.16</span>: <span class="token number">100</span>% complete, waiting on image-registry </span>
<span class="line"></span>
<span class="line">DEBUG Still waiting <span class="token keyword">for</span> the cluster to initialize: Cluster operator image-registry is still updating </span>
<span class="line"></span>
<span class="line">DEBUG Still waiting <span class="token keyword">for</span> the cluster to initialize: Cluster operator image-registry is still updating </span>
<span class="line"></span>
<span class="line">DEBUG Cluster is initialized </span>
<span class="line"></span>
<span class="line">INFO Waiting up to 10m0s <span class="token keyword">for</span> the openshift-console route to be created<span class="token punctuation">..</span>.</span>
<span class="line"></span>
<span class="line">DEBUG Route found <span class="token keyword">in</span> openshift-console namespace: console </span>
<span class="line"></span>
<span class="line">DEBUG Route found <span class="token keyword">in</span> openshift-console namespace: downloads </span>
<span class="line"></span>
<span class="line">DEBUG OpenShift console route is created </span>
<span class="line"></span>
<span class="line">INFO Install complete<span class="token operator">!</span> </span>
<span class="line"></span>
<span class="line">INFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp.ngs.local</span>
<span class="line"></span>
<span class="line">INFO Login to the console with user: kubeadmin, password: a6hKv-okLUA-Q9p3q-UXLc3</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The RHOCP cluster is successfully installed.</p><ol start="5"><li>After the installation is complete, check the status of the created cluster:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc get nodes</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="running-red-hat-openshift-container-platform-console" tabindex="-1"><a class="header-anchor" href="#running-red-hat-openshift-container-platform-console"><span><strong>Running Red Hat OpenShift Container Platform Console</strong></span></a></h4><p><strong>Prerequisites:</strong></p><p>The RHOCP cluster installation must be complete.</p><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>The installer machine provides the Red Hat OpenShift Container Platform Console link and login details when the RHOCP cluster installation is complete.</p></div><p>To access the Red Hat OpenShift Container Platform Console:</p><ol><li>Open a web browser and enter the following link:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">https://console-openshift-console.apps.<span class="token operator">&lt;</span>customer.defined.domain<span class="token operator">&gt;</span> </span>
<span class="line"></span>
<span class="line">Sample one <span class="token keyword">for</span> reference:  https://console-openshift-console.apps.ocp.ngs.local</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Log in to the Red Hat OpenShift Container Platform Console with the following credentials:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">- Username: kubeadmin</span>
<span class="line">- Password: <span class="token operator">&lt;</span>password<span class="token operator">&gt;</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>If the password is lost or forgotten, search for the kubeadmin-password file located in the /opt/hpe-solutions-openshift/DL-LTI-Openshift/playbooks/roles/generate_ignition_files/ignitions/auth/kubeadmin-password directory on the installer machine.</p></div><p>The following figure shows the Red Hat OpenShift Container Platform Console after successful deployment:</p><p><img src="`+l+`" alt=""></p><p><strong>FIGURE 8.</strong> Red Hat OpenShift Container Platform Console login screen</p><hr><h2 id="adding-rhel-9-4-worker-nodes-to-rhocp-cluster-using-ansible-playbooks" tabindex="-1"><a class="header-anchor" href="#adding-rhel-9-4-worker-nodes-to-rhocp-cluster-using-ansible-playbooks"><span><strong>Adding RHEL 9.4 worker nodes to RHOCP cluster using Ansible playbooks</strong></span></a></h2><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>RHEL Worker Nodes are supported as best effort and require it own update and lifecycle management hence as not actively recommended.</p></div><p>The Lite Touch Installation (LTI) package includes Ansible playbooks with scripts to add the RHEL 9.4 worker nodes to the RHOCP cluster. You can use one of the following two methods to add the RHEL 9.4 worker nodes:</p><ul><li><strong>Run a consolidated playbook:</strong> This method includes a single playbook, site.yml, that contains a script to perform all the tasks for adding the RHEL 9.4 worker nodes to the existing RHOCP cluster. To run LTI using a consolidated playbook:</li></ul><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> inventory/hosts site.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>The default password for the Ansible vault file is <strong>changeme</strong></p></div><ul><li><strong>Run individual playbooks:</strong> This method includes multiple playbooks with scripts that enable you to deploy specific tasks for adding the RHEL 9.4 worker nodes to the existing RHOCP cluster. The playbooks in this method must be executed in a specific sequence to add the worker nodes.</li></ul><p>The following table includes the purpose of each playbook required for the deployment:</p><p><strong>TABLE 9.</strong> Add RHEL 9.4 nodes using Ansible playbooks</p><table><thead><tr><th style="text-align:left;">Playbook</th><th style="text-align:left;">Description</th></tr></thead><tbody><tr><td style="text-align:left;">rhel9_os_deployment.yml</td><td style="text-align:left;">This playbook contains the scripts to deploy RHEL 9.4 OS on worker nodes.</td></tr><tr><td style="text-align:left;">copy_ssh.yml</td><td style="text-align:left;">This playbook contains the script to copy the SSH public key to the RHEL 9.4 worker nodes.</td></tr><tr><td style="text-align:left;">prepare_worker_nodes.yml</td><td style="text-align:left;">This playbook contains the script to prepare nodes for the RHEL 9.4 worker nodes.</td></tr><tr><td style="text-align:left;">ntp.yml</td><td style="text-align:left;">This playbook contains the script to create NTP setup to enable time synchronization on the worker nodes.</td></tr><tr><td style="text-align:left;">openshift-ansible/playbooks/scaleup.yml</td><td style="text-align:left;">This playbook contains the script to add worker nodes to the RHOCP cluster. This playbook queries the master, generates and distributes new certificates for the new hosts, and then runs the configuration playbooks on the new hosts.</td></tr></tbody></table><p>To run individual playbooks do one of the following:</p><ol><li>Edit site.yaml file and add a comment for all the playbooks except the ones that you want to execute.</li></ol><p>For example, add the following comments in the site.yaml file to deploy RHEL 9.4 OS on the worker nodes:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">import_playbook: playbooks/rhel9_os_deployment.yml</span>
<span class="line"></span>
<span class="line"><span class="token comment"># import_playbook: playbooks/copy_ssh.yml</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># import_playbook: playbooks/prepare_worker_nodes.yml</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># import_playbook: playbooks/ntp.yml</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># import_playbook: openshift-ansible/playbooks/scaleup.yml</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>OR</p><p>Run the individual YAML files using the following command:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> inventory/hosts playbooks/<span class="token operator">&lt;</span>yaml_filename<span class="token operator">&gt;</span>.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>For example, run the following YAML file to deploy RHEL 9.4 OS on the worker nodes:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> inventory/hosts playbooks/rhel9_os_deployment.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>For more information on executing individual playbooks, see the consecutive sections.</p><h4 id="adding-rhel-9-4-worker-nodes" tabindex="-1"><a class="header-anchor" href="#adding-rhel-9-4-worker-nodes"><span><strong>Adding RHEL 9.4 worker nodes</strong></span></a></h4><p>This section describes how to add RHEL 9.4 worker nodes to an existing RHOCP cluster.</p><p>To add RHEL 9.4 worker nodes to the RHOCP cluster:</p><ol><li>Login to the Installer VM.</li></ol><p>This installer VM was created as a KVM VM on one of the head nodes using the rhel9_installerVM.yml playbook. For more information, see the <a href="#creating-rhel-9-installer-machine">Creating RHEL 9 installer machine</a> section.</p><ol start="2"><li>Navigate to the directory $BASE_DIR/worker_nodes/</li></ol><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">cd $BASE_DIR/worker_nodes/</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>$BASE_DIR refers to <strong>/opt/hpe-solutions-openshift/DL-LTI-Openshift/</strong></p></div><p>Run the following commands on the rhel9 installer VM to edit the vault input file.</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">ansible-vault edit input.yaml</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>The installation user should review hosts file (located on the installer VM at $BASE_DIR/inventory/hosts)</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">vi inventory/hosts</span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><ol start="3"><li>Copy Rhel9.4 DVD ISO to <strong>/usr/share/nginx/html/</strong></li><li>Navigate to the $BASE_DIR/worker_nodes/ directory and run the following command:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ <span class="token function">sh</span> setup.sh</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="5"><li>Add the worker nodes to the cluster using one of the following methods:</li></ol><ul><li>Run the following sequence of playbooks:</li></ul><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">ansible-playbook <span class="token parameter variable">-i</span> inventory/hosts playbooks/rhel9_os_deployment.yml --ask-vault-pass</span>
<span class="line"></span>
<span class="line">ansible-playbook <span class="token parameter variable">-i</span> inventory/hosts playbooks/copy_ssh.yml --ask-vault-pass</span>
<span class="line"></span>
<span class="line">ansible-playbook <span class="token parameter variable">-i</span> inventory/hosts playbooks/prepare_worker_nodes.yml --ask-vault-pass</span>
<span class="line"></span>
<span class="line">ansible-playbook <span class="token parameter variable">-i</span> inventory/hosts playbooks/ntp.yml --ask-vault-pass</span>
<span class="line"></span>
<span class="line">ansible-playbook <span class="token parameter variable">-i</span> inventory/hosts openshift-ansible/playbooks/scaleup.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>OR</p><ul><li>If you want to deploy the entire solution to add the worker nodes to the cluster, execute the following playbook:</li></ul><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> inventory/hosts site.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="6"><li>Once all the playbooks are executed successfully, check the status of the node using the following command:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc get nodes</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>The following output is displayed:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">NAME			STATUS	ROLES		AGE	VERSION</span>
<span class="line"></span>
<span class="line">master0.ocp.ngs.local	Ready	master,worker	3d	v1.29.4+8ca71f7</span>
<span class="line"></span>
<span class="line">master1.ocp.ngs.local	Ready	master,worker	3d	v1.29.4+8ca71f7</span>
<span class="line"></span>
<span class="line">master2.ocp.ngs.local	Ready	master,worker	3d	v1.29.4+8ca71f7</span>
<span class="line"></span>
<span class="line">worker1.ocp.ngs.local	Ready	worker		1d	v1.29.4+8ca71f7</span>
<span class="line"></span>
<span class="line">worker2.ocp.ngs.local	Ready	worker		1d	v1.29.4+8ca71f7</span>
<span class="line"></span>
<span class="line">worker3.ocp.ngs.local	Ready	worker		1d	v1.29.4+8ca71f7</span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol start="7"><li>Once the worker nodes are added to the cluster, set the “mastersSchedulable” parameter as false to ensure that the master nodes are not used to schedule pods.</li><li>Edit the schedulers.config.openshift.io resource.</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc edit schedulers.config.openshift.io cluster</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>Configure the mastersSchedulable field.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">apiVersion: config.openshift.io/v1 </span>
<span class="line"></span>
<span class="line">kind: Scheduler </span>
<span class="line"></span>
<span class="line">metadata: </span>
<span class="line"></span>
<span class="line"><span class="token variable"><span class="token variable">\`</span>	<span class="token variable">\`</span></span>creationTimestamp: “2024-01-04T09:20:06Z<span class="token string">&quot;</span>
<span class="line"></span>
<span class="line"><span class="token variable"><span class="token variable">\`</span>	<span class="token variable">\`</span></span>generation: 2</span>
<span class="line"></span>
<span class="line"><span class="token variable"><span class="token variable">\`</span>	<span class="token variable">\`</span></span>name: cluster</span>
<span class="line"></span>
<span class="line"><span class="token variable"><span class="token variable">\`</span>	<span class="token variable">\`</span></span>resourceVersion: “5939203&quot;</span></span>
<span class="line"></span>
<span class="line"><span class="token variable"><span class="token variable">\`</span>	<span class="token variable">\`</span></span>uid: a636d30a-d377-11e9-88d4-0a60097bee62</span>
<span class="line"></span>
<span class="line">spec:</span>
<span class="line"></span>
<span class="line"><span class="token variable"><span class="token variable">\`</span>	<span class="token variable">\`</span></span>mastersSchedulable: <span class="token boolean">false</span> </span>
<span class="line"></span>
<span class="line"><span class="token variable"><span class="token variable">\`</span>	<span class="token variable">\`</span></span>policy:</span>
<span class="line"></span>
<span class="line"><span class="token variable"><span class="token variable">\`</span>		<span class="token variable">\`</span></span>name: “&quot;</span>
<span class="line"></span>
<span class="line">status: <span class="token punctuation">{</span> <span class="token punctuation">}</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>Set the mastersSchedulable to true to allow Control Plane nodes to be schedulable or false to disallow Control Plane nodes to be schedulable.</p></div><ol start="9"><li>Save the file to apply the changes.</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc get nodes</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>The following output is displayed:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">NAME			STATUS	ROLES	AGE	VERSION</span>
<span class="line"></span>
<span class="line">master0.ocp.ngs.local	Ready	master	3d	v1.29.4+8ca71f7</span>
<span class="line"></span>
<span class="line">master1.ocp.ngs.local	Ready	master	3d	v1.29.4+8ca71f7</span>
<span class="line"></span>
<span class="line">master2.ocp.ngs.local	Ready	master	3d	v1.29.4+8ca71f7</span>
<span class="line"></span>
<span class="line">worker1.ocp.ngs.local	Ready	worker	1d	v1.29.4+8ca71f7</span>
<span class="line"></span>
<span class="line">worker2.ocp.ngs.local	Ready	worker	1d	v1.29.4+8ca71f7</span>
<span class="line"></span>
<span class="line">worker3.ocp.ngs.local	Ready	worker	1d	v1.29.4+8ca71f7</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>To add more worker nodes, update worker details in HAProxy and binddns on head nodes and then add RHEL 9.4 worker nodes to the RHOCP cluster.</p></div><hr><h2 id="adding-rhcos-worker-nodes-to-rhocp-cluster-using-ansible-playbooks" tabindex="-1"><a class="header-anchor" href="#adding-rhcos-worker-nodes-to-rhocp-cluster-using-ansible-playbooks"><span><strong>Adding RHCOS worker nodes to RHOCP cluster using Ansible playbooks</strong></span></a></h2><p>The Lite Touch Installation (LTI) package includes Ansible playbooks with scripts to add the RHCOS worker nodes to the RHOCP cluster. You can use one of the following two methods to add the RHCOS worker nodes:</p><ul><li><strong>Run a consolidated playbook:</strong> This method includes a single playbook, site.yml, that contains a script to perform all the tasks for adding the RHCOS worker nodes to the existing RHOCP cluster. To run LTI using a consolidated playbook:</li></ul><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts site.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>The default password for the Ansible vault file is <strong>changeme</strong></p></div><ul><li><strong>Run individual playbooks:</strong> This method includes multiple playbooks with scripts that enable you to deploy specific tasks for adding the RHCOS worker nodes to the existing RHOCP cluster. The playbooks in this method must be executed in a specific sequence to add the worker nodes.</li></ul><p>The following table includes the purpose of each playbook required for the deployment:</p><p><strong>TABLE 9.</strong> Playbook Description</p><table><thead><tr><th style="text-align:left;">Playbook</th><th style="text-align:left;">Description</th></tr></thead><tbody><tr><td style="text-align:left;">rhel9_os_deployment.yml</td><td style="text-align:left;">This playbook contains the scripts to deploy RHEL 9.4 OS on baremetal servers.</td></tr><tr><td style="text-align:left;">copy_ssh_workernode.yml</td><td style="text-align:left;">This playbook contains the script to copy the ssh public key from installer machine to the KVM worker nodes.</td></tr><tr><td style="text-align:left;">prepare_rhel_hosts.yml</td><td style="text-align:left;">This playbook contains the script to prepare KVM worker nodes with required packages and subscription.</td></tr><tr><td style="text-align:left;">ntp.yml</td><td style="text-align:left;">This playbook contains the script to create NTP setup on KVM worker nodes to make sure time synchronization.</td></tr><tr><td style="text-align:left;">binddns.yml</td><td style="text-align:left;">This playbook contains the script to deploy bind dns on three head nodes and it will work as both Active &amp; Passive.</td></tr><tr><td style="text-align:left;">haproxy.yml</td><td style="text-align:left;">This playbook contains the script to deploy haproxy on the head nodes and it will act as Active.</td></tr><tr><td style="text-align:left;">storage_pool.yml</td><td style="text-align:left;">This playbook contains the script to create the storage pools on the KVM Worker nodes.</td></tr><tr><td style="text-align:left;">deploy_ipxe_ocp.yml</td><td style="text-align:left;">This playbook contains the script to deploy the ipxe code on the RHEL 9 installer machine.</td></tr><tr><td style="text-align:left;">ocp_rhcosworkervm.yml</td><td style="text-align:left;">This playbook contains the script to add kvm based coreos nodes to exsting Openshift cluster.</td></tr></tbody></table><p>To run individual playbooks do one of the following:</p><ol><li>Edit site.yaml file and add a comment for all the playbooks except the ones that you want to execute.</li></ol><p>For example, add the following comments in the site.yaml file to deploy RHCOS on the worker nodes:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">import_playbook: playbooks/rhel9_os_deployment.yml</span>
<span class="line"><span class="token comment"># import_playbook: playbooks/copy_ssh_workernode.yml</span></span>
<span class="line"><span class="token comment"># import_playbook: playbooks/prepare_rhel_hosts.yml</span></span>
<span class="line"><span class="token comment"># import_playbook: playbooks/ntp.yml</span></span>
<span class="line"><span class="token comment"># import_playbook: playbooks/binddns.yml</span></span>
<span class="line"><span class="token comment"># import_playbook: playbooks/haproxy.yml</span></span>
<span class="line"><span class="token comment"># import_playbook: playbooks/storage_pool.yml</span></span>
<span class="line"><span class="token comment"># import_playbook: playbooks/deploy_ipxe_ocp.yml</span></span>
<span class="line"><span class="token comment"># import_playbook: playbooks/ocp_rhcosworkervm.yml</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>OR</p><p>Run the individual YAML files using the following command:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/<span class="token operator">&lt;</span>yaml_filename<span class="token operator">&gt;</span>.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>For example, run the following YAML file to deploy RHEL 9.4 OS on the worker nodes:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/rhel9_os_deployment.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>For more information on executing individual playbooks, see the consecutive sections.</p><h4 id="adding-rhcos-worker-nodes" tabindex="-1"><a class="header-anchor" href="#adding-rhcos-worker-nodes"><span><strong>Adding RHCOS worker nodes</strong></span></a></h4><p>This section covers the steps to Enable KVM hypervisor on Worker Nodes and add RHCOS worker VM nodes to an existing Red Hat OpenShift Container Platform cluster.</p><ol><li>Login to the Installer VM.</li></ol><p>This installer VM was created as a KVM VM on one of the head nodes using the rhel9_installerVM.yml playbook. For more information, see the <a href="#creating-rhel-9-installer-machine">Creating RHEL 9 installer machine</a> section.</p><ol start="2"><li>Navigate to the $BASE_DIR(<strong>/opt/hpe-solutions-openshift/DL-LTI-Openshift/</strong>) directory, then copy <strong>input file and hosts</strong> file to $BASE_DIR/coreos_kvmworker_nodes/ and later update ocp worker details in input file and kvm_workernodes group as per sample host file.</li></ol><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">ansible-vault edit input.yaml</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">vi hosts</span>
<span class="line">    &#39;[kvm_workernodes]</span>
<span class="line">    KVMworker1 IP</span>
<span class="line">    KVMworker2 IP</span>
<span class="line">    KVMworker3 IP&#39;</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>ansible vault password is <strong>changeme</strong></p></div><ol start="3"><li>Copy RHEL 9.4 DVD ISO to the /usr/share/nginx/html/ directory.</li><li>Navigate to the /opt/hpe-solutions-openshift/DL-LTI-Openshift/coreos_kvmworker_nodes/ directory add the worker nodes to the cluster using one of the following methods:</li></ol><ul><li>Run the following sequence of playbooks:</li></ul><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">    ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/rhel9_os_deployment.yml --ask-vault-pass</span>
<span class="line">    ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/copy_ssh_workernode.yml --ask-vault-pass</span>
<span class="line">    ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/prepare_rhel_hosts.yml --ask-vault-pass</span>
<span class="line">    ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/ntp.yml --ask-vault-pass</span>
<span class="line">    ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/binddns.yml --ask-vault-pass</span>
<span class="line">    ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/haproxy.yml --ask-vault-pass</span>
<span class="line">    ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/storage_pool.yml --ask-vault-pass</span>
<span class="line">    ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/deploy_ipxe_ocp.yml --ask-vault-pass</span>
<span class="line">    ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/ocp_rhcosworkervm.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>OR</p><ul><li>If you want to deploy the entire solution to add the RHCOS worker nodes to the cluster, execute the following playbook:</li></ul><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts site.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="5"><li>After successful execution of all playbooks, check the node status as below.</li></ol><p><strong>Approving server certificates (CSR) for newly added nodes</strong></p><p>The administrator needs to approve the CSR requests generated by each kubelet.</p><p>You can approve all Pending CSR requests using below command</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc get csr <span class="token parameter variable">-o</span> json <span class="token operator">|</span> jq <span class="token parameter variable">-r</span> <span class="token string">&#39;.items[] | select(.status == {} ) | .metadata.name&#39;</span> <span class="token operator">|</span> <span class="token function">xargs</span> oc adm certificate approve</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="6"><li>Later, Verify Node status using below command:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc get nodes</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="7"><li>Execute the following command to set the parameter mastersSchedulable parameter as false, so that master nodes will not be used to schedule pods.</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc edit scheduler</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><hr><h2 id="adding-baremetal-coreos-worker-nodes-to-rhocp-cluster-using-ansible-playbooks" tabindex="-1"><a class="header-anchor" href="#adding-baremetal-coreos-worker-nodes-to-rhocp-cluster-using-ansible-playbooks"><span><strong>Adding BareMetal CoreOS worker nodes to RHOCP cluster using Ansible playbooks</strong></span></a></h2><p>The Lite Touch Installation (LTI) package includes Ansible playbooks with scripts to add the bare metal CoreOS worker nodes to the RHOCP cluster. You can use one of the following two methods to add the CoreOS worker nodes:</p><ul><li><strong>Run a consolidated playbook:</strong> This method includes a single playbook, site.yml, that contains a script to perform all the tasks for adding the CoreOS worker nodes to the existing RHOCP cluster. To run LTI using a consolidated playbook:</li></ul><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts site.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>The default password for the Ansible vault file is <strong>changeme</strong></p></div><ul><li><strong>Run individual playbooks:</strong> This method includes multiple playbooks with scripts that enable you to deploy specific tasks for adding the CoreOS worker nodes to the existing RHOCP cluster. The playbooks in this method must be executed in a specific sequence to add the worker nodes.</li></ul><p>The following table includes the purpose of each playbook required for the deployment:</p><p><strong>TABLE 9.</strong> Playbook Description</p><table><thead><tr><th style="text-align:left;">Playbook</th><th style="text-align:left;">Description</th></tr></thead><tbody><tr><td style="text-align:left;">binddns.yml</td><td style="text-align:left;">This playbook contains the script to deploy bind dns on three worker nodes and it will work as both Active &amp; Passive.</td></tr><tr><td style="text-align:left;">haproxy.yml</td><td style="text-align:left;">This playbook contains the script to deploy haproxy on the worker nodes and it will act as Active.</td></tr><tr><td style="text-align:left;">deploy_ipxe_ocp.yml</td><td style="text-align:left;">This playbook contains the script to deploy the ipxe code on the worker machine.</td></tr></tbody></table><p>To run individual playbooks do one of the following:</p><ol><li>Edit site.yml file and add a comment for all the playbooks except the ones that you want to execute.</li></ol><p>For example, add the following comments in the site.yml file to bind dns on the worker nodes:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">import_playbook: playbooks/binddns.yml</span>
<span class="line"><span class="token comment"># import_playbook: playbooks/haproxy.yml</span></span>
<span class="line"><span class="token comment"># import_playbook: playbooks/deploy_ipxe_ocp.yml</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>OR</p><p>Run the individual YAML files using the following command:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/<span class="token operator">&lt;</span>yaml_filename<span class="token operator">&gt;</span>.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>For example, run the following YAML file to bind dns to the worker nodes:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/binddns.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>For more information on executing individual playbooks, see the consecutive sections.</p><h4 id="adding-coreos-worker-nodes" tabindex="-1"><a class="header-anchor" href="#adding-coreos-worker-nodes"><span><strong>Adding CoreOS worker nodes</strong></span></a></h4><p>This section covers the steps to add RHCOS worker nodes to an existing Red Hat OpenShift Container Platform cluster.</p><ol><li>Login to the Installer VM.</li></ol><p>This installer VM was created as a KVM VM on one of the head nodes using the rhel8_installerVM.yml playbook. For more information, see the <a href="#creating-rhel-9-installer-machine">Creating RHEL 9 installer machine</a> section.</p><ol start="2"><li>Navigate to the $BASE_DIR(<strong>/opt/hpe-solutions-openshift/DL-LTI-Openshift/</strong>) directory, then copy <strong>input file and hosts</strong> file to $BASE_DIR/coreos_BareMetalworker_nodes/ and later update ocp worker details in input file.</li></ol><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">ansible-vault edit input.yaml</span>
<span class="line">------------------------------------------------------------------------------------------------------------</span>
<span class="line">ocp_workers:</span>
<span class="line"> - name: worker1</span>
<span class="line">   ip: 172.28.xx.xxx</span>
<span class="line">   fqdn: xxx.ocp.isv.local                   #ex. mworker1.ocp.isv.local</span>
<span class="line">   mac_address: XX:XX:XX:XX:XX:XX			 #For BareMetal core os worker update mac address of server NIC</span>
<span class="line"> - name: worker2</span>
<span class="line">   ip: 172.28.xx.xxx</span>
<span class="line">   fqdn: xxx.ocp.isv.local                 #ex. mworker2.ocp.isv.local</span>
<span class="line">   mac_address: XX:XX:XX:XX:XX:XX 		   #For BareMetal core os worker update mac address of server NIC</span>
<span class="line"> - name: worker3</span>
<span class="line">   ip: 172.28.xx.xxx</span>
<span class="line">   fqdn: xxx.ocp.isv.local                   #ex. mworker3.ocp.isv.local</span>
<span class="line">   mac_address: XX:XX:XX:XX:XX:XX 		     #For BareMetal core os worker update mac address of server NIC</span>
<span class="line">------------------------------------------------------------------------------------------------------------</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>import the hosts file from the $BASE_DIR</p><p>ansible vault password is <strong>changeme</strong></p></div><ol start="3"><li>Navigate to the /opt/hpe-solutions-openshift/DL-LTI-Openshift/coreos_BareMetalworker_nodes/ directory add the worker nodes to the cluster using one of the following methods:</li></ol><ul><li>Run the following sequence of playbooks:</li></ul><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/binddns.yml --ask-vault-pass</span>
<span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/haproxy.yml --ask-vault-pass</span>
<span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts playbooks/deploy_ipxe_ocp.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>OR</p><ul><li>If you want to deploy the entire solution to add the RH CoreOS worker nodes to the cluster, execute the following playbook:</li></ul><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ ansible-playbook <span class="token parameter variable">-i</span> hosts site.yml --ask-vault-pass</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="4"><li>After successful execution of all playbooks, check the node status as below.</li></ol><p><strong>Approving server certificates (CSR) for newly added nodes</strong></p><p>The administrator needs to approve the CSR requests generated by each kubelet.</p><p>You can approve all Pending CSR requests using below command</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc get csr <span class="token parameter variable">-o</span> json <span class="token operator">|</span> jq <span class="token parameter variable">-r</span> <span class="token string">&#39;.items[] | select(.status == {} ) | .metadata.name&#39;</span> <span class="token operator">|</span> <span class="token function">xargs</span> oc adm certificate approve</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="5"><li>Later, Verify Node status using below command:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc get nodes</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="6"><li>Execute the following command to set the parameter mastersSchedulable parameter as false, so that master nodes will not be used to schedule pods.</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc edit scheduler</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><hr><h2 id="adding-coreos-gpu-worker-nodes-to-rhocp-cluster" tabindex="-1"><a class="header-anchor" href="#adding-coreos-gpu-worker-nodes-to-rhocp-cluster"><span>Adding COREOS GPU worker nodes to RHOCP cluster</span></a></h2><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>For addition of worker node into the RHOCP cluster, you can follow the process documented at section &quot;Adding BareMetal CoreOS worker nodes to RHOCP cluster using Ansible playbooks&quot;.</p></div><h4 id="nvidia-gpu-operator-on-openshift-cluster" tabindex="-1"><a class="header-anchor" href="#nvidia-gpu-operator-on-openshift-cluster"><span>NVIDIA GPU Operator on openshift cluster</span></a></h4><p>NVIDIA supports the use of graphics processing unit (GPU) resources on OpenShift Container Platform.</p><p>The NVIDIA GPU Operator leverages the Operator framework within OpenShift Container Platform to manage the full lifecycle of NVIDIA software components required to run GPU-accelerated workloads.</p><p>The prerequisites needed for running containers and VMs with GPU(s) differs, with the primary difference being the drivers required. For example, the datacenter driver is needed for containers, the vfio-pci driver is needed for GPU passthrough, and the NVIDIA vGPU Manager is needed for creating vGPU devices.</p><ul><li><p><strong>Prerequisites:</strong></p></li><li><p>A working OpenShift 4.16 cluster with GPU enabled worker node.</p></li><li><p>Access to the OpenShift cluster as a cluster-admin to perform the required steps.</p></li><li><p>OpenShift CLI (oc) is installed.</p></li><li><p>OpenShift Virtualization operator is installed</p></li><li><p>NFD Operator need to install</p></li></ul><h4 id="installing-the-node-feature-discovery-nfd-operator" tabindex="-1"><a class="header-anchor" href="#installing-the-node-feature-discovery-nfd-operator"><span>Installing the Node Feature Discovery (NFD) Operator</span></a></h4><p>The Node Feature Discovery (NFD) Operator is a prerequisite for the NVIDIA GPU Operator. Install the NFD Operator using the Red Hat OperatorHub catalog in the OpenShift Container Platform web console</p><p><strong>Procedure</strong></p><ol><li>In the OpenShift Container Platform web console, click Operators → OperatorHub.</li><li>Choose Node Feature Discovery from the list of available Operators, and then click Install.</li><li>On the Install Operator page, select A specific namespace on the cluster, and then click Install. You do not need to create a namespace because it is created for you.</li></ol><p><strong>Verification</strong></p><p>To verify that the NFD Operator installed successfully:</p><ol><li>Navigate to the Operators → Installed Operators page.</li><li>Ensure that Node Feature Discovery is listed in the openshift-nfd project with a Status of InstallSucceeded.</li></ol><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>During installation an Operator might display a Failed status. If the installation later succeeds with an InstallSucceeded message, you can ignore the Failed message.</p></div><p><img src="`+o+`" alt=""></p><h4 id="create-nodefeaturediscovery-cr-instance" tabindex="-1"><a class="header-anchor" href="#create-nodefeaturediscovery-cr-instance"><span>Create NodeFeatureDiscovery CR instance</span></a></h4><p>When the Node Feature Discovery is installed, create an instance of Node Feature Discovery using the NodeFeatureDiscovery tab.</p><ul><li>Click Operators &gt; Installed Operators from the side menu.</li><li>Find the Node Feature Discovery entry.</li><li>Click NodeFeatureDiscovery under the Provided APIs field.</li><li>Click Create NodeFeatureDiscovery.</li><li>In the subsequent screen click Create. This starts the Node Feature Discovery Operator that proceeds to label the nodes in the cluster that have GPUs.</li></ul><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>The values pre-populated by the OperatorHub are valid for the GPU Operator.</p></div><h4 id="verify-that-the-node-feature-discovery-operator-is-functioning-correctly" tabindex="-1"><a class="header-anchor" href="#verify-that-the-node-feature-discovery-operator-is-functioning-correctly"><span>Verify that the Node Feature Discovery Operator is functioning correctly</span></a></h4><p>The Node Feature Discovery Operator uses vendor PCI IDs to identify hardware in a node. NVIDIA uses the PCI ID 10de. Use the OpenShift Container Platform web console or the CLI to verify that the Node Feature Discovery Operator is functioning correctly.</p><ul><li>In the OpenShift Container Platform web console, click Compute &gt; Nodes from the side menu.</li><li>Select a worker node that you know contains a GPU.</li><li>Click the Details tab.</li><li>Under Node labels verify that the following label is present: feature.node.kubernetes.io/pci-10de.present=true</li></ul><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>0x10de is the PCI vendor ID that is assigned to NVIDIA.</p></div><ul><li>Verify the GPU device (pci-10de) is discovered on the GPU node:</li></ul><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc describe <span class="token function">node</span> <span class="token operator">|</span> <span class="token function">egrep</span> <span class="token string">&#39;Roles|pci&#39;</span> <span class="token operator">|</span> <span class="token function">grep</span> <span class="token parameter variable">-v</span> master</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="`+r+`" alt=""></p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc get nodes <span class="token parameter variable">-l</span> feature.node.kubernetes.io/pci-10de.present</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="`+p+`" alt=""></p><h4 id="enabling-the-iommu-driver-on-hosts" tabindex="-1"><a class="header-anchor" href="#enabling-the-iommu-driver-on-hosts"><span>Enabling the IOMMU driver on hosts</span></a></h4><p>To enable the IOMMU (Input-Output Memory Management Unit) driver in the kernel, create the MachineConfig object and add the kernel arguments.</p><div class="hint-container tip"><p class="hint-container-title">NOTE</p><p>Enabling IOMMU is needed for GPU with Openshift Virtualization</p></div><ul><li><strong>Prerequisites:</strong><ul><li>Administrative privilege to a working OpenShift Container Platform cluster.</li><li>Intel or AMD CPU hardware.</li><li>Intel Virtualization Technology for Directed I/O extensions or AMD IOMMU in the BIOS (Basic Input/Output System) is enabled.</li></ul></li><li><strong>Procedure:</strong><ul><li>Create a MachineConfig object that identifies the kernel argument. The following example shows a kernel argument for an Intel CPU:</li></ul></li></ul><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">apiVersion: machineconfiguration.openshift.io/v1</span>
<span class="line">kind: MachineConfig</span>
<span class="line">metadata:</span>
<span class="line">  labels:</span>
<span class="line">    machineconfiguration.openshift.io/role: worker</span>
<span class="line">  name: <span class="token number">100</span>-worker-iommu</span>
<span class="line">spec:</span>
<span class="line">  config:</span>
<span class="line">    ignition:</span>
<span class="line">      version: <span class="token number">3.2</span>.0</span>
<span class="line">  kernelArguments:</span>
<span class="line">      - <span class="token assign-left variable">intel_iommu</span><span class="token operator">=</span>on</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><pre><code>- Create the new MachineConfig object:
</code></pre><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc create <span class="token parameter variable">-f</span> <span class="token number">100</span>-worker-kernel-arg-iommu.yaml</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><pre><code>- Verify that the new MachineConfig object was added:
</code></pre><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc get machineconfig</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="labeling-worker-nodes" tabindex="-1"><a class="header-anchor" href="#labeling-worker-nodes"><span>Labeling worker nodes</span></a></h4><p>The GPU Operator uses the value of the label to determine which operands to deploy. Assign the following values to the label: container, vm-passthrough, and vm-vgpu. Use the following command to add a label to a worker node:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc label <span class="token function">node</span> <span class="token operator">&lt;</span>node1-name<span class="token operator">&gt;</span> <span class="token parameter variable">--overwrite</span> nvidia.com/gpu.workload.config<span class="token operator">=</span>vm-vgpu</span>
<span class="line">$ oc label <span class="token function">node</span> <span class="token operator">&lt;</span>node2-name<span class="token operator">&gt;</span> <span class="token parameter variable">--overwrite</span> nvidia.com/gpu.workload.config<span class="token operator">=</span>container</span>
<span class="line">$ oc label <span class="token function">node</span> <span class="token operator">&lt;</span>node3-name<span class="token operator">&gt;</span> <span class="token parameter variable">--overwrite</span> nvidia.com/gpu.workload.config<span class="token operator">=</span> vm-passthrough</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><img src="`+d+`" alt=""></p><h4 id="verify-the-gpu-device-details-before-installation" tabindex="-1"><a class="header-anchor" href="#verify-the-gpu-device-details-before-installation"><span>Verify the GPU device details before installation.</span></a></h4><p>ssh to the node, you can list the NVIDIA GPU devices with a command like the following example:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ lspci <span class="token parameter variable">-nnk</span> <span class="token parameter variable">-d</span> 10de:</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="`+c+`" alt=""></p><h4 id="installing-the-nvidia-gpu-operator-by-using-the-web-console" tabindex="-1"><a class="header-anchor" href="#installing-the-nvidia-gpu-operator-by-using-the-web-console"><span>Installing the NVIDIA GPU Operator by using the web console</span></a></h4><ol><li>In the OpenShift Container Platform web console from the side menu, navigate to Operators &gt; OperatorHub and select All Projects.</li><li>In Operators &gt; OperatorHub, search for the NVIDIA GPU Operator. For additional information see the Red Hat OpenShift Container Platform documentation.</li></ol><div class="hint-container tip"><p class="hint-container-title">Note</p><p>The suggested namespace to use is the nvidia-gpu-operator. You can choose any existing namespace or create a new namespace name. If you install in any other namespace other than nvidia-gpu-operator, the GPU Operator will not automatically enable namespace monitoring, and metrics and alerts will not be collected by Prometheus.</p></div><p>If only trusted operators are installed in this namespace, you can manually enable namespace monitoring with this command:</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">$ oc label ns/$NAMESPACE_NAME openshift.io/cluster-monitoring=true</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="3"><li>Select the NVIDIA GPU Operator, click Install. In the subsequent screen click Install <img src="`+h+`" alt=""></li></ol><p><strong>FIGURE 9.</strong> NVIDIA GPU Operator deployment</p><h4 id="setup-openshift-internal-image-registry-to-upload-vgpu-manager-image" tabindex="-1"><a class="header-anchor" href="#setup-openshift-internal-image-registry-to-upload-vgpu-manager-image"><span>Setup Openshift internal image registry to upload vGPU Manager image</span></a></h4><p>To start the image registry, change the Image Registry Operator configuration’s managementState from Removed to Managed</p><p>$ oc patch configs.imageregistry.operator.openshift.io cluster –type merge –patch ‘{“spec”:{“managementState”:”Managed”}}’</p><p>Image registry storage configuration:</p><p>Configuring the Image Registry Operator to use CephFS storage with Red Hat OpenShift Data Foundation</p><p>Create a PVC to use the cephfs storage class. For example:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line"><span class="token function">cat</span> <span class="token operator">&lt;&lt;</span><span class="token string">EOF<span class="token bash punctuation"> <span class="token operator">|</span> oc apply <span class="token parameter variable">-f</span> -</span></span>
<span class="line">apiVersion: v1</span>
<span class="line">kind: PersistentVolumeClaim</span>
<span class="line">metadata:</span>
<span class="line"> name: registry-storage-pvc</span>
<span class="line"> namespace: openshift-image-registry</span>
<span class="line">spec:</span>
<span class="line"> accessModes:</span>
<span class="line"> - ReadWriteMany</span>
<span class="line"> resources:</span>
<span class="line">   requests:</span>
<span class="line">     storage: 100Gi</span>
<span class="line"> storageClassName: ocs-storagecluster-cephfs</span>
<span class="line">EOF</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Configure the image registry to use the CephFS file system storage by entering the following command:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc patch config.image/cluster <span class="token parameter variable">-p</span> <span class="token string">&#39;{&quot;spec&quot;:{&quot;managementState&quot;:&quot;Managed&quot;,&quot;replicas&quot;:2,&quot;storage&quot;:{&quot;managementState&quot;:&quot;Unmanaged&quot;,&quot;pvc&quot;:{&quot;claim&quot;:&quot;registry-storage-pvc&quot;}}}}&#39;</span> <span class="token parameter variable">--type</span><span class="token operator">=</span>merge</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>Set Default Route to True:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc patch configs.imageregistry.operator.openshift.io/cluster <span class="token parameter variable">--patch</span> <span class="token string">&#39;{&quot;spec&quot;:{&quot;defaultRoute&quot;:true}}&#39;</span> <span class="token parameter variable">--type</span><span class="token operator">=</span>merge</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>Login to image registry:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ <span class="token assign-left variable">HOST</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span>oc get route default-route <span class="token parameter variable">-n</span> openshift-image-registry <span class="token parameter variable">--template</span><span class="token operator">=</span><span class="token string">&#39;{{ .spec.host }}&#39;</span><span class="token variable">)</span></span></span>
<span class="line">$ <span class="token function">podman</span> login <span class="token parameter variable">-u</span> kubeadmin <span class="token parameter variable">-p</span> <span class="token variable"><span class="token variable">$(</span>oc <span class="token function">whoami</span> <span class="token parameter variable">-t</span><span class="token variable">)</span></span> --tls-verify<span class="token operator">=</span>false <span class="token variable">$HOST</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="create-the-secret-to-access-the-vgpu-manager-image" tabindex="-1"><a class="header-anchor" href="#create-the-secret-to-access-the-vgpu-manager-image"><span>Create the secret to access the vGPU manager image</span></a></h4><p>create a secret object for storing your registry API key (the mechanism used to authenticate your access to the private container registry).</p><ol><li><p>Navigate to Home &gt; Projects and ensure the nvidia-gpu-operator is selected.</p></li><li><p>In the OpenShift Container Platform web console, click Secrets from the Workloads drop down.</p></li><li><p>Click the Create Drop down.</p></li><li><p>Select Image Pull Secret.</p></li><li><p>Enter the following into each field:</p><p>a. Secret name: private-registry-secret</p><p>b. Authentication type: Image registry credentials</p><p>c. Registry server address: &lt; image-registry.openshift-image-registry.svc:5000 &gt;</p><p>d. Username: kubeadmin</p><p>e. Password: &lt; kubeadm-password &gt;</p><p>f. Email: &lt; YOUR-EMAIL &gt;</p></li><li><p>Click Create.</p><p>A pull secret is created.</p></li></ol><p><img src="`+u+`" alt=""></p><p><strong>FIGURE 10.</strong> Pull secret creation</p><h4 id="building-the-vgpu-manager-image" tabindex="-1"><a class="header-anchor" href="#building-the-vgpu-manager-image"><span>Building the vGPU Manager image</span></a></h4><div class="hint-container tip"><p class="hint-container-title">Note</p><p>Building a vGPU Manager image is only required for NVIDIA vGPU. If you plan to use GPU Passthrough only, skip this section.</p></div><p>Use the following steps to build the vGPU Manager container and push it to a private registry.</p><ol start="4"><li><p>Download the vGPU Software from the NVIDIA Licensing Portal.</p><p>• Login to the NVIDIA Licensing Portal and navigate to the Software Downloads section.</p><p>• The NVIDIA vGPU Software is located on the Driver downloads tab of the Software Downloads page.</p><p>• Click the Download link for the Linux KVM complete vGPU package. Confirm that the Product Version column shows the vGPU version to install. Unzip the bundle to obtain the NVIDIA vGPU Manager for Linux (NVIDIA-Linux-x86_64-{version}-vgpu-kvm.run file)</p></li></ol><p>Use the following steps to clone the driver container repository and build the driver image.</p><ol start="5"><li>Open a terminal and clone the driver container image repository:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ <span class="token function">git</span> clone https://gitlab.com/nvidia/container-images/driver</span>
<span class="line">$ <span class="token builtin class-name">cd</span> driver</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><ol start="6"><li>Change to the vgpu-manager directory for your OS:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ <span class="token builtin class-name">cd</span> vgpu-manager/rhel9</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="7"><li>Copy the NVIDIA vGPU Manager from your extracted zip file:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ <span class="token function">cp</span> NVIDIA-Linux-x86_64-550.90.05-vgpu-kvm.run <span class="token builtin class-name">.</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="8"><li><p>Set the following environment variables:</p><p>• PRIVATE_REGISTRY - Name of the private registry used to store the driver image.</p><p>• VERSION - The NVIDIA vGPU Manager version downloaded from the NVIDIA Software Portal.</p><p>• OS_TAG - This must match the Guest OS version. For RedHat OpenShift, specify rhcos4.x where <em>x</em> is the supported minor OCP version.</p><p>• CUDA_VERSION - CUDA base image version to build the driver image with.</p><p>• $ export PRIVATE_REGISTRY=image-registry.openshift-image-registry.svc:5000/openshift VERSION=550.90.05 OS_TAG=rhcos4.16 CUDA_VERSION=12.4</p></li></ol><div class="hint-container tip"><p class="hint-container-title">Note</p><p>The recommended registry to use is the Integrated OpenShift Container Platform registry.</p></div><ol start="9"><li>Build the NVIDIA vGPU Manager image:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ <span class="token function">docker</span> build <span class="token punctuation">\\</span></span>
<span class="line">    --build-arg <span class="token assign-left variable">DRIVER_VERSION</span><span class="token operator">=</span><span class="token variable">\${VERSION}</span> <span class="token punctuation">\\</span></span>
<span class="line">    --build-arg <span class="token assign-left variable">CUDA_VERSION</span><span class="token operator">=</span><span class="token variable">\${CUDA_VERSION}</span> <span class="token punctuation">\\</span></span>
<span class="line">    <span class="token parameter variable">-t</span> <span class="token variable">\${PRIVATE_REGISTRY}</span>/vgpu-manager:<span class="token variable">\${VERSION}</span>-<span class="token variable">\${OS_TAG}</span> <span class="token builtin class-name">.</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol start="10"><li>Push the NVIDIA vGPU Manager image to your private registry:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ <span class="token function">docker</span> push <span class="token variable">\${PRIVATE_REGISTRY}</span>/vgpu-manager:<span class="token variable">\${VERSION}</span>-<span class="token variable">\${OS_TAG}</span></span>
<span class="line"></span>
<span class="line">$ <span class="token function">docker</span> push image-registry.openshift-image-registry.svc:5000/openshift/vgpu-manager: <span class="token number">550.90</span>.05- rhcos4.16</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="create-the-cluster-policy-for-the-nvidia-gpu-operator" tabindex="-1"><a class="header-anchor" href="#create-the-cluster-policy-for-the-nvidia-gpu-operator"><span>Create the cluster policy for the NVIDIA GPU Operator:</span></a></h4><p>The ClusterPolicy configures the GPU stack, configuring the image names and repository, pod restrictions/credentials and so on.</p><p>Table 12: clusterpolicy configuration for GPU-accelerated containers, GPU accelerated VMs with GPU passthrough and GPU accelerated-VMs with vGPU</p><p><img src="`+m+'" alt=""></p><p>Create the ClusterPolicy:</p><ol><li>In the OpenShift Container Platform web console, from the side menu, select Operators -&gt; Installed Operators, and click NVIDIA GPU Operator.</li><li>Select the ClusterPolicy tab, then click Create ClusterPolicy. The platform assigns the default name gpu-cluster-policy.</li></ol><p><img src="'+v+'" alt=""></p><p><strong>FIGURE 11.</strong> ClusterPolicy creations</p><ol start="3"><li>Modify the clusterpolicy.json file as described in the Table 12:</li></ol><div class="hint-container tip"><p class="hint-container-title">Note</p><p>The vgpuManager options are only required if you want to use the NVIDIA vGPU.</p></div><p><img src="'+b+'" alt=""></p><p>save the changes:</p><p><img src="'+g+`" alt=""></p><p><strong>FIGURE 12.</strong> created Clusterpolicy for NVIDIA GPU&#39;s</p><p>The vGPU Device Manager, deployed by the GPU Operator, automatically creates vGPU devices which can be assigned to KubeVirt VMs. Without additional configuration, the GPU Operator creates a default set of devices on all GPUs.</p><h4 id="verify-the-successful-installation-of-the-nvidia-gpu-operator" tabindex="-1"><a class="header-anchor" href="#verify-the-successful-installation-of-the-nvidia-gpu-operator"><span>Verify the successful installation of the NVIDIA GPU Operator:</span></a></h4><p>Run the following command to view these new pods and daemonsets:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc get pods,daemonset <span class="token parameter variable">-n</span> nvidia-gpu-operator</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="`+y+`" alt=""></p><p>Running a sample GPU Application Run a simple CUDA VectorAdd sample, which adds two vectors together to ensure the GPUs have bootstrapped correctly. Run the following:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ <span class="token function">cat</span> <span class="token operator">&lt;&lt;</span> <span class="token string">EOF<span class="token bash punctuation"> <span class="token operator">|</span> oc create <span class="token parameter variable">-f</span> -</span></span>
<span class="line">apiVersion: v1</span>
<span class="line">kind: Pod</span>
<span class="line">metadata:</span>
<span class="line">  name: cuda-vectoradd</span>
<span class="line">spec:</span>
<span class="line"> restartPolicy: OnFailure</span>
<span class="line"> containers:</span>
<span class="line"> - name: cuda-vectoradd</span>
<span class="line">   image: &quot;nvidia/samples:vectoradd-cuda11.2.1&quot;</span>
<span class="line">   resources:</span>
<span class="line">     limits:</span>
<span class="line">       nvidia.com/gpu: 1</span>
<span class="line">EOF</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Check the logs of the container:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc logs cuda-vectoradd</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="`+f+`" alt=""></p><h5 id="getting-information-about-the-gpu" tabindex="-1"><a class="header-anchor" href="#getting-information-about-the-gpu"><span>Getting information about the GPU</span></a></h5><p>The nvidia-smi shows memory usage, GPU utilization, and the temperature of the GPU. Test the GPU access by running the popular nvidia-smi command within the pod. To view GPU utilization, run nvidia-smi from a pod in the GPU Operator daemonset.</p><ol><li>Change to the nvidia-gpu-operator project:</li><li>Run the following command to view these new pods:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$  oc get pod <span class="token parameter variable">-owide</span> -lopenshift.driver-toolkit<span class="token operator">=</span>true</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="`+k+`" alt=""></p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> nvidia-driver-daemonset-<span class="token operator">&lt;</span>4xx.xxxxxxxxxxxx<span class="token operator">&gt;</span> -- nvidia-smi</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="`+w+`" alt=""></p><p>2 tables are generated. The first table reflects the information about all available GPUs (the example shows 2 GPU). The second table provides details on the processes using the GPUs.</p><h4 id="verify-the-gpu-devices-on-worker-node-after-installation" tabindex="-1"><a class="header-anchor" href="#verify-the-gpu-devices-on-worker-node-after-installation"><span>Verify the GPU devices on worker node after installation:</span></a></h4><p>ssh to the worker node which is configured for vm-vgpu workload, and you can list the NVIDIA vGPU device created with a command like the following example:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ lspci <span class="token parameter variable">-nnk</span> <span class="token parameter variable">-d</span> 10de:</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="`+x+`" alt=""></p><p>ssh to the worker node which is configured for vm-passthrough workload, and you can list the NVIDIA GPU devices with a command like the following example:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ lspci <span class="token parameter variable">-nnk</span> <span class="token parameter variable">-d</span> 10de:</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="`+_+`" alt=""></p><p>ssh to the worker node which is configured for Container workload, and you can list the NVIDIA GPU devices with a command like the following example:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ lspci <span class="token parameter variable">-nnk</span> <span class="token parameter variable">-d</span> 10de:</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="`+I+`" alt=""></p><h5 id="add-gpu-resources-to-the-hyperconverged-custom-resource" tabindex="-1"><a class="header-anchor" href="#add-gpu-resources-to-the-hyperconverged-custom-resource"><span>Add GPU Resources to the HyperConverged Custom Resource:</span></a></h5><p>Update the HyperConverged custom resource so that all GPU and vGPU devices in your cluster are permitted and can be assigned to virtual machines.</p><p>Determine the resource names for the GPU devices:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc get <span class="token function">node</span> <span class="token operator">&lt;</span>gpu-node <span class="token operator">&gt;</span> <span class="token parameter variable">-o</span> json <span class="token operator">|</span> jq <span class="token string">&#39;.status.allocatable | with_entries(select(.key | startswith(&quot;nvidia.com/&quot;))) | with_entries(select(.value != &quot;0&quot;))&#39;</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="`+T+`" alt=""></p><h4 id="determine-the-pci-device-ids-for-the-gpus" tabindex="-1"><a class="header-anchor" href="#determine-the-pci-device-ids-for-the-gpus"><span>Determine the PCI device IDs for the GPUs.</span></a></h4><p>ssh to the node, you can list the NVIDIA GPU devices with a command like the following example:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ lspci <span class="token parameter variable">-nnk</span> <span class="token parameter variable">-d</span> 10de:</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="`+P+`" alt=""></p><p>Modify the HyperConvered custom resource like the following example:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc edit hyperconverged kubevirt-hyperconverged <span class="token parameter variable">-n</span> openshift-cnv</span>
<span class="line"></span>
<span class="line">  permittedHostDevices:</span>
<span class="line">    mediatedDevices:</span>
<span class="line">    - externalResourceProvider: <span class="token boolean">true</span></span>
<span class="line">      mdevNameSelector: NVIDIA_L40S-24Q</span>
<span class="line">      resourceName: nvidia.com/NVIDIA_L40S-24Q</span>
<span class="line">    pciHostDevices:</span>
<span class="line">    - externalResourceProvider: <span class="token boolean">true</span></span>
<span class="line">      pciDeviceSelector: 10DE:26b9</span>
<span class="line">      resourceName: nvidia.com/AD102GL_L40S</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><img src="`+O+`" alt=""></p><h5 id="creating-a-virtual-machine-with-gpu" tabindex="-1"><a class="header-anchor" href="#creating-a-virtual-machine-with-gpu"><span>Creating a virtual machine with GPU</span></a></h5><p>Assign GPU devices, either passthrough or vGPU, to virtual machines.</p><ul><li><strong>Prerequisites</strong></li></ul><p>The GPU devices are configured in the HyperConverged custom resource (CR).</p><ul><li><strong>Procedure</strong></li></ul><p>Assign the GPU devices to a virtual machine (VM) by editing the spec.domain.devices.gpus field of the VirtualMachine manifest:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">apiVersion: kubevirt.io/v1</span>
<span class="line">kind: VirtualMachine</span>
<span class="line"><span class="token punctuation">..</span><span class="token punctuation">..</span></span>
<span class="line">spec:</span>
<span class="line">  domain:</span>
<span class="line">    devices:</span>
<span class="line">      gpus:</span>
<span class="line">      - deviceName: nvidia.com/NVIDIA_L40S-24Q</span>
<span class="line">        name: gpu1</span>
<span class="line">….</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>deviceName:</strong> the resource name associated with the GPU. <strong>name:</strong> name to identify the device on the VM.</p><h4 id="vm-with-vm-vgpu-device-attached" tabindex="-1"><a class="header-anchor" href="#vm-with-vm-vgpu-device-attached"><span>VM with vm-vGPU device attached:</span></a></h4><p><img src="`+R+'" alt=""></p><p><strong>FIGURE 13.</strong> Virtual Machine with vGPU</p><h4 id="vm-with-vm-passthrough-device-attached" tabindex="-1"><a class="header-anchor" href="#vm-with-vm-passthrough-device-attached"><span>VM with vm-passthrough device attached:</span></a></h4><p><img src="'+S+`" alt=""></p><p><strong>FIGURE 14.</strong> Virtual Machine with vm-passthrough</p><h2 id="deploying-sample-application-on-rhocp-4-16-using-ephemeral-storage" tabindex="-1"><a class="header-anchor" href="#deploying-sample-application-on-rhocp-4-16-using-ephemeral-storage"><span>Deploying sample application on RHOCP 4.16 using Ephemeral storage</span></a></h2><p>NGINX is an open-source software for web serving, reverse proxying,caching, load balancing, media streaming, and so on. This section describes how to deploy a sample NGINX application on RHOCP 4.16 using Ephemeral storage.</p><p><strong>Prerequisites:</strong></p><ul><li>RHOCP must be up and running.</li></ul><p>To deploy a sample application on RHOCP 4.16 using Ephemeral storage:</p><ol><li>Create a new project with namespace as &quot;my-nginx-example&quot;.</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc new-project my-nginx-example  </span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="2"><li>Deploy a new application.</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc new-app httpd-example <span class="token parameter variable">--name</span><span class="token operator">=</span>my-nginx-example <span class="token parameter variable">--param</span><span class="token operator">=</span>NAME<span class="token operator">=</span>my-nginx-example </span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="3"><li>Validate the created service and route for the application.</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc status </span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="4"><li>Retrieve the service details of the application.</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc get svc my-nginx-example </span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="5"><li>Retrieve the route created for the application.</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line">$ oc get route my-nginx-example </span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="6"><li>Use the route in your browser to access the application.</li></ol><p><img src="`+C+'" alt=""></p><p><strong>FIGURE 15.</strong> NGINX application Web Console login screen</p>',526)]))}const A=s(E,[["render",H],["__file","Solution-deployment-flow.html.vue"]]),N=JSON.parse('{"path":"/Solution-Deployment/Solution-deployment-flow.html","title":"Solution Deployment Workflow","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"Preparing the execution environment for RHOCP worker node","slug":"preparing-the-execution-environment-for-rhocp-worker-node","link":"#preparing-the-execution-environment-for-rhocp-worker-node","children":[]},{"level":2,"title":"Deploying RHOCP cluster using Ansible playbooks","slug":"deploying-rhocp-cluster-using-ansible-playbooks","link":"#deploying-rhocp-cluster-using-ansible-playbooks","children":[]},{"level":2,"title":"Adding RHEL 9.4 worker nodes to RHOCP cluster using Ansible playbooks","slug":"adding-rhel-9-4-worker-nodes-to-rhocp-cluster-using-ansible-playbooks","link":"#adding-rhel-9-4-worker-nodes-to-rhocp-cluster-using-ansible-playbooks","children":[]},{"level":2,"title":"Adding RHCOS worker nodes to RHOCP cluster using Ansible playbooks","slug":"adding-rhcos-worker-nodes-to-rhocp-cluster-using-ansible-playbooks","link":"#adding-rhcos-worker-nodes-to-rhocp-cluster-using-ansible-playbooks","children":[]},{"level":2,"title":"Adding BareMetal CoreOS worker nodes to RHOCP cluster using Ansible playbooks","slug":"adding-baremetal-coreos-worker-nodes-to-rhocp-cluster-using-ansible-playbooks","link":"#adding-baremetal-coreos-worker-nodes-to-rhocp-cluster-using-ansible-playbooks","children":[]},{"level":2,"title":"Adding COREOS GPU worker nodes to RHOCP cluster","slug":"adding-coreos-gpu-worker-nodes-to-rhocp-cluster","link":"#adding-coreos-gpu-worker-nodes-to-rhocp-cluster","children":[]},{"level":2,"title":"Deploying sample application on RHOCP 4.16 using Ephemeral storage","slug":"deploying-sample-application-on-rhocp-4-16-using-ephemeral-storage","link":"#deploying-sample-application-on-rhocp-4-16-using-ephemeral-storage","children":[]}],"git":{},"filePathRelative":"Solution-Deployment/Solution-deployment-flow.md"}');export{A as comp,N as data};
