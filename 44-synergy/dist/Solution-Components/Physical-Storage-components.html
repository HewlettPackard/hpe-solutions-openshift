<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Solution components | OpenShift Container Platform 4.4 on Synergy</title>
    <meta name="generator" content="VuePress 1.8.0">
    
    <meta name="description" content="Hewlett Packard Enterprise">
    
    <link rel="preload" href="/hpe-solutions-openshift/44-synergy/assets/css/0.styles.848b002a.css" as="style"><link rel="preload" href="/hpe-solutions-openshift/44-synergy/assets/js/app.25b016e2.js" as="script"><link rel="preload" href="/hpe-solutions-openshift/44-synergy/assets/js/2.4639eab6.js" as="script"><link rel="preload" href="/hpe-solutions-openshift/44-synergy/assets/js/5.39a73899.js" as="script"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/10.f90a53b1.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/11.0d45f317.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/12.e8a20512.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/13.1fedfc0f.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/14.6429256e.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/15.1a190dba.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/16.cf38ac5c.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/17.87256181.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/18.e95f5833.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/19.93d66256.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/20.6daf5fdb.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/21.29f23ca8.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/22.a073cf57.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/23.523cb9c6.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/24.a24a3dc4.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/25.2ba844c2.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/26.6be710ed.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/27.be5b9d1a.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/28.75aa7f8e.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/29.beb1893f.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/3.f31d8e9a.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/30.06a297f9.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/31.c1790d7d.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/32.c53f7492.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/33.3d562199.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/4.9e5da2a7.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/6.fb4d3fa6.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/7.203554ca.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/8.6465cc95.js"><link rel="prefetch" href="/hpe-solutions-openshift/44-synergy/assets/js/9.c38254a6.js">
    <link rel="stylesheet" href="/hpe-solutions-openshift/44-synergy/assets/css/0.styles.848b002a.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/hpe-solutions-openshift/44-synergy/" class="home-link router-link-active"><!----> <span class="site-name">OpenShift Container Platform 4.4 on Synergy</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/hpe-solutions-openshift/44-synergy/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="http://www.hpe.com/info/ra" target="_blank" rel="noopener noreferrer" class="nav-link external">
  RA Library
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/hpe-solutions-openshift/44-synergy/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="http://www.hpe.com/info/ra" target="_blank" rel="noopener noreferrer" class="nav-link external">
  RA Library
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Red Hat OpenShift Container Platform 4 on HPE Synergy</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Solution Overview</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>Solution Components</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/hpe-solutions-openshift/44-synergy/Solution-Components/Physical-Storage-components.html" aria-current="page" class="active sidebar-link">Solution components</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/44-synergy/Solution-Components/Physical-Storage-components.html#hardware" class="sidebar-link">Hardware</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/44-synergy/Solution-Components/Physical-Storage-components.html#software" class="sidebar-link">Software</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/44-synergy/Solution-Components/Physical-Storage-components.html#services" class="sidebar-link">Services</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/44-synergy/Solution-Components/Physical-Storage-components.html#storage-system" class="sidebar-link">Storage system</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/44-synergy/Solution-Components/Physical-Storage-components.html#hpe-3par-iscsi" class="sidebar-link">HPE 3PAR iSCSI</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/44-synergy/Solution-Components/Physical-Storage-components.html#hpe-nimble-iscsi" class="sidebar-link">HPE Nimble iSCSI</a></li></ul></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Solution Deployment</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Additional Features and Functionality</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Resosurces and Additional Links</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="solution-components"><a href="#solution-components" class="header-anchor">#</a> Solution components</h1> <h2 id="hardware"><a href="#hardware" class="header-anchor">#</a> Hardware</h2> <p>Figure 4 shows the physical configuration of the racks along with storage devices used in this solution. It also depicts the hardware layout in the test environment. However, this is subject to change based on the customer's requirements.</p> <p><img src="/hpe-solutions-openshift/44-synergy/assets/img/figure5.9edec646.png" alt=""></p> <p><strong>Figure 4.</strong> Hardware layout within the rack along with HPE Storage systems</p> <p>The configuration outlined in this document is based on the design guidance of an HPE Converged Architecture 750 Foundation model which offers an improved time to deployment and tested firmware recipe. The recipe can be retrieved at <a href="https://support.hpe.com/hpesc/public/docDisplay?docId=a00098137en_us" target="_blank" rel="noopener noreferrer">https://support.hpe.com/hpesc/public/docDisplay?docId=a00098137en_us<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>. It is strongly recommended that the installation user utilizes the latest available matrix. Hewlett Packard Enterprise has tested this solution with the latest firmware recipe available as of March 2020, including HPE OneView for Synergy 5.0. The installation user has the flexibility to customize the HPE components throughout this stack in accordance with the unique IT and workload requirements or to build the solution with individual components rather than using HPE CS750.</p> <p>Table 1 highlights the individual components and their quantities as deployed within the solution.</p> <p><strong>Table 1.</strong> Components utilized in the creation of this solution.</p> <table><thead><tr><th>Component</th> <th>Quantity</th> <th>Description</th></tr></thead> <tbody><tr><td>HPE Synergy 12000 Frame</td> <td>3</td> <td>Three (3) HPE Synergy 12000 Frames house the infrastructure used for the solution</td></tr> <tr><td>HPE Synergy Composer</td> <td>2</td> <td>Two (2) HPE Synergy Composers for core configuration and lifecycle management of Synergy components</td></tr> <tr><td>HPE Virtual Connect 40Gb SE F8 Module</td> <td>2</td> <td>A total of two (2) HPE Virtual Connect 40Gb SE F8 Modules provide network connectivity into and out of the frames</td></tr> <tr><td>HPE Synergy 12G SAS Connection Module</td> <td>6</td> <td>Six (6) HPE 12G SAS Connection Modules (two (2) per frame)</td></tr> <tr><td>HPE Synergy 480 Gen10 Compute Module</td> <td>6</td> <td>Three (3) bare metal master nodes and three (3) bare metal for worker nodes</td></tr> <tr><td>HPE Synergy D3940 Storage</td> <td>3</td> <td>Three (3) HPE Synergy D3940 12Gb SAS CTO Drive Enclosure with 40 SFF (2.5in) Drive Bays</td></tr> <tr><td>HPE Nimble Storage</td> <td>1</td> <td>One (1) array for persistent volume</td></tr> <tr><td>HPE 3PAR StoreServ</td> <td>1</td> <td>One (1) HPE 3PAR array</td></tr> <tr><td>HPE FlexFabric 2-Slot Switch</td> <td>2</td> <td>Each switch contains one (1) each of the HPE 5945 modules listed as follows</td></tr> <tr><td>HPE 5945 24p SFP+ and 2p QSFP+ Module</td> <td>2</td> <td>One module per HPE FlexFabric 2-Slot Switch</td></tr> <tr><td>HPE 5945 8p QSFP+ Module</td> <td>2</td> <td>One module per HPE FlexFabric 2-Slot Switch</td></tr></tbody></table> <div class="custom-block tip"><p class="custom-block-title">Note</p> <p>The HPE Storage systems mentioned in Table 1 is for representational purpose. Use the required amount of storage system based on the deployment requirements.</p></div> <h2 id="software"><a href="#software" class="header-anchor">#</a> Software</h2> <p>Table 2 describes the versions of important software utilized in the creation of this solution. The installation user should ensure that they download or have access to this software. Ensure that the appropriate subscription and licensing are in place to use within the planned time frame.</p> <p><strong>Table 2.</strong> Major software versions used in the creation of this solution</p> <table><thead><tr><th>Component</th> <th>Version</th></tr></thead> <tbody><tr><td>Red Hat Enterprise Linux CoreOS (RHCOS)</td> <td>4.4</td></tr> <tr><td>Red Hat OpenShift Container Platform</td> <td>4</td></tr> <tr><td>HPE Nimble OS</td> <td>5.0.8</td></tr> <tr><td>HPE 3PAR OS</td> <td>3.3.1</td></tr></tbody></table> <div class="custom-block tip"><p class="custom-block-title">Note</p> <p>The latest sub-version of each component listed in Table 2 should be installed.</p></div> <p>When utilizing virtualized nodes, the software version used in the creation of this solution are shown in Table 3.</p> <p><strong>Table 3.</strong> Software versions used with virtualized implementations</p> <table><thead><tr><th>Component</th> <th>Version</th></tr></thead> <tbody><tr><td>VMware vSphere</td> <td>ESXi 6.7 U2 (Build: 13981272)</td></tr> <tr><td>VMware vCenter Server Appliance</td> <td>6.7 Update 2c (Build: 14070457)</td></tr></tbody></table> <p>Table 4 shows the software installed on the installer machine.</p> <p><strong>Table 4.</strong> Software installed on the installer machine</p> <table><thead><tr><th>Component</th> <th>Version</th></tr></thead> <tbody><tr><td>Ansible</td> <td>2.9</td></tr> <tr><td>Python</td> <td>3.6</td></tr> <tr><td>Java</td> <td>1.8</td></tr> <tr><td>Openshift Container Platform packages</td> <td>4.4</td></tr></tbody></table> <h2 id="services"><a href="#services" class="header-anchor">#</a> Services</h2> <p>This document is built with assumptions about services and network ports available within the implementation environment. This section discusses those assumptions.</p> <p>Table 5 disseminates the services required in this solution and provides a high-level explanation of their function.</p> <p><strong>Table 5.</strong> Services used in the creation of this solution.</p> <table><thead><tr><th>Service</th> <th>Description/Notes</th></tr></thead> <tbody><tr><td>DNS</td> <td>Provides name resolution on management and data center networks</td></tr> <tr><td>DHCP</td> <td>Provides IP address leases on PXE, management and usually for data center networks</td></tr> <tr><td>NTP</td> <td>Ensures consistent time across the solution stack</td></tr> <tr><td>PXE</td> <td>Enables booting of operating systems</td></tr></tbody></table> <h3 id="dns"><a href="#dns" class="header-anchor">#</a> DNS</h3> <p>Domain Name Services must be in place for the management and data center networks. Ensure that both forward and reverse lookups are working for all hosts.</p> <h3 id="dhcp"><a href="#dhcp" class="header-anchor">#</a> DHCP</h3> <p>DHCP should be present and able to provide IP address leases on the PXE, management, and data center networks.</p> <h3 id="ntp"><a href="#ntp" class="header-anchor">#</a> NTP</h3> <p>A Network Time Protocol (NTP) server should be available to hosts within the solution environment.</p> <h3 id="pxe"><a href="#pxe" class="header-anchor">#</a> PXE</h3> <p>Because all nodes in this solution are booted using PXE, a properly configured PXE server is essential.</p> <h3 id="network-port"><a href="#network-port" class="header-anchor">#</a> Network port</h3> <p>The port information listed in Table 6 allows cluster components to communicate with each other. This information can be retrieved from bootstrap, master, and worker nodes by running the following command.</p> <div class="language-bash extra-class"><pre class="language-bash"><code> <span class="token function">netstat</span> –tupln
</code></pre></div><p>Table 6 shows list of network ports used by the services under OpenShift Container Platform 4.</p> <p><strong>Table 6.</strong> List of network ports</p> <table><thead><tr><th>Protocol</th> <th style="text-align:center;">Port Number/Range</th> <th>Service Type</th> <th>Other details</th></tr></thead> <tbody><tr><td>TCP</td> <td style="text-align:center;">80</td> <td>HTTP Traffic</td> <td>The machines that run the Ingress router pods, compute, or worker by default.</td></tr> <tr><td></td> <td style="text-align:center;">443</td> <td>HTTPs traffic</td> <td></td></tr> <tr><td></td> <td style="text-align:center;">2379-2380</td> <td>etcd server, peer and metrics ports</td> <td></td></tr> <tr><td></td> <td style="text-align:center;">6443</td> <td>Kubernetes API</td> <td>The Bootstrap machine and masters.</td></tr> <tr><td></td> <td style="text-align:center;">9000-9999</td> <td>Host level services, including the node exporter on ports 9100-9101 and the Cluster Version Operator on port 9099</td> <td></td></tr> <tr><td></td> <td style="text-align:center;">10249-10259</td> <td>The default ports that Kubernetes reserves</td> <td></td></tr> <tr><td></td> <td style="text-align:center;">10256</td> <td>openshift-sdn</td> <td></td></tr> <tr><td></td> <td style="text-align:center;">22623</td> <td>Machine Config Server</td> <td>The Bootstrap machine and masters</td></tr> <tr><td>UDP</td> <td style="text-align:center;">4789</td> <td>VXLAN and GENEVE</td> <td></td></tr> <tr><td></td> <td style="text-align:center;">6081</td> <td>VXLAN and GENEVE</td> <td></td></tr> <tr><td></td> <td style="text-align:center;">9000-9999</td> <td>Host level services, including the node exporter on ports 9100-9101</td> <td></td></tr> <tr><td></td> <td style="text-align:center;">30000-32767</td> <td>Kubernetes NodePort</td> <td></td></tr></tbody></table> <p>For more information on the network port requirements for Red Hat OpenShift 4, see the documentation from Red Hat at <a href="https://docs.openshift.com/container-platform/4.4/installing/installing_bare_metal/installing-bare-metal.html#installation-network-user-infra_installing-bare-metal" target="_blank" rel="noopener noreferrer">https://docs.openshift.com/container-platform/4.4/installing/installing_bare_metal/installing-bare-metal.html#installation-network-user-infra_installing-bare-metal<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <h2 id="storage-system"><a href="#storage-system" class="header-anchor">#</a> Storage system</h2> <p>Use the following steps to configure the storage system.</p> <ol><li>From the SP, select <strong>Launch</strong> adjacent to the <strong>Storage System Setup Wizard</strong> and accept the EULA. Click <strong>Next</strong>.</li> <li>Confirm the serial number of your array and click <strong>Next</strong>.</li> <li>At the <em>Configure Networking</em> screen you will need to enter a name and network information for the array. Click <strong>Next</strong>.</li> <li>Select <strong>Copy time options from the Service Processor</strong> and click <strong>Next</strong>.</li> <li>Create a password for the 3paradm user and click <strong>Next</strong>.</li> <li>Verify the configuration information and click <strong>Next</strong>. This will initialize and test the array and then add it to the SP. This process can take an extended amount of time. After all tests have passed, click <strong>Finish</strong>.</li></ol> <div class="custom-block tip"><p class="custom-block-title">Note</p> <p>Ensure that information such as user credentials, network access details, and serial numbers referenced are securely recorded for current and future reference.</p></div> <h3 id="initializing-the-hpe-3par-storeserv-8440-storage"><a href="#initializing-the-hpe-3par-storeserv-8440-storage" class="header-anchor">#</a> Initializing the HPE 3PAR StoreServ 8440 Storage</h3> <p>This section assumes that the HPE 3PAR StoreServ 8440 Storage was ordered with a physical service processor or with a Virtual Service Processor. VM is installed and functioning within the environment and is available on the same network as HPE Synergy Composer. It also assumes that a DHCP server is present on the network. The user should have the serial number of the storage that is being installed.</p> <h3 id="service-processor-networking"><a href="#service-processor-networking" class="header-anchor">#</a> Service Processor networking</h3> <p>To configure the Service Processor networking, use the following steps:</p> <ol><li>Login to a physical Service Processor (SP) or access the console of a Virtual Service Processor via the virtual console.</li> <li>Log on as <strong>setupusr</strong> without a password.</li> <li>When you configure the network, type <strong>Y</strong> and press <strong>Enter</strong>.</li> <li>Confirm the network parameters that were handed to the Service Processor by your DHCP server and if correct, type Y and then press <strong>Enter</strong>. Ensure you note the IP address.</li> <li>When prompted, press <strong>Enter</strong> to exit. You will now configure the Service Processor. Connect to the SP using the address <strong>https://&lt;ip_address&gt;/sp/SpSetupWizard.html</strong>, and log on with the user setupusr and no password.</li> <li>At the Welcome screen click <strong>Next</strong>.</li> <li>Enter the serial number of your HPE 3PAR StoreServ storage and select <strong>Generate SP ID</strong>. Click <strong>Next</strong> when done.</li> <li>On the Configure Service Processor Networking screen give the SP a hostname, check <strong>Enable DNS Support</strong> and enter the domain name and DNS server IP. Click <strong>Next</strong>.</li> <li>Enter any proxy information for <em>Remote Support</em> and click <strong>Next</strong>.</li> <li>Enter the appropriate information in the <em>System Support Information</em> screen and click <strong>Next</strong>.</li> <li>At the <em>Time and Region</em> screen, select <strong>Automatic</strong> and enter the <strong>IP address</strong> of your NTP server. Choose the appropriate Time Zone and click <strong>Next</strong>.</li> <li>When prompted, change the password and click <strong>Next</strong>.</li> <li>Click <strong>Next</strong> at the summary screen after validating all information.</li> <li>Remote connectivity will be tested. In the event of a failed test, ensure that the SP can speak outbound via HTTPS and that the proxy information entered is correct.</li></ol> <p>After completion, configure the Web services API and Common Information Module (CIM).</p> <ol><li>Log onto the array via SSH as the 3paradm user.</li> <li>Run the following command to generate a self-signed certificate.</li></ol> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token comment"># createcert unified-server -selfsigned -keysize 2048 -days 1095 -CN</span>
<span class="token operator">&lt;</span>fqdn of 3PAR<span class="token operator">&gt;</span> -SAN <span class="token string">&quot;DNS:&lt;fqdn of 3PAR&gt;, IP:&lt;management IP of
3PAR&gt;&quot;</span>
</code></pre></div><ol start="3"><li>Answer <strong>Yes</strong> when prompted to continue creating the certificate.</li> <li>Issue the following commands to start the WSAPI and CIM services.</li></ol> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token comment"># setcim -https enable</span>

<span class="token comment"># startcim</span>

<span class="token comment"># startwsapi</span>
</code></pre></div><ol start="5"><li>You can verify that the services are enabled and running by typing the following commands.</li></ol> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token comment"># showcim</span>

<span class="token comment"># showwsapi</span>
</code></pre></div><h3 id="san-switch-configuration"><a href="#san-switch-configuration" class="header-anchor">#</a> SAN Switch configuration</h3> <p>The following steps should be repeated on each SAN switch:</p> <ol><li>Connect via the serial port to the first SAN switch and open a terminal session using <strong>9600</strong>, <strong>8</strong>, <strong>N</strong> and <strong>1</strong> with <strong>no flow control</strong>.</li> <li>Logon as <strong>admin</strong> with the default password of ' <strong>password</strong>'. Change the passwords for admin and user as prompted.</li> <li>At the command line, run the following commands to configure IP addressing for the switch.</li></ol> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token comment"># switchname &lt;hostname of switch&gt;</span>
<span class="token comment"># ipaddrset</span>
</code></pre></div><ol start="4"><li>When prompted, turn off DHCP and configure date and time information for the switch.</li></ol> <div class="language- extra-class"><pre class="language-text"><code># tstimezone ##
# date MMddhhmmyy
</code></pre></div><p>where ## is the difference in the local time zone from GMT and  MM is the 2-digit month, day is the 2-digit date, hh is the 2-digit hour (24-hour format), mm is the 2-digit minute and yy is the year.
5.	Configure the NTP Server with the following command.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token comment"># tsclockserver &lt;ip address&gt;</span>
</code></pre></div><h3 id="configure-the-fabric-and-licensing"><a href="#configure-the-fabric-and-licensing" class="header-anchor">#</a> Configure the fabric and licensing</h3> <p>The user will need to configure fabric and licensing in the environment.</p> <ol><li>Run the following command to configure the fabric name.</li></ol> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token comment"># fabricname --set &lt;san_a_fabricname&gt;</span>
</code></pre></div><ol start="2"><li>Add any licenses by running the following command.</li></ol> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token comment"># licenseadd</span>
</code></pre></div><ol start="3"><li>Verify all ports that will be used have been enabled. If you have not enabled, then run the following command.</li></ol> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token comment"># licenseport -reserve</span>
</code></pre></div><ol start="4"><li>Configure the Domain ID by typing the following.</li></ol> <div class="language- extra-class"><pre class="language-text"><code># switchdisable

# configure
</code></pre></div><ol start="5"><li>When prompted, enter <strong>yes</strong> for Fabric parameters and assign a Domain ID. After entering, press <strong>Control-D</strong> to exit.</li></ol> <div class="language- extra-class"><pre class="language-text"><code># switchenable
</code></pre></div><ol start="6"><li>Reboot the switch to apply changes and repeat these steps on the second switch.</li></ol> <h3 id="isl-trunk-the-fibre-channel-switches"><a href="#isl-trunk-the-fibre-channel-switches" class="header-anchor">#</a> ISL Trunk the Fibre Channel switches</h3> <ol><li>Login to each switch via SSH and use the <strong>licenseshow</strong> command to verify that the trunk license has been applied.</li> <li>On switch 1 and switch 2, run the <strong>portcfgpersistentdisable</strong> to disable the ports that will be used for ISL trunks.</li> <li>Run the <strong>portcfgislmode</strong> command and set the ports to mode 1 to make them ready for ISL traffic.</li> <li>Enable the ISL trunk ports on the switches by running the <strong>portcfgpersistentenable</strong> command.</li> <li>Run the <strong>trunkshow</strong> and <strong>fabricshow</strong> commands to verify that the trunk ports are configured and that both switches display the correct partner switch.</li></ol> <h4 id="configure-an-active-zone-set"><a href="#configure-an-active-zone-set" class="header-anchor">#</a> Configure an active zone set</h4> <p>To allow HPE OneView for Synergy to manage the SAN fabric, the user must create an active zone set.</p> <ol><li>To create a zone using the WWPN of node 0, slot 0, port 1 on the HPE 3PAR StoreServ 8440, obtain the WWPN by typing <strong>switchshow</strong> and check the information on the switch port that 3PAR port N0:S0:P1 is connected.</li> <li>Run the following commands to create the zone for this port.</li></ol> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token comment"># zonecreate &lt;3PAR_1&gt;_N0S0P1, &quot;wwpn&quot;</span>

<span class="token comment"># cfgcreate solution_cfg, &quot;&lt;3PAR_1&gt;_N0S0P1&quot;</span>

<span class="token comment"># cfgenable solution_cfg</span>
</code></pre></div><ol start="3"><li>Follow the prompts to enable the solution configuration.</li> <li>Repeat these steps for the remaining SAN fabrics using the WWPN of each 3PAR node, slot, and port.</li></ol> <h4 id="install-and-configure-the-hpe-3par-ssmc"><a href="#install-and-configure-the-hpe-3par-ssmc" class="header-anchor">#</a> Install and configure the HPE 3PAR SSMC</h4> <ol><li>From within the Windows management station, install the HPE 3PAR StoreServ Management Console by copying the media to the station and running the <em>HPESSMC-</em>-win64.exe* user. Follow the onscreen prompts.</li> <li>Use a web browser to connect to <strong>https://&lt;mgmt_vm_ip&gt;:8443</strong>.</li> <li>Select <strong>Set credential</strong> and enter the <strong>username</strong> and <strong>password</strong>. Select <strong>Set</strong>.</li> <li>Install the HPE 3PAR Admin Tools by copying the media to the management server, mounting it and executing <em>cli\windows\setup.exe</em>. Follow the prompts to install. The user will need to add each array in the solution to the SSMC Administrator Console.</li> <li>Logon to the console and select <strong>Actions</strong> and then <strong>Add</strong>.</li> <li>Under System DNS names or IP addresses, <strong>enter the IP or name</strong> of the 3PAR array. The username will be <strong>3paradm</strong> and the password is the one the user created earlier. Select <strong>Add</strong>.</li> <li>After the array appears in the <strong>NotConnected</strong> state, select it and select <strong>Actions</strong> and then <strong>AcceptCertificate</strong>. Select <strong>Accept and cache</strong>. After a moment, the array will appear in the <strong>Connected</strong> state.</li> <li>Repeat these steps for any additional arrays within the solution.</li></ol> <h4 id="create-common-provisioning-groups-cpgs"><a href="#create-common-provisioning-groups-cpgs" class="header-anchor">#</a> Create Common Provisioning Groups (CPGs)</h4> <p>CPGs are required within this solution. By default, HPE 3PAR StoreServ arrays create a default set of CPGs.</p> <ol><li>Logon to the array as the 3PAR admin user using the SSMC.</li> <li>Select <strong>3PAR StoreServ -&gt; Show all -&gt; Common Provisioning Groups</strong>. Select <strong>Create CPG</strong>. Provide a descriptive name and use RAID6. The remaining default parameters are acceptable for this solution.</li> <li>Create a second CPG for snapshots.</li></ol> <h4 id="configure-hpe-oneview-for-synergy-to-manage-hpe-3par-storeserv-resources"><a href="#configure-hpe-oneview-for-synergy-to-manage-hpe-3par-storeserv-resources" class="header-anchor">#</a> Configure HPE OneView for Synergy to manage HPE 3PAR StoreServ resources</h4> <h5 id="add-a-san-manager-to-hpe-oneview-for-synergy"><a href="#add-a-san-manager-to-hpe-oneview-for-synergy" class="header-anchor">#</a> Add a SAN Manager to HPE OneView for Synergy</h5> <ol><li><p>From the HPE OneView for Synergy interface, select <strong>OneView</strong> and then <strong>SAN manager</strong>.</p></li> <li><p>Select <strong>Add SAN manager</strong> and use the following values when prompted. Select <strong>Add</strong> when the values have been filled in.</p> <p>a. SAN manager type: <strong>Brocade Network Advisor</strong></p> <p>b. IP address of hostname: <strong>&lt;mgmt._vm_fqdn&gt;</strong></p> <p>c. Port: 5989</p> <p>d. User name: Administrator</p> <p>e. Password: &lt;bna_administrator_password&gt;&gt;</p></li> <li><p>Select <strong>OneView</strong> and then select <strong>SANs</strong>.</p></li> <li><p>Select the <strong>SAN switch hostname</strong>, hover over <strong>Zoning Policy</strong> and select Edit. Click OK when done. For zoning parameters, use the following values.</p> <p>a. Automatic zoning: <strong>Yes</strong></p> <p>b. Zone layout: <strong>Single initiator / single storage system</strong></p> <p>c. Zone name format: <strong>server profiles _ server profile connection _ storage system _ storage system port group</strong></p> <p>d. Check <strong>Update zone names as referenced resources are renamed</strong></p> <p>e. Create aliases: <strong>Yes</strong></p> <p>f. Manage pre-existing aliases: <strong>No</strong></p> <p>g. Initiator alias format: <strong>Initiator _ server profile _ server profile connection</strong></p> <p>h. Check <strong>Update initiator</strong> aliases as referenced resources are renamed</p> <p>i. Target alias format: <strong>Target _ storage system _ storage</strong> system port</p> <p>j. Check <strong>Update target aliases and target group aliases as referenced resources are renamed</strong></p> <p>k. Create target group aliases: <strong>Yes</strong></p> <p>l. Target group alias format: <strong>TargetGroup _ storage system _ storage system port group</strong></p></li> <li><p>Repeat the steps for each SAN in OneView.</p></li></ol> <h5 id="associate-san-networks-with-fabrics"><a href="#associate-san-networks-with-fabrics" class="header-anchor">#</a> Associate SAN networks with fabrics</h5> <ol><li>Select <strong>OneView</strong> and then <strong>Networks</strong>.</li> <li>Select <strong>SAN_A</strong> and then choose <strong>Actions -&gt; Edit</strong>.</li> <li>In the Associated SAN field select the hostname of <strong>SAN switch 1</strong> and click <strong>OK</strong>.</li> <li>Repeat these steps for SAN_B and associate it with the <strong>hostname</strong> of the second switch.</li> <li>Verify that the Expected Network is set to the correct SAN network for each port on the HPE 3PAR StoreServ storage. In the event of an error, check the wiring and previously entered settings and correct them.</li> <li>Enter the Port Group name and assign every pair of peer persistence ports to the same Port Group.</li></ol> <h5 id="import-and-configure-the-hpe-3par-storeserv-storage"><a href="#import-and-configure-the-hpe-3par-storeserv-storage" class="header-anchor">#</a> Import and configure the HPE 3PAR StoreServ Storage</h5> <ol><li>Select <strong>OneView</strong> and then choose <strong>Storage Systems</strong>.</li> <li>Choose <strong>Add Storage Systems</strong> and enter the information for your array. Select <strong>Connect</strong> and ensure the storage shows up correctly.</li> <li>If needed, select a domain under the Storage domain dropdown.</li> <li>For Storage Pools, select the <strong>Manage</strong> checkbox for all of your previously created CPGs. Select <strong>Add</strong>.</li></ol> <h2 id="hpe-3par-iscsi"><a href="#hpe-3par-iscsi" class="header-anchor">#</a> HPE 3PAR iSCSI</h2> <h3 id="configure-the-management-server"><a href="#configure-the-management-server" class="header-anchor">#</a> Configure the management server</h3> <p>This section assumes that a physical or virtual management server running Microsoft Windows Server 2012 R2 is available and able to communicate on the same network as the HPE 3PAR StoreServ storage. If it is not, the user should create this management VM with the following:</p> <ul><li>Microsoft Windows Server 2012 R2</li> <li>2 vCPU</li> <li>8GB RAM</li> <li>1x 100GB HDD for OS and applications</li> <li>1x 200GB HDD for Media</li> <li>One (1) Network Interface connected to the management network where the storage resides</li></ul> <p>The management VM should have Microsoft IIS configured with the web server role and WebDAV publishing with basic authentication enabled. After the role is installed, perform the following steps.</p> <ol><li>In Server Manager select Tools and then Internet Information Services (IIS) Manager.</li> <li>Select the server name and double-select MIME Types.</li> <li>Select Add and enter the filename extension .vib as type application/octet stream.</li> <li>Click OK.</li> <li>Repeat this process but substitute the filename extension .iso for .vib.</li> <li>Close the IIS Manager window.</li></ol> <p>Next, the user will need to create a repository to house the Service Pack for ProLiant (SPP) and HPE Synergy Release Set hotfix update files associated with the HPE CA750 recipe.</p> <ol><li><p>From File Explorer navigate to the second HDD and create a folder named Media</p></li> <li><p>Within the media folder, create a folder named SPP.</p></li> <li><p>Copy the SPP and HPE Synergy release set files to this folder.</p></li> <li><p>Create a folder under SPP and name the folder Hotfixes. Copy the SPP and/or HPE Synergy release set hotfix update files to this folder.</p></li> <li><p>In Server Manager, relaunch Internet Information Services manager as in Step 1.</p></li> <li><p>Expand the hostname and then expand Sites.</p></li> <li><p>Right-click the default web site and select add virtual directory. Enter Media in the Alias field and select the media folder on the second drive for the physical path. Click OK twice when done.</p></li> <li><p>From within IIS Manager ensure that the folders exist under default web site.</p></li> <li><p>Select the Media folder and then double-select directory browsing and ensure it is Enabled.</p></li> <li><p>Select default web site and double-click theWebDAV Authoring Rules icon. Select Enable WebDAV in the Actions Pane and then select Add Authoring Rule.</p></li> <li><p>Select All content and All users radio button and then check the Read box under permissions. Click OK to commit.</p></li> <li><p>WebDAV setting in the Actions pane and in the Property Behavior section, ensure that the Allow Anonymous Property Queries and Allow Property Queries of Infinite Depth are both set to True. Select Apply.</p></li> <li><p>Select the Media directory in the left pane and in the right pane, double-click the HTTP Response Headers icon.</p></li> <li><p>Select Add in the Actions pane and in the 'name' field enter MaxRepoSize. In the value field, enter the size of the drive that the Media folder was created on. In the case of this document you would enter 200G. Click OK when done.</p></li> <li><p>Select the server name in the left pane and then in the Actions pane select Restart to restart the web server.</p></li> <li><p>The next step is to create an external HPE OneView for Synergy repository. Follow the steps as listed.</p> <p>a. Using Google Chrome, log on to the HPE Synergy Composer and navigate to OneView -&gt; Settings</p> <p>b. Select Repository and then select '+' to add repository</p> <p>c. Name the repo OneView Repo and in the web server address field enter http://&lt;ip_of_webserver&gt;/media/SPP</p> <p>d. Uncheck the Requires Authentication checkbox and select Add</p></li></ol> <h4 id="install-and-configure-the-hpe-3par-ssmc-2"><a href="#install-and-configure-the-hpe-3par-ssmc-2" class="header-anchor">#</a> Install and configure the HPE 3PAR SSMC</h4> <ol><li>From within the Windows management station, install the HPE 3PAR StoreServ Management Console by copying the media to the station and running the HPESSMC-*-win64.exe installer. Follow the onscreen prompts.</li> <li>Use a web browser to connect to https://&lt;mgmt_vm_ip&gt;:8443.</li> <li>Select Set credential and enter theusername and password. Select Set when done.</li> <li>Install the HPE 3PAR admin tools by copying the media to the management server, mounting it and executing cli\windows\setup.exe. Follow the prompts to install. The user will need to add each array in the solution to the SSMC Administrator Console.</li> <li>Log on to the console and select Actions and then Add.</li> <li>Under System DNS names or IP addresses enter the IP or name of the 3PAR array. The username will be 3paradm and the password is the one the user created earlier. Select Add.</li> <li>After the array appears in the Not Connected state, select it and select Actions and then Accept Certificate. Select Accept and cache. After a moment the array will show in the Connected state.</li> <li>Repeat these steps for any additional arrays within the solution.</li></ol> <h4 id="create-common-provisioning-groups-cpgs-2"><a href="#create-common-provisioning-groups-cpgs-2" class="header-anchor">#</a> Create Common Provisioning Groups (CPGs)</h4> <p>CPGs are required within this solution. By default, HPE 3PAR StoreServ Storage Arrays create a default set of CPGs.</p> <ol><li>Log on to the array as the 3PAR admin user using the SSMC.</li> <li>Select 3PAR StoreServ -&gt; Show all -&gt; Common Provisioning Groups.</li> <li>Select Create CPG. Provide a descriptive name and use RAID6. The remaining default parameters are acceptable for this solution.</li> <li>Create a second CPG for snapshots.</li></ol> <h4 id="integrate-hpe-3par-storeserv-storage-to-vsphere-hosts"><a href="#integrate-hpe-3par-storeserv-storage-to-vsphere-hosts" class="header-anchor">#</a> Integrate HPE 3PAR StoreServ Storage to vSphere hosts</h4> <p>This section describes the process of creating hosts and storage volumes for the virtualization hosts. The steps below describes overview of the tasks.</p> <ol><li>Create hosts HPE 3PAR StoreServ Storage Management Console.</li> <li>Create virtual volume for the vSphere hosts.</li></ol> <h5 id="creating-hosts"><a href="#creating-hosts" class="header-anchor">#</a> Creating hosts</h5> <ol><li>Login to the 3PAR StoreServ Management Console. Select Hosts from the dropdown menu as shown. <strong>Figure 5</strong> shows the selection of hosts in HPE 3PAR StoreServ dashboard.</li></ol> <p><img src="/hpe-solutions-openshift/44-synergy/assets/img/figure97.2984ea73.png" alt=""></p> <p><strong>Figure 5.</strong> Selection of hosts in HPE 3PAR StoreServ dashboard</p> <ol start="2"><li><p>On the Hosts page, click <strong>Create Hosts</strong>.</p></li> <li><p>In the Create Host page, provide appropriate values for the variables.
a. Name: &lt;&gt;
b. System: &lt;&lt;3PAR storage system name&gt;&gt;
c. Domain: None
d. Host Set: NA
e. Host OS: VMware (ESXi)</p></li> <li><p>In the Paths section, click Add iSCSI. On the Add iSCSI page, provide the vSphere hostnames and IQN and then click Add. Repeat this step for all the vSphere hosts to permit all the vSphere hosts with access to the volume. <strong>Figure 6</strong> shows the addition of iSCSI name to the hosts.</p></li></ol> <p><img src="/hpe-solutions-openshift/44-synergy/assets/img/figure37.d218e0f3.png" alt=""></p> <p><strong>Figure 6.</strong> Addition of iSCSI name to the host</p> <ol start="5"><li>After all values have been filled in, click <strong>Create</strong>.</li></ol> <h5 id="creating-virtual-volume"><a href="#creating-virtual-volume" class="header-anchor">#</a> Creating virtual volume</h5> <ol><li>Login to the 3PAR StoreServ Management Console. From the dropdown menu, navigate to Virtual Volumes as shown. <strong>Figure 7</strong> shows the selection of hosts on 3PAR StoreServ dashboard.</li></ol> <p><img src="/hpe-solutions-openshift/44-synergy/assets/img/figure38.7849a3a0.png" alt=""></p> <p><strong>Figure 7.</strong> Selection of hosts on 3PAR StoreServ dashboard</p> <ol start="2"><li>Click <strong>Create virtual volumes</strong>. <strong>Figure 8</strong> shows how to create virtual volume.</li></ol> <p><img src="/hpe-solutions-openshift/44-synergy/assets/img/figure39.2369ed69.png" alt=""></p> <p><strong>Figure 8.</strong> Creation of virtual volume</p> <ol start="3"><li><p>From the Create Virtual Volume page, provide values for the following fields and click <strong>Create</strong>.</p> <p>a. Name:</p> <p>b. System: &lt;3PAR storage system name&gt;</p> <p>c. Domain: None</p> <p>d. Provisioning: Thin Provisioned</p> <p>e. Dedup: No</p> <p>f. Compression: No</p> <p>g. CPG: &lt;disk/raid type&gt;</p> <p>h. Size: 3 TB</p> <p>i. Volume Set: NA</p></li></ol> <p><strong>Figure 9</strong> displays the parameters to create new virtual volume.</p> <p><img src="/hpe-solutions-openshift/44-synergy/assets/img/figure40.6e274580.png" alt=""></p> <p><strong>Figure 9.</strong> Parameters to create new virtual volume</p> <ol start="4"><li>After the Virtual volume has been created, select the Export option in the Actions drop down menu. <strong>Figure 10</strong> shows how to export new create virtual volume to the hosts.</li></ol> <p><img src="/hpe-solutions-openshift/44-synergy/assets/img/figure41.60eb1eba.png" alt=""></p> <p><strong>Figure 10.</strong> Export newly created virtual volume to the host</p> <ol start="5"><li>From the Export Virtual Volumes page, click <strong>Add</strong>. <strong>Figure 11</strong> shows how to add hosts to export the volume.</li></ol> <p><img src="/hpe-solutions-openshift/44-synergy/assets/img/figure42.a71045fc.png" alt=""></p> <p><strong>Figure 11.</strong> Add hosts to export the volume</p> <ol start="6"><li>From the Add page, select the host to which the virtual volume needs to be exported and click Add.</li> <li>After adding the host, select the LUN Auto check box and then click Export.</li></ol> <h2 id="hpe-nimble-iscsi"><a href="#hpe-nimble-iscsi" class="header-anchor">#</a> HPE Nimble iSCSI</h2> <h3 id="integrate-hpe-nimble-storage-with-vsphere-hosts"><a href="#integrate-hpe-nimble-storage-with-vsphere-hosts" class="header-anchor">#</a> Integrate HPE Nimble Storage with vSphere hosts</h3> <ol><li>Create initiator groups in the HPE Nimble Storage management console.</li> <li>Create a volume for the ESXi hosts.</li></ol> <h4 id="create-initiator-groups-in-the-hpe-nimble-storage-management-console"><a href="#create-initiator-groups-in-the-hpe-nimble-storage-management-console" class="header-anchor">#</a> Create initiator groups in the HPE Nimble Storage management console</h4> <p>The initiator group allows connecting volumes directly to the IQNs of the iSCSI adapters. From the HPE Nimble Storage management console, initiator groups should be created with the IQNs of each of the ESXi hosts. Initiator groups can be created by following the steps outlined as follows:</p> <ol><li>Login  to the HPE Nimble Storage management console.</li> <li>Navigate to Manage -&gt; Data Access. <strong>Figure 12</strong> shows data access option within the HPE Nimble Storage management console.</li></ol> <p><img src="/hpe-solutions-openshift/44-synergy/assets/img/figure43.2dc663a0.png" alt=""></p> <p><strong>Figure 12.</strong> Data access option within the HPE Nimble Storage management console</p> <ol start="4"><li><p>On the Create Initiator Group page, enter the details for the following parameters.</p> <p>a. Name: &lt; Name of the initiator group &gt;</p> <p>b. Subnets: From the drop-down menu, select Use selected subnets and add the selected data subnets.</p> <p>c. Initiators: Add the name and IQNs of all the initiators (vSphere hosts), and click Create. <strong>Figure 13</strong> shows creation of an initiator group within the HPE Nimble Storage management console.</p></li></ol> <p><img src="/hpe-solutions-openshift/44-synergy/assets/img/figure44.1094252c.png" alt=""></p> <p><strong>Figure 13.</strong> Creation of an initiator group within the HPE Nimble Storage management console</p> <div class="custom-block tip"><p class="custom-block-title">Note</p> <p>IQNs can be found in the Server Profile of the ESXi hosts in HPE OneView. If hosts are already added into the cluster of vCenter, IQNs can be found at Host &gt; Configuration &gt; Storage Adapter &gt; Highlight your iSCSI Software Adapter &gt; Details.</p></div> <h5 id="create-a-volume-for-the-esxi-hosts-in-hpe-nimble-storage-management-console"><a href="#create-a-volume-for-the-esxi-hosts-in-hpe-nimble-storage-management-console" class="header-anchor">#</a> Create a volume for the ESXi hosts in HPE Nimble Storage management console</h5> <p>After the initiator group is created, perform the following to provision a new volume to store the management virtual machines. A minimum volume size of 3TB is recommended to host the management nodes.</p> <ol><li>From the HPE Nimble Storage management console, navigate to MANAGE -&gt; DATA STORAGE. <strong>Figure 14</strong> shows the Data storage option within the HPE Nimble Storage management console.</li></ol> <p><img src="/hpe-solutions-openshift/44-synergy/assets/img/figure45.4f7a0b71.png" alt=""></p> <p><strong>Figure 14.</strong> Data storage option within the HPE Nimble Storage management console</p> <ol start="2"><li>Click &quot;+&quot; icon to create a new volume. <strong>Figure 15</strong> shows creation of volume within the HPE Nimble Storage management console.</li></ol> <p><img src="/hpe-solutions-openshift/44-synergy/assets/img/figure46.6482a79b.png" alt=""></p> <p><strong>Figure 15.</strong> Creation of volume within the HPE Nimble Storage management console</p> <ol start="3"><li><p>Provide the values to the following parameters for creating a volume. Sample values for the parameters are listed as follows.</p> <p>a. Name: &lt; Name for the Volume &gt;</p> <p>b. Location: &lt; Desired location of the Volume &gt;</p> <p>c. Performance policy: &lt; Assign a performance policy for the volume&gt;</p> <p>d. Size: As per the need of user environment ( 3 TB recommended)</p> <p>e. Protection policy: Assign a protection policy as required</p> <p>f. Access: Assign the initiator group for the vSphere hosts created earlier</p> <p>g. CHAP Account: Assign the CHAP account and select the Allow Multiple Initiator access box.</p></li></ol> <p><img src="/hpe-solutions-openshift/44-synergy/assets/img/figure47.d5f01aa8.png" alt=""></p> <ol start="4"><li>Click <strong>Create</strong> to complete the volume creation.</li></ol> <div class="custom-block tip"><p class="custom-block-title">Note</p> <p>If you utilize virtual worker nodes, it is recommended to create another volume with a size based on the installation environment. 1TB is the minimum recommended size.</p></div></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/hpe-solutions-openshift/44-synergy/Solution-overview/Solution-overview.html" class="prev">
        Solution overview
      </a></span> <span class="next"><a href="/hpe-solutions-openshift/44-synergy/Solution-Deployment/Solution-deployment-flow.html">
        Solution Deployment Flow
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/hpe-solutions-openshift/44-synergy/assets/js/app.25b016e2.js" defer></script><script src="/hpe-solutions-openshift/44-synergy/assets/js/2.4639eab6.js" defer></script><script src="/hpe-solutions-openshift/44-synergy/assets/js/5.39a73899.js" defer></script>
  </body>
</html>
