<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Cabling the HPE Synergy 12000 Frame and HPE Virtual Connect 40Gb SE F8 Modules for HPE Synergy | RED HAT OPENSHIFT CONTAINER PLATFORM 4 ON HPE SYNERGY</title>
    <meta name="description" content="">
    <meta name="generator" content="VuePress 1.4.0">
    
    
    <link rel="preload" href="/hpe-solutions-openshift/assets/css/0.styles.03110986.css" as="style"><link rel="preload" href="/hpe-solutions-openshift/assets/js/app.97ab5217.js" as="script"><link rel="preload" href="/hpe-solutions-openshift/assets/js/2.7869ffdd.js" as="script"><link rel="preload" href="/hpe-solutions-openshift/assets/js/5.bae94e2f.js" as="script"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/10.eaca1b1f.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/11.e4a910b4.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/12.23c485d1.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/13.1dfc2227.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/14.6cfe7b83.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/15.396fb9b8.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/16.a35ed86c.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/17.821cf572.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/18.861062ec.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/19.790531fa.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/20.77410566.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/21.64fb4b9d.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/22.d1f7b188.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/23.47b1fcea.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/24.5e6bc1df.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/25.739eaeab.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/26.c868f3e2.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/3.df4c1f1d.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/4.4ba88d9e.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/6.29d6f4db.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/7.1836f0b8.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/8.7912b669.js"><link rel="prefetch" href="/hpe-solutions-openshift/assets/js/9.9e7c1a91.js">
    <link rel="stylesheet" href="/hpe-solutions-openshift/assets/css/0.styles.03110986.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/hpe-solutions-openshift/" class="home-link router-link-active"><!----> <span class="site-name">RED HAT OPENSHIFT CONTAINER PLATFORM 4 ON HPE SYNERGY</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/hpe-solutions-openshift/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="http://www.hpe.com/info/ra" target="_blank" rel="noopener noreferrer" class="nav-link external">
  RA Library
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/hpe-solutions-openshift/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="http://www.hpe.com/info/ra" target="_blank" rel="noopener noreferrer" class="nav-link external">
  RA Library
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/hpe-solutions-openshift/Introduction/Introduction.html" class="sidebar-link">Introduction</a></li><li><a href="/hpe-solutions-openshift/Solution overview/Solution-overview.html" class="sidebar-link">Solution overview</a></li><li><a href="/hpe-solutions-openshift/Solution components/Solution-components.html" class="sidebar-link">Solution components</a></li><li><a href="/hpe-solutions-openshift/Preparing the execution environment/Preparing-the-execution-environment.html" class="sidebar-link">Preparing the execution environment</a></li><li><a href="/hpe-solutions-openshift/Physical environment configuration/Physical-environment-configuration.html" class="active sidebar-link">Physical environment configuration</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Physical environment configuration/Physical-environment-configuration.html#cabling-the-hpe-synergy-12000-frame-and-hpe-virtual-connect-40gb-se-f8-modules-for-hpe-synergy" class="sidebar-link">Cabling the HPE Synergy 12000 Frame and HPE Virtual Connect 40Gb SE F8 Modules for HPE Synergy</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Physical environment configuration/Physical-environment-configuration.html#cabling-the-hpe-synergy-12000-frame-to-fibre-channel-5945-switch" class="sidebar-link">Cabling the HPE Synergy 12000 Frame to Fibre Channel 5945 switch</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Physical environment configuration/Physical-environment-configuration.html#cabling-between-hpe-synergy-12000-frame-to-3par-5945-switch" class="sidebar-link">Cabling between HPE Synergy 12000 Frame to 3PAR 5945 switch</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Physical environment configuration/Physical-environment-configuration.html#configuring-the-solution-switching" class="sidebar-link">Configuring the solution switching</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Physical environment configuration/Physical-environment-configuration.html#hpe-synergy-480-gen10-compute-modules" class="sidebar-link">HPE Synergy 480 Gen10 Compute Modules</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Physical environment configuration/Physical-environment-configuration.html#hpe-synergy-integration-with-cisco-application-centric-infrastructure-aci" class="sidebar-link">HPE Synergy integration with Cisco Application Centric Infrastructure (ACI)</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Physical environment configuration/Physical-environment-configuration.html#hpe-synergy-composer-2" class="sidebar-link">HPE Synergy Composer 2</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Physical environment configuration/Physical-environment-configuration.html#bootstrap-node" class="sidebar-link">Bootstrap node</a></li><li class="sidebar-sub-header"><a href="/hpe-solutions-openshift/Physical environment configuration/Physical-environment-configuration.html#server-profiles" class="sidebar-link">Server profiles</a></li></ul></li><li><a href="/hpe-solutions-openshift/Physical node configuration/Physical-node-configuration.html" class="sidebar-link">Physical node configuration</a></li><li><a href="/hpe-solutions-openshift/Virtual node configuration/Virtual-node-configuration.html" class="sidebar-link">Virtual node configuration</a></li><li><a href="/hpe-solutions-openshift/vSphere Provisioner/vSphere-Provisioner.html" class="sidebar-link">vSphere Provisioner</a></li><li><a href="/hpe-solutions-openshift/Operating system deployment/Operating-system-deployment.html" class="sidebar-link">Operating system deployment</a></li><li><a href="/hpe-solutions-openshift/Red Hat OpenShift Container Platform deployment/Red-Hat-OpenShift-Container-Platform-deployment.html" class="sidebar-link">Red Hat OpenShift Container Platform deployment</a></li><li><a href="/hpe-solutions-openshift/Red Hat Local Storage Operator/Red-Hat-Local-Storage-Operator.html" class="sidebar-link">Red Hat Local Storage Operator</a></li><li><a href="/hpe-solutions-openshift/Integration of HPE OneView with Prometheus/Integration-of-HPE-OneView-with-Prometheus.html" class="sidebar-link">Integration of HPE OneView with Prometheus</a></li><li><a href="/hpe-solutions-openshift/Storage/Storage.html" class="sidebar-link">Storage</a></li><li><a href="/hpe-solutions-openshift/HPE CSI drivers/HPE-CSI-drivers.html" class="sidebar-link">HPE CSI drivers</a></li><li><a href="/hpe-solutions-openshift/Securing Red Hat OpenShift Container Platform using Sysdig Secure and Sysdig Monitor/Securing-Red-Hat-OpenShift-Container-Platform-using-Sysdig-Secure-and-Sysdig-Monitor.html" class="sidebar-link">Securing Red Hat OpenShift Container Platform using Sysdig Secure and Sysdig Monitor</a></li><li><a href="/hpe-solutions-openshift/Physical worker node labeling in Red Hat OpenShift cluster/Physical-worker-node-labeling-in-Red-Hat-OpenShift-cluster.html" class="sidebar-link">Physical worker node labeling in Red Hat OpenShift cluster</a></li><li><a href="/hpe-solutions-openshift/OpenShift Operators/OpenShift-Operators.html" class="sidebar-link">OpenShift Operators</a></li><li><a href="/hpe-solutions-openshift/Validating OpenShift Container Platform deployment/Validating-OpenShift-Container-Platform-deployment.html" class="sidebar-link">Validating OpenShift Container Platform deployment</a></li><li><a href="/hpe-solutions-openshift/Resources and additional links/Resources-and-additional-links.html" class="sidebar-link">Resources and additional links</a></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h2 id="cabling-the-hpe-synergy-12000-frame-and-hpe-virtual-connect-40gb-se-f8-modules-for-hpe-synergy"><a href="#cabling-the-hpe-synergy-12000-frame-and-hpe-virtual-connect-40gb-se-f8-modules-for-hpe-synergy" class="header-anchor">#</a> Cabling the HPE Synergy 12000 Frame and HPE Virtual Connect 40Gb SE F8 Modules for HPE Synergy</h2> <p>This section shows the physical cabling between frames, Virtual Connect modules and solution switching. It is intended to provide an understanding of how the infrastructure was interconnected during testing and to serve as a guide on which the installation user can base their configuration.</p> <p>Figure 7 describes the cabling configuration of the three (3) HPE Synergy 12000 Frames as well as the HPE FlexFabric 5945 Switches and Intelligent Resilient Fabric (IRF) within the context of this solution. These cables carry frame management, inter-frame and interconnect traffic between frames.</p> <p><img src="/hpe-solutions-openshift/assets/img/figure7.f87d199e.png" alt=""></p> <p><strong>Figure 7.</strong> Frame and switch cabling within the solution</p> <h2 id="cabling-the-hpe-synergy-12000-frame-to-fibre-channel-5945-switch"><a href="#cabling-the-hpe-synergy-12000-frame-to-fibre-channel-5945-switch" class="header-anchor">#</a> Cabling the HPE Synergy 12000 Frame to Fibre Channel 5945 switch</h2> <p>HPE 3PAR StoreServ 8440 Storage provides shared and dedicated storage for a variety of purposes within this solution including housing virtual machines and persistent volume. Figure 8 shows the cabling of the HPE Synergy 12000 frames to the HPE Fibre Channel switching utilized in this solution.</p> <p><img src="/hpe-solutions-openshift/assets/img/figure8.8601f6e8.png" alt=""></p> <p><strong>Figure 8.</strong> Cabling of HPE Synergy interconnects and HPE SN6600B switching</p> <p>Table 10 describes the cabling from the HPE Synergy 12000 Frames to the HPE SN6600B Fibre Channel switches.</p> <p><strong>Table 10.</strong> Cabling between HPE Synergy and HPE SN6600B switching</p> <div class="language- extra-class"><pre><code>| Source Port (Frame) | Destination Port (Switch)  |
| --------------------------- | --------------------------- |
| Frame 1, ICM 2, Port 1 | SAN Switch A, Port 10|
| Frame 1, ICM 2, Port 2 | SAN Switch A, Port 11|
| Frame 1, ICM 2, Port 3 | SAN Switch A, Port 16|
| Frame 1, ICM 2, Port 4 | SAN Switch A, Port 17|
| Frame 1, ICM 5, Port 1 | SAN Switch B, Port 10|
| Frame 1, ICM 5, Port 2 | SAN Switch B, Port 11|
| Frame 1, ICM 5, Port 3 | SAN Switch B, Port 16|
| Frame 1, ICM 5, Port 4 | SAN Switch B, Port 17|
| Frame 2, ICM 2, Port 1 | SAN Switch A, Port 14|
| Frame 2, ICM 2, Port 2 | SAN Switch A, Port 15|
| Frame 2, ICM 2, Port 3 | SAN Switch A, Port 20|
| Frame 2, ICM 2, Port 4 | SAN Switch A, Port 21|
| Frame 2, ICM 5, Port 1 | SAN Switch B, Port 14|
| Frame 2, ICM 5, Port 2 | SAN Switch B, Port 15|
| Frame 2, ICM 5, Port 3 | SAN Switch B, Port 20|
| Frame 2, ICM 5, Port 4 | SAN Switch B, Port 21|
| Frame 3, ICM 2, Port 1 | SAN Switch A, Port 18|
| Frame 3, ICM 2, Port 2 | SAN Switch A, Port 19|
| Frame 3, ICM 2, Port 3 | SAN Switch A, Port 22|
| Frame 3, ICM 2, Port 4 | SAN Switch A, Port 23|
| Frame 3, ICM 2, Port 4 | SAN Switch A, Port 23|
| Frame 3, ICM 5, Port 1 | SAN Switch B, Port 18|
| Frame 3, ICM 5, Port 2 | SAN Switch B, Port 19|
| Frame 3, ICM 5, Port 3 | SAN Switch B, Port 22|
| Frame 3, ICM 5, Port 4 | SAN Switch B, Port 23|
</code></pre></div><p>Table 11 describes the connectivity of the HPE 3PAR StoreServ 8440 controllers to the HPE SN6600B SAN switching.</p> <p><strong>Table 11.</strong> HPE 3PAR StoreServ 8440 Controller to SAN</p> <div class="language- extra-class"><pre><code>| Source Port (HPE 3PAR)| Destination Port (Switch)  |
| --------------------------- | --------------------------- |
| Node 0, S2P1 | SAN Switch A, Port 8 |
| Node 0, S2P2 | SAN Switch B, Port 8 |
| Node 1, S2P1 | SAN Switch A, Port 12|
| Node 1, S2P2 | SAN Switch B, Port 12|
| Node 2, S2P1 | SAN Switch A, Port 9 |
| Node 2, S2P2 | SAN Switch B, Port 9 |
| Node 3, S2P1 | SAN Switch A, Port 13|
| Node 3, S2P2 | SAN Switch B, Port 13|
</code></pre></div><h2 id="cabling-between-hpe-synergy-12000-frame-to-3par-5945-switch"><a href="#cabling-between-hpe-synergy-12000-frame-to-3par-5945-switch" class="header-anchor">#</a> Cabling between HPE Synergy 12000 Frame to 3PAR 5945 switch</h2> <p>The HPE 3PAR StoreServ 8440 Storage used in this solution provides shared and dedicated storage for a variety of purposes within this solution including virtual machine hosting, registry storage, and persistent volume for containers.</p> <h3 id="hpe-nimble-iscsi"><a href="#hpe-nimble-iscsi" class="header-anchor">#</a> HPE Nimble iSCSI</h3> <p>A HPE Nimble Storage AF40 array provides shared and dedicated storage for a variety of purposes within this solution.</p> <p>Figure 9 describes the cabling configuration of the three (3) HPE Synergy 12000 Frames as well as the HPE FlexFabric 5945 Switches and Intelligent Resilient Fabric (IRF) within the context of this solution. These cables carry frame management, inter-frame and interconnect traffic between frames.</p> <p><img src="/hpe-solutions-openshift/assets/img/figure9.f87d199e.png" alt=""></p> <p><strong>Figure 9.</strong> Frame and switch cabling within the solution</p> <p>Figure 10 shows the cabling of HPE Synergy Frames to the network switches. At the lowest level, there are four (4) 40GbE connections dedicated to carrying redundant, production network traffic to the first layer switch where it is further distributed.
iSCSI traffic is separated into two (2) VLANs and is carried to the first network switch pair over two (2) 40GbE links per VLAN. Unlike the Ethernet traffic which is distributed between the switches, each iSCSI VLAN is sent directly to one switch configured with a pair of access ports.</p> <p><img src="/hpe-solutions-openshift/assets/img/figure10.0be72d8b.png" alt=""></p> <p><strong>Figure 10.</strong> Cabling of the HPE Synergy interconnects to the HPE FlexFabric 5945 switches</p> <p>Figure 11 shows the cabling of the HPE Nimble Storage AF40 to the HPE switching utilized in this solution. Note that this diagram shows the storage and switching in the same rack to provide clarity. As implemented for this solution, the switching resided in the HPE Synergy rack. The orange and purple wires in the figure represent the separate iSCSI VLANs.</p> <p><img src="/hpe-solutions-openshift/assets/img/figure11.a124e3ac.png" alt=""></p> <p><strong>Figure 11.</strong> Cabling of the HPE Nimble Storage arrays to the HPE FF 5940 switches</p> <h3 id="networking"><a href="#networking" class="header-anchor">#</a> Networking</h3> <p>Figure 12 documents the cabling of the solution from the HPE Virtual Connect SE 40 GB modules to the switches. All egressing Ethernet networks are carried on a single bridge-aggregation group (BAGG). Top of rack switching was used in the creation of this solution, but end of row switching is equally effective in HPE Synergy environments and can reduce overall solution costs by reducing the number of physical switches.</p> <p><img src="/hpe-solutions-openshift/assets/img/figure12.2926a866.png" alt=""></p> <p><strong>Figure 12.</strong> Network cabling from the HPE Synergy 12000 Frames to the switches</p> <p>Table 12 describes the configuration of the network as defined within HPE OneView for HPE Synergy and the bandwidth associated with each network. Network is carried outbound on a single BAGG except the Synergy Management network.</p> <p><strong>Table 12.</strong> Network defined within HPE OneView for HPE Synergy</p> <table><thead><tr><th>Network Name</th> <th>Type</th> <th>VLAN Number</th> <th>Purpose</th> <th>Requested Bandwidth (Gb)</th> <th>Maximum Bandwidth (Gb)</th></tr></thead> <tbody><tr><td>Management</td> <td>Ethernet</td> <td>1193</td> <td>Solution management</td> <td>5</td> <td>20</td></tr> <tr><td>Data_Center</td> <td>Ethernet</td> <td>2193</td> <td>Application access and authentication</td> <td>10</td> <td>20</td></tr> <tr><td>iSCSI_VLAN_A</td> <td>Ethernet</td> <td>3193</td> <td>iSCSI_VLAN_A</td> <td>10</td> <td>40</td></tr> <tr><td>iSCSI_VLAN_B</td> <td>Ethernet</td> <td>3194</td> <td>iSCSI_VLAN_A</td> <td>10</td> <td>40</td></tr></tbody></table> <p>Table 13 explains the cabling of the Virtual Connect interconnect modules to the HPE FlexFabric 5945 switching.</p> <p><strong>Table 13.</strong> Networks used in this solution</p> <table><thead><tr><th>Uplink Set</th> <th>Synergy Source</th> <th>Switch Destination</th></tr></thead> <tbody><tr><td>Network</td> <td>Enclosure 1 Port Q3</td> <td>FortyGigE1/1/1</td></tr> <tr><td>Enclosure 1 Port Q4</td> <td>FortyGigE2/1/1</td> <td></td></tr> <tr><td>Enclosure 2 Port Q3</td> <td>FortyGigE1/1/2</td> <td></td></tr> <tr><td>Enclosure 2 Port Q4</td> <td>FortyGigE2/1/2</td> <td></td></tr> <tr><td>iSCSI_SAN_A</td> <td>Enclosure 1 Port Q5</td> <td>FortyGigE1/1/5</td></tr> <tr><td>Enclosure 1 Port Q6</td> <td>FortyGigE1/1/6</td> <td></td></tr> <tr><td>iSCSI_SAN_B</td> <td>Enclosure 2 Port Q5</td> <td>FortyGigE2/1/5</td></tr> <tr><td>Enclosure 2 Port Q6</td> <td>FortyGigE2/1/6</td> <td></td></tr></tbody></table> <p>Utilizing HPE Synergy, the network within the solution can traverse the HPE Synergy infrastructure in an east-west fashion across high speed, low latency links both within and between HPE Virtual Connect Modules. The communication between the Red Hat OpenShift Container Platform 4 management pieces remains within the HPE Synergy Frames.</p> <h2 id="configuring-the-solution-switching"><a href="#configuring-the-solution-switching" class="header-anchor">#</a> Configuring the solution switching</h2> <p>The solution described in this document utilized HPE FlexFabric 5945 switches. The HPE FlexFabric 5945 switches are configured according to the configuration parameters found later in this section. The switches should be configured with an HPE Intelligent Resilient Framework (IRF). To understand the process of configuring IRF, refer the HPE FlexFabric 5945 Switch Series Installation Guide at <a href="https://support.hpe.com/hpsc/doc/public/display?sp4ts.oid=null&amp;docLocale=en_US&amp;docId=emr_na-c05212026" target="_blank" rel="noopener noreferrer">https://support.hpe.com/hpsc/doc/public/display?sp4ts.oid=null&amp;docLocale=en_US&amp;docId=emr_na-c05212026<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>. This guide may also be used to understand the initial installation of switching, creation of user accounts and access methods. The remainder of this section is built with the assumption that the switch has been installed, configured for IRF, hardened, and is accessible over SSH.</p> <div class="custom-block tip"><p class="custom-block-title">Note</p> <p>HPE Synergy facilitates the use of end of row switching to reduce switch and port counts in the context of the solution. If end of row switching is chosen, then this section should be used as guidance for how to route network traffic outside of the HPE Synergy Frames.</p></div> <h3 id="physical-cabling"><a href="#physical-cabling" class="header-anchor">#</a> Physical cabling</h3> <p>Table 14 is a map of source ports to ports on the HPE FlexFabric 5945 switches.</p> <p><strong>Table 14.</strong> HPE FlexFabric 5945 port map</p> <table><thead><tr><th>Source Port</th> <th>Switch Port</th></tr></thead> <tbody><tr><td>Nimble Management Port Eth1</td> <td>TenGigE1/2/17</td></tr> <tr><td>Nimble Controller A TG1</td> <td>TenGigE1/2/13</td></tr> <tr><td>Nimble Controller A TG2</td> <td>TenGigE2/2/13</td></tr> <tr><td>Nimble Controller B TG1</td> <td>TenGigE1/2/14</td></tr> <tr><td>Nimble Controller B TG2</td> <td>TenGigE2/2/14</td></tr> <tr><td>Nimble Replication Port Eth2</td> <td>TenGigE1/2/15</td></tr> <tr><td>Virtual Connect Frame U30, Q3</td> <td>FortyGigE1/1/1</td></tr> <tr><td>Virtual Connect Frame U30, Q4</td> <td>FortyGigE2/1/1</td></tr> <tr><td>Virtual Connect Frame U30, Q5</td> <td>FortyGigE1/1/5</td></tr> <tr><td>Virtual Connect Frame U30, Q6</td> <td>FortyGigE1/1/6</td></tr> <tr><td>Virtual Connect Frame U40, Q3</td> <td>FortyGigE1/1/2</td></tr> <tr><td>Virtual Connect Frame U40, Q4</td> <td>FortyGigE2/1/2</td></tr> <tr><td>Virtual Connect Frame U40, Q5</td> <td>FortyGigE2/1/5</td></tr> <tr><td>Virtual Connect Frame U40, Q6</td> <td>FortyGigE2/1/6</td></tr> <tr><td>To Upstream Switching</td> <td>Customer Choice</td></tr></tbody></table> <p>It is recommended that the installation user logs on to the switch post-configuration and provides a description for each of these ports.</p> <h3 id="network-definition"><a href="#network-definition" class="header-anchor">#</a> Network definition</h3> <p>There are multiple networks defined in this solution:</p> <ul><li><p><strong>Management Network</strong> : This network facilitates the management of hardware and software interfaced by IT.</p></li> <li><p><strong>Data Center Network</strong>: This network carries traffic from the overlay network used by the pods to external consumers of pod deployed services.</p></li> <li><p><strong>iSCSI Network</strong> : This network consists of two separate network segments that provide a redundant path for iSCSI storage traffic within the solution.</p></li></ul> <p>Table 15 defines the VLANs configured using HPE Synergy Composer in the creation of this solution. These networks should be defined at both the first layer switch and within Composer. This solution utilizes unique VLANs for the data center and solution management segments. Actual VLANs and network count will be determined by the requirements of your production environment.</p> <p><strong>Table 15.</strong> Networks used in this solution</p> <table><thead><tr><th>Network Function</th> <th>VLAN Number</th> <th>Bridge Aggregation Group</th></tr></thead> <tbody><tr><td>Solution_Management</td> <td>1193</td> <td>111</td></tr> <tr><td>Data_Center</td> <td>2193</td> <td>111</td></tr> <tr><td>iSCSI_A</td> <td>3193</td> <td>112</td></tr> <tr><td>iSCSI_B</td> <td>3194</td> <td>113</td></tr></tbody></table> <ol><li><p>To add these networks to the switch, log on to the switch console over SSH and run the following commands.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> sys
<span class="token operator">&gt;</span> vlan <span class="token number">1193</span> <span class="token number">2193</span> <span class="token number">3193</span> <span class="token number">3194</span>
</code></pre></div></li> <li><p>For each of these VLANs, perform the following steps.</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> interface vlan-interface <span class="token comment">####</span>
<span class="token operator">&gt;</span> name VLAN Name per table above
<span class="token operator">&gt;</span> description Add text that describes the purpose of the VLAN
<span class="token operator">&gt;</span> quit
</code></pre></div></li></ol> <div class="custom-block tip"><p class="custom-block-title">Note</p> <div class="language- extra-class"><pre><code>It is strongly recommended to configure a dummy VLAN on the switches and assign unused ports to that VLAN.
</code></pre></div></div> <p>The switches should be configured with a bridge aggregation group (BAGG) for the different links to the HPE Synergy Frame connections. To configure the BAGG and ports as described in <strong>Table 5</strong>, run the following commands.</p> <div class="language- extra-class"><pre><code>```bash
&gt; interface Bridge-Aggregation111
&gt; link-aggregation mode dynamic
&gt; description &lt;FrameNameU30&gt;-ICM
&gt; quit

&gt; interface range name &lt;FrameNameU30&gt;-ICM interface Bridge-Aggregation111
&gt; quit

&gt; interface range FortyGigE 1/1/1 to FortyGigE 1/1/2 FortyGigE 2/1/1 to FortyGigE 2/1/2
&gt; port link-aggregation group 111
&gt; quit

&gt; interface range name &lt;FrameNameU30&gt;-ICM
&gt; port link-type trunk
&gt; undo port trunk permit vlan 1
&gt; port trunk permit vlan 193 1193 2193
&gt; quit
```
</code></pre></div><ol start="3"><li>After the configuration of the switches is complete, save the state and apply it by typing <strong>save</strong> and follow the resulting prompts.</li></ol> <h2 id="hpe-synergy-480-gen10-compute-modules"><a href="#hpe-synergy-480-gen10-compute-modules" class="header-anchor">#</a> HPE Synergy 480 Gen10 Compute Modules</h2> <p>This section describes the connectivity of the HPE Synergy 480 Gen10 Compute Modules used in the creation of this solution. The HPE Synergy 480 Gen10 Compute Modules, regardless of function, were all configured identically. Table 16 describes the host configuration tested for this solution. Server configuration should be based on customer needs and the configuration used in the creation of this solution might not align with the requirements of any given production implementation.</p> <p><strong>Table 16.</strong> Host configuration</p> <table><thead><tr><th>Component</th> <th>Quantity</th></tr></thead> <tbody><tr><td>HPE Synergy 480/660 Gen10 Intel Xeon-Gold 6130 (2.1GHz/16-core/125W) FIO Processor Kit</td> <td>2 per server</td></tr> <tr><td>HPE 8GB (1x 8GB) Single Rank x8 DDR4-2666 CAS-19-19 Registered Smart Memory Kit</td> <td>20 per server</td></tr> <tr><td>HPE 16GB (1x 16GB) Single Rank x4 DDR4-2666 CAS-19-19 Registered Smart Memory Kit</td> <td>4 per server</td></tr> <tr><td>HPE Synergy 3820C 10/20Gb Converged Network Adapter</td> <td>1 per server</td></tr> <tr><td>HPE Smart Array P204i-c SR Gen10 12G SAS controller</td> <td>1 per server</td></tr> <tr><td>HPE 1.92TB SATA 6GB Mixed Use SFF (2.5in) 3yr Warranty Digitally Signed Firmware SSD</td> <td>2 per management host</td></tr></tbody></table> <h2 id="hpe-synergy-integration-with-cisco-application-centric-infrastructure-aci"><a href="#hpe-synergy-integration-with-cisco-application-centric-infrastructure-aci" class="header-anchor">#</a> HPE Synergy integration with Cisco Application Centric Infrastructure (ACI)</h2> <h3 id="what-is-cisco-aci"><a href="#what-is-cisco-aci" class="header-anchor">#</a> What is Cisco ACI?</h3> <p>Cisco® Application Centric Infrastructure (Cisco ACI™) is an industry-leading secure, open, and comprehensive Software-Defined Networking (SDN) solution. It radically simplifies, optimizes, and accelerates infrastructure deployment and governance and expedites the application deployment lifecycle.</p> <p>Cisco ACI delivers an intent-based networking framework to enable agility in the data centre. It captures high-level business and user intent in the form of a policy and translates this intent into the network constructs necessary to dynamically provision the network, security, and infrastructure services. It uses a holistic system-based approach, with tight integration between hardware and software and physical and virtual elements, an open ecosystem model, and innovative Cisco customer Application-Specific Integrated Circuits (ASICs) to enable unique business value for modern data centres. This unique approach uses a common policy-based operating model across the network, drastically reducing the cost and complexity of operating your network.</p> <h3 id="integration-with-hpe-synergy"><a href="#integration-with-hpe-synergy" class="header-anchor">#</a> Integration with HPE Synergy</h3> <p>This section specifies the reference to the technical whitepaper that will help the Data Center administrators to configure both baremetal OS and VMware ESXi hosts running on HPE Synergy compute nodes with Cisco ACI.</p> <p>The two use cases where HPE Synergy can be integrated with Cisco ACI are:</p> <div class="language- extra-class"><pre><code> 1. VMware virtual infrastructure integration with Cisco ACI to provide dynamic network provisioning.
 2. Bare metal OS installation on HPE Synergy compute nodes integrated with Cisco ACI.
</code></pre></div><p>For detailed information on integration of HPE Synergy with Cisco ACI, see <a href="https://support.hpe.com/hpesc/public/docDisplay?docId=emr_na-a00003736en_us" target="_blank" rel="noopener noreferrer">https://support.hpe.com/hpesc/public/docDisplay?docId=emr_na-a00003736en_us<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <h2 id="hpe-synergy-composer-2"><a href="#hpe-synergy-composer-2" class="header-anchor">#</a> HPE Synergy Composer 2</h2> <p>At the core of the management of the HPE Synergy environment is HPE Synergy Composer 2. A pair of HPE Synergy Composers are deployed across frames to provide redundant management of the environment for both initial deployment and changes over the lifecycle of the solution. HPE Synergy Composer 2 is used to configure the environment prior to the deployment of the operating systems and applications.</p> <p>This section walks the installation user through the process of installing and configuring the HPE Synergy Composer.</p> <h3 id="configure-the-hpe-synergy-composer-via-vnc"><a href="#configure-the-hpe-synergy-composer-via-vnc" class="header-anchor">#</a> Configure the HPE Synergy Composer via VNC</h3> <p>To configure HPE Synergy Composer with the user laptop, follow these steps:</p> <ol><li><p>Configure the laptop Ethernet port to the IP address 192.168.10.2/24. No gateway is required.</p></li> <li><p>Use a CAT5e cable to connect the laptop computer Ethernet port to laptop port on a front panel module of HPE Synergy Composer.</p></li> <li><p>Access the HPE Synergy Console using a web browser. Start a new browser session and enter <a href="http://192.168.10.1:5800" target="_blank" rel="noopener noreferrer">http://192.168.10.1:5800<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p></li> <li><p>Click <strong>Connect</strong> to start HPE OneView for Synergy from the HPE Synergy console.</p></li> <li><p>Click <strong>Hardware Setup</strong> to connect with <strong>Installation Technician</strong> user privileges.</p></li> <li><p>In the Appliance Network dialog box, fill in the following information.</p> <p>a.  Appliance host name: Enter <strong>a fully qualified domain name of the HPE Synergy Composer.</strong></p> <p>b.  Address assignment: <strong>Manual</strong></p> <p>c.  IP address: <strong>Enter an IP address on the management network</strong>.</p> <p>d.  Subnet mask or CIDR: <strong>Enter the subnet mask of the management network</strong>.</p> <p>e.  Gateway address: <strong>Enter the gateway for the management network</strong>.</p> <p>f.  Maintenance IP address 1: <strong>Enter a maintenance IP address on the management</strong> <strong>network</strong>.</p> <p>g.  Maintenance IP address 2: <strong>Enter a secondary maintenance IP</strong> <strong>address on the management network.</strong></p> <p>h.  Preferred DNS server: <strong>Enter the DNS server.</strong></p> <p>i.  IPv6 Address assignment: <strong>Unassign</strong></p></li> <li><p>Click OK.</p></li> <li><p>When the hardware discovery process is complete, all HPE Synergy hardware including the Frames, Composer modules, Frame Link modules, Interconnect modules, Compute modules, and storage modules must be discovered and claimed by HPE OneView for Synergy.</p></li> <li><p>Review and correct any issues listed in the hardware setup checklist. The HPE Synergy 12000 Frame Setup and Installation Guide available at <a href="http://www.hpe.com/info/synergy-docs" target="_blank" rel="noopener noreferrer">http://www.hpe.com/info/synergy-docs<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> provides troubleshooting steps for common issues during hardware setup.</p></li> <li><p>Navigate to <strong>OneView</strong> -&gt; <strong>Settings</strong> and then click <strong>Appliance</strong>. Verify that the active and standby appliances show a status of <strong>Connected</strong>.</p></li></ol> <h3 id="configure-appliance-credentials"><a href="#configure-appliance-credentials" class="header-anchor">#</a> Configure appliance credentials</h3> <ol><li><p>Log in to the HPE OneView for Synergy Web Administration Portal, review and accept the License Agreement.</p></li> <li><p>On the HPE OneView Support Dialog box, verify that Authorized Service has a setting of Enabled. Click OK.</p></li> <li><p>Log in as <strong>Administrator</strong> with default password <strong>admin</strong>, set the new password to &lt;&lt;composer_administrator_password&gt;&gt; and click <strong>Ok</strong>.</p></li></ol> <h3 id="configure-solution-firmware"><a href="#configure-solution-firmware" class="header-anchor">#</a> Configure solution firmware</h3> <p>This solution adheres to the firmware recipe specified with the HPE Converged Solutions 750 specifications which can be found at CS750 Firmware and Software Compatibility Matrix. The solution used the latest firmware recipe available as of March 2020 including HPE OneView for Synergy 5.0:</p> <ol><li><p>Select the <strong>OneView menu</strong> and select <strong>Settings.</strong></p></li> <li><p>Under Appliance, select <strong>Update Appliance</strong> and <strong>update Composer.</strong></p></li> <li><p>After the update process completes, validate that both composer modules are connected and there is a <strong>green</strong> <strong>checkmark.</strong></p></li></ol> <h3 id="solution-configuration"><a href="#solution-configuration" class="header-anchor">#</a> Solution configuration</h3> <p>The installation user should utilize the Synergy Guided Setup to complete the following solution configuration details.</p> <h4 id="create-additional-users"><a href="#create-additional-users" class="header-anchor">#</a> Create additional users</h4> <p>It is recommended that you create a read-only user and an administrator account with a different username than administrator.</p> <h4 id="firmware"><a href="#firmware" class="header-anchor">#</a> Firmware</h4> <p>Upload the latest version of the firmware bundle based on the HPE Converged Solutions 750 recipe. After the bundle starts uploading, proceed to additional steps without disrupting the upload.</p> <h4 id="create-an-ip-pool-on-the-management-network"><a href="#create-an-ip-pool-on-the-management-network" class="header-anchor">#</a> Create an IP pool on the management network</h4> <p>Follow the guidance to create an IP pool on the management network. This IP pool will provide IP addresses to management IP’s and HPE device iLOs within the solution. Ensure that the pool is enabled prior to proceeding.</p> <h4 id="configure-ethernet-networks"><a href="#configure-ethernet-networks" class="header-anchor">#</a> Configure Ethernet networks</h4> <p>As explained in the <a href="#network-definition">Network definition</a> section of this document, the solution utilizes three (3) network segments. Refer to the Create networks section of the OneView Guided Setup wizard to define the networks shown in Table 17 at a minimum. Your VLAN values will generally differ from those described.</p> <p><strong>Table 17.</strong> Network defined within HPE Synergy Composer for this solution</p> <table><thead><tr><th>Network Name</th> <th>VLAN Number</th> <th>Purpose</th></tr></thead> <tbody><tr><td>Management</td> <td>Ethernet</td> <td>1193</td></tr> <tr><td>Data_Center</td> <td>Ethernet</td> <td>2193</td></tr></tbody></table> <p>The management network should be associated with the management network IP pool, which the user specified in the prior step. The installation user should create any additional required networks for the solution.</p> <h4 id="create-logical-interconnect-groups"><a href="#create-logical-interconnect-groups" class="header-anchor">#</a> Create Logical Interconnect Groups</h4> <p>Within Composer, use the Guided Setup to create a Logical Interconnect Group (LIG) with three (3) uplink sets defined. For this solution, the uplink sets are named Network. The uplink sets “Network” carries all other networks defined for the solution. Table 18 defines the ports used to carry the uplink sets.</p> <p><strong>Table 18.</strong> Networks used in this solution</p> <table><thead><tr><th>Uplink Set</th> <th>Synergy Source</th></tr></thead> <tbody><tr><td>Network</td> <td>Enclosure 1, Bay 3, Port Q3</td></tr> <tr><td></td> <td>Enclosure 1, Bay 3, Port Q4</td></tr> <tr><td></td> <td>Enclosure 2, Bay 6, Port Q3</td></tr> <tr><td></td> <td>Enclosure 2, Bay 6, Port Q4</td></tr></tbody></table> <h4 id="create-enclosure-group"><a href="#create-enclosure-group" class="header-anchor">#</a> Create Enclosure Group</h4> <ol><li><p>From the OneView Guided Setup, select Create <strong>enclosure group.</strong></p></li> <li><p>Provide a <strong>name</strong> and enter the <strong>number of frames</strong>.</p></li> <li><p>Select <strong>Use address pool</strong> and utilize the <strong>management pool</strong> defined earlier.</p></li> <li><p>Use the Logical Interconnect Group from the prior step in the creation of the Enclosure Group.</p></li> <li><p>Select <strong>Create</strong> when ready.</p></li></ol> <h4 id="create-logical-enclosure"><a href="#create-logical-enclosure" class="header-anchor">#</a> Create Logical Enclosure</h4> <p>Use the Guided Setup to create a logical enclosure making use of all three (3) enclosures. Select the firmware you uploaded earlier as a baseline. It can take some time for the firmware to update across the solution stack. Ensure that firmware complies with the baseline by selecting <strong>Actions</strong> and then <strong>Update</strong> <strong>Firmware</strong>. Click <strong>Cancel</strong> to exit.</p> <h3 id="configuring-solution-storage"><a href="#configuring-solution-storage" class="header-anchor">#</a> Configuring solution storage</h3> <p>The HPE Synergy D3940 Storage Module provides SSDs and optional Hard Disk Drives (HDDs) consumed by the Local Storage Operator, if not utilizing local disks within the HPE Synergy 480 Gen 10 Compute Modules. It can also provide boot volumes. HPE Nimble Storage and HPE 3PAR dynamically provides Persistent Volume (PV) for containers using Dynamic Volume Provisioner which is integrated with the HPE Container Storage Interface Driver.</p> <p>Figure 13 describes the logical storage layout used in the solution. The HPE Synergy D3940 Storage Module provides SAS volume.</p> <p><img src="/hpe-solutions-openshift/assets/img/figure13.80530807.png" alt=""></p> <p><strong>Figure 13.</strong> Logical storage layout</p> <p>Table 19 lists all volume used within the solution and highlights what storage provides the capacity and performance of each function.</p> <p><strong>Table 19.</strong> Volume used in this solution</p> <table><thead><tr><th>Volume/Disk Function</th> <th>Qty</th> <th>Size</th> <th>Source</th> <th>Hosts</th> <th>Shared/Dedicated</th></tr></thead> <tbody><tr><td>Local volume</td> <td>3</td> <td>960GB</td> <td>HPE Synergy D3940 storage</td> <td>OpenShift worker nodes</td> <td>dedicated</td></tr> <tr><td>Operating System (optional)</td> <td>6</td> <td>300GB</td> <td>HPE Synergy D3940 storage</td> <td>All nodes</td> <td>dedicated</td></tr></tbody></table> <h2 id="bootstrap-node"><a href="#bootstrap-node" class="header-anchor">#</a> Bootstrap node</h2> <p>A temporary bootstrap node is required for OpenShift cluster creation. This section assumes that a VMware vSphere host is present within the deployment environment and is associated with a VMware vCenter server. The host should be configured with appropriate storage and networking configuration.</p> <h3 id="playbooks-for-creating-the-bootstrap-node"><a href="#playbooks-for-creating-the-bootstrap-node" class="header-anchor">#</a> Playbooks for creating the bootstrap node</h3> <ol><li>inputs.yml: This file contains input variables to create the bootstrap VM. Some of the variables pertaining to the VM configuration are provided with default values as per the Red Hat guidelines. It is expected that the installation user updates the values to suit their installation environment.</li></ol> <ul><li><p><strong>datacenter_name</strong>: Name of the VMware data center.</p></li> <li><p><strong>cluster_name</strong>: Name of the VMware cluster.</p></li> <li><p><strong>datastore_name</strong>: Name of the VMware datastore.</p></li> <li><p><strong>network_name</strong>: Name of the network associated with the vSphere host.</p></li> <li><p><strong>bootstrap_disk</strong>: Disk size for the bootstrap node.</p></li> <li><p><strong>bootstrap_cpu</strong>: Number of vCPUs for the bootstrap node.</p></li> <li><p><strong>bootstrap_name</strong>: Custom name of the bootstrap node.</p></li></ul> <ol start="2"><li><p><strong>secret.yml</strong>: This is an Ansible vault file that contains sensitive information such as the VMware vCenter server IP address and credentials.</p></li> <li><p><strong>playbooks/deploy_vm.yml</strong>: This playbook is used to create the bootstrap VM.</p></li> <li><p><strong>roles/deploy_vm.yml</strong>: This is the Ansible role file that is required to create the bootstrap VM. Each role is associated with a set of tasks to accomplish the expected output and they are present in the tasks directory within the role.</p></li></ol> <p>Follow these steps to create the bootstrap node:</p> <ol><li><p>Login to the installer VM.</p></li> <li><p>Use the following command to change the directory.</p></li></ol> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> <span class="token builtin class-name">cd</span> /opt/hpe/solutions/ocp/hpe-solutions-openshift/synergy/scalable/installer/ignitions
</code></pre></div><ol start="3"><li>Update the secret.yml to provide the details of the VMware vCenter server using the following command. A sample input is provided and it is expected that the installation user updates the configuration to suit the deployment environment.</li></ol> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> ansible-vault edit secret.yml
</code></pre></div><div class="language- extra-class"><pre class="language-text"><code># vcenter hostname/ip address and credentials
vcenter_hostname: &lt;vcenter hostname&gt;;
vcenter_username: &lt;vcenter username&gt;;
vcenter_password: &lt;vcenter password&gt;;
</code></pre></div><ol start="4"><li>Update the <em>input.yml</em> file with the data center, cluster, and datastore information that will be used within the VMware vCenter server. This file provides default configuration information for the bootstrap node, if required. The configuration can be updated to suit the environment needs.</li></ol> <div class="language- extra-class"><pre class="language-text"><code># Variables for creating the bootstrap VM, as per the vSphere host configuration within the vCenter
datacenter_name: &lt;name of data center within vcenter&gt;;
cluster_name: &amp;lt;name of cluster within vcenter&gt;;
datastore_name: &amp;lt;name of datastore within vcenter&gt;;
network_name: &lt;name of the network within vcenter&gt;;

# Default values for creating the bootstrap VM
bootstrap_disk: 150
bootstrap_cpu: 4
bootstrap_memory: 16400
bootstrap_name: Bootstrap
</code></pre></div><ol start="5"><li>After the <em>input.yml</em> and <em>secret.yml</em> files are updated with appropriate values, execute the playbook with the following command to create the bootstrap VM.</li></ol> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token operator">&gt;</span> ansible-playbook –i hosts playbooks/deploy_vm.yml ––ask-vault-pass
</code></pre></div><div class="custom-block tip"><p class="custom-block-title">Note</p> <p>It is essential that all the nodes within the deployment environment are synchronized for time using an NTP server. Failure to do so will result in an installation failure due to mismatches in certificates or other files with time dependencies.</p></div> <h2 id="server-profiles"><a href="#server-profiles" class="header-anchor">#</a> Server profiles</h2> <p>Server profiles are used to configure the personality of the compute resources. A server profile allows a set of configuration parameters, including firmware recipe, network and SAN connectivity, BIOS tuning, boot order configuration, local storage configuration, and more to be templatized. These templates are the key to delivering the “infrastructure as code” capabilities of the HPE Synergy platform. For the purpose of this solution, a template is created which can be leveraged for OpenShift master nodes and OpenShift worker nodes.</p> <p>This section consists of Ansible playbooks developed to automate the tasks such as uploading firmware baseline iso package to OneView, creating server profile template and server profiles in HPE OneView and the scripts to create a virtual machine in VMware vCenter server.</p> <div class="custom-block warning"><p class="custom-block-title">Prerequisites</p> <ul><li>Ansible engine with Ansible 2.9.x and Python 3.6.x</li> <li>Python module for HPE OneView: hpOneView is the Python SDK for the OneView API that allows you to manage OneView functionalities. Download the python repository at <a href="https://github.com/HewlettPackard/oneview-python" target="_blank" rel="noopener noreferrer">https://github.com/HewlettPackard/oneview-python<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</li> <li>Ansible module for HPE OneView: OneView-ansible is the Ansible Module for HPE OneView which utilizes the python SDK to enable infrastructure as a code. Download the repository at <a href="https://github.com/HewlettPackard/oneview-ansible/" target="_blank" rel="noopener noreferrer">https://github.com/HewlettPackard/oneview-ansible/<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</li> <li>Python SDK for VMware vSphere: PyVmomi is the Python SDK for the VMware vSphere API that allows you to manage ESXi and vCenter.</li></ul></div> <div class="custom-block tip"><p class="custom-block-title">Note</p> <p>To run the automation scripts described in this section, it is mandatory to configure the Installer Machine with non-root user access and other prerequisites mentioned in the Installer machine section.</p></div> <div class="custom-block tip"><p class="custom-block-title">Note</p> <p>Some pre and post &quot;server profile template and server profile&quot; creation requirements, that need to be executed manually are as follows:</p> <ul><li>Before using the profile automation, the user needs to look out if there are any hardware errors or warnings on the compute that will be used for deploying server profile template and server profile. If there are errors or warnings on compute node, the user needs to resolve them or clear them before using the automation scripts.</li> <li>Before running automation, reset iLO for the corresponding compute, so that any communication issues between OneView and iLO can be resolved.</li> <li>After applying the profile, if there are local storage or Interconnect errors, then the user needs to perform the steps as stated in error resolution.</li></ul></div> <h3 id="software-requirements"><a href="#software-requirements" class="header-anchor">#</a> Software requirements</h3> <table><thead><tr><th>Software</th> <th>Version</th></tr></thead> <tbody><tr><td>HPE OneView</td> <td>5</td></tr> <tr><td>Red Hat Enterprise Linux Server</td> <td>7.6</td></tr> <tr><td>VMware ESXi</td> <td>6.7</td></tr> <tr><td>VMware vCenter Server Appliance</td> <td>6.7</td></tr> <tr><td>Red Hat CoreOS</td> <td>4.4</td></tr></tbody></table> <h3 id="upload-firmware-bundle"><a href="#upload-firmware-bundle" class="header-anchor">#</a> Upload firmware bundle</h3> <p>This role consists of Ansible playbooks developed to automate the task of uploading the firmware bundle or firmware baseline for Compute Module of HPE Synergy to HPE OneView.</p> <h4 id="input-files"><a href="#input-files" class="header-anchor">#</a> Input files</h4> <ul><li><p>It is mandatory to update all the input files (*inputs.yml, hosts, secret.yml, fw_version_inputs.yml) with appropriate values before running any of the playbooks available in this repository.</p></li> <li><p>Input file name: hosts</p> <ul><li>This file is an inventory of host details.</li> <li>Variables from &quot;hosts&quot; that are required by playbooks under &quot;infrastructure&quot; directory are listed as follows.</li></ul> <div class="language- extra-class"><pre class="language-text"><code># [server_profile_template]
# [server_profile]
</code></pre></div><ul><li>Input file name: <em>inputs.yml</em> <ul><li>Variables from &quot;<em>inputs.yml</em>&quot; that are required by playbooks under &quot;infrastructure&quot; directory are listed as follows.</li></ul></li></ul> <div class="language- extra-class"><pre class="language-text"><code># enclosure_group: &lt;Enclosure group name as per OneView&gt; 
# deployment_network_name: &lt;Deployment network name as per OneView&gt;
# server_profile_template_name: &lt;Custom name for Server Profile Template&gt;
# fw_bundle_path: &lt;Firmware Bundle file path&gt;
# fw_bundle_file_name: &lt;Firmware file name with extension&gt;
</code></pre></div></li> <li><p>Input file name: <em>secret.yml</em></p> <ul><li>This is an Ansible vault file.</li> <li>Variables from &quot;<em>secret.yml</em>&quot; that are required by playbooks under &quot;infrastructure&quot; directory are listed as follows.</li></ul> <div class="language- extra-class"><pre class="language-text"><code># oneview_ip: x.x.x.x 
# oneview_username: username
# oneview_password: password
# oneview_api_version: 1200
</code></pre></div></li> <li><p>Input file name: <em>fw_version_inputs.yml</em></p> <ul><li>This file contains the version information of the firmware that should be updated on the server hardware.</li> <li>Variables from &quot;<em>fw_version_inputs.yml</em>&quot; that are required by playbooks under &quot;infrastructure&quot; directory are listed as follows.</li></ul> <div class="language- extra-class"><pre class="language-text"><code># innovationengine: &lt; INNOVATION_ENGINE_VERSION &gt;
# systemrombios: &lt; SYSTEM_ROM_VERSION &gt;
# serverplatformservices: &lt; SERVER_PLATFORM_SERVICES &gt;
# powermanagementcontroller: &lt; POWER_MANAGEMENT_CONTROLLER &gt;
# ilo5: &lt; iLO_5_VERSION &gt;
</code></pre></div></li> <li><p>Execute the following commands on the installer VM to upload the firmware bundle to HPE OneView.</p></li></ul> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token comment"># cd $BASE_DIR/infrastructure</span>
<span class="token comment"># ansible-playbook -i hosts playbooks/upload_firmware_bundle.yml --ask-vault-pass</span>
</code></pre></div><div class="custom-block tip"><p class="custom-block-title">Note</p> <p>BASE_DIR is defined and set in <a href="/hpe-solutions-openshift/Preparing the execution environment/Preparing-the-execution-environment.html#installer-machine">Installer machine</a> section.</p></div> <p>Expected output on installer machine after successful upload of firmware bundle to OneView.</p> <p><img src="/hpe-solutions-openshift/assets/img/figure95.d3f7de83.png" alt=""></p> <p>Expected output after Firmware Baseline id uploaded to OneView.</p> <p><img src="/hpe-solutions-openshift/assets/img/figure96.bf0308b6.png" alt=""></p> <h3 id="create-server-profile-template"><a href="#create-server-profile-template" class="header-anchor">#</a> Create server profile template</h3> <p>This section consists of ansible playbooks developed to automate the task of creating and deploying the Server Profile Template along with attaching the firmware baseline (firmware bundle for updating the firmwares on HPE Synergy compute module) with the profile template in HPE OneView and also updating the BIOS and iLO settings.</p> <h4 id="input-files-2"><a href="#input-files-2" class="header-anchor">#</a> Input files</h4> <p>It is mandatory to update all the input files (inputs.yml, hosts, secret.yml, fw_version_inputs.yml) with appropriate values before running any of the playbooks available in this repository.</p> <ul><li><p>Input file name: hosts</p> <ul><li>This file is an inventory of host details.</li> <li>Variables from &quot;hosts&quot; that are required by playbooks under &quot;infrastructure&quot; directory are as follows.</li></ul> <div class="language- extra-class"><pre class="language-text"><code># [server_profile_template]
# [server_profile]
</code></pre></div></li> <li><p>Input file name: <em>inputs.yml</em></p> <ul><li>Variables from &quot;<em>inputs.yml</em>&quot; that are required by playbooks under &quot;infrastructure&quot; directory are listed as follows.</li> <li>Details about each of the variable is explained in the comments section of &quot;<em>input.yml</em>&quot;.</li></ul> <div class="language- extra-class"><pre class="language-text"><code># os_is_coreos: &lt;true_or_false&gt;
# enclosure_group: &lt;Enclosure group name as per OneView&gt; 
# deployment_network_name: &lt;Deployment network name as per OneView&gt;
# server_profile_template_name: &lt;Custom name for SPT&gt;
# fw_bundle_path: &lt;Firmware Bundle file path&gt;
# fw_bundle_file_name: &lt;Firmware file name with extension&gt;
# manageBios: &lt;true_or_false&gt;
# bioscomplianceControl: &lt;Checked_or_Unchecked&gt;
# manageilo: &lt;true_or_false&gt;
# ilocomplianceControl: &lt;Checked_or_Unchecked&gt;
# managefw: &lt;true_or_false&gt;
</code></pre></div></li> <li><p>Input file name: <em>secret.yml</em></p> <ul><li>This is an ansible vault file.</li> <li>Variables from &quot;<em>secret.yml</em>&quot; that are required by playbooks under &quot;infrastructure&quot; directory are listed as follows. These variables are for OneView access, iLO new user account details and privileges, and BIOS security settings.</li></ul> <div class="language- extra-class"><pre class="language-text"><code># oneview_ip: x.x.x.x 
# oneview_username: username
# oneview_password: password
# oneview_api_version: 1200
# ilo_username: &lt;ilo_new_user&gt;
# ilo_displayname: &lt;ilo_new_user_display_name&gt;
# ilo_password: &lt;ilo_new_user_password&gt;
# ilo_user_userConfigPriv: &lt;boolean_true_or_false&gt;
# ilo_user_iLOConfigPriv: &lt;boolean_true_or_false&gt;
# ilo_user_loginPriv: &lt;boolean_true_or_false&gt;
# ilo_user_remoteConsolePriv: &lt;boolean_true_or_false&gt;
# ilo_user_virtualMediaPriv: &lt;boolean_true_or_false&gt;
# ilo_user_virtualPowerAndResetPriv: &lt;boolean_true_or_false&gt;
# ilo_user_hostBIOSConfigPriv: &lt;boolean_true_or_false&gt;
# ilo_user_hostNICConfigPriv: &lt;boolean_true_or_false&gt;
# ilo_user_hostStorageConfigPriv: &lt;boolean_true_or_false&gt;
# bios_ProcAes: &lt;Enabled_or_Disabled&gt;
# bios_AssetTagProtection: &lt;Unloacked_or_Locked&gt;
# bios_SecStartBackupImage: &lt;Enabled_or_Disabled&gt;
# bios_AdvancedMemProtection: &lt;value&gt;
# bios_F11BootMenu: &lt;Enabled_or_Disabled&gt;
# bios_Workload Profile: &lt;workload_profile&gt;
</code></pre></div></li> <li><p>Input file name: <em>fw_version_inputs.yml</em></p> <ul><li>This file contains the version information of the firmware that should be updated on the server hardware.</li> <li>Variables from &quot;<em>fw_version_inputs.yml</em>&quot; that are required by playbooks under &quot;infrastructure&quot; directory are listed as follows.</li></ul> <div class="language- extra-class"><pre class="language-text"><code># innovationengine: &lt; INNOVATION_ENGINE_VERSION &gt;
# systemrombios: &lt; SYSTEM_ROM_VERSION &gt;
# serverplatformservices: &lt; SERVER_PLATFORM_SERVICES &gt;
# powermanagementcontroller: &lt; POWER_MANAGEMENT_CONTROLLER &gt;
# ilo5: &lt; iLO_5_VERSION &gt;
</code></pre></div></li> <li><p>Execute the following commands on the installer VM to create the Server Profile Template in OneView.</p> <div class="language- extra-class"><pre class="language-text"><code># cd $BASE_DIR/infrastructure
# ansible-playbook -i hosts playbooks/deploy_server_profile_template.yml --ask-vault-pass
</code></pre></div></li> <li><p>Expected output on successful creation of Server Profile Template using &quot;<em>deploy_server_profile_template.yml</em>&quot; playbook.</p></li></ul> <p><img src="/hpe-solutions-openshift/assets/img/figure14.0b0e0d7d.png" alt=""></p> <ul><li>In case template is already available, then the expected output on successful updation of Server Profile Template with the Server Profile Facts is specified in the <em>server_profile_template_file.yml</em>.</li></ul> <p><img src="/hpe-solutions-openshift/assets/img/figure15.3406502d.png" alt=""></p> <ul><li>Expected output on successful creation or updation of Server Profile Template in OneView using &quot;<em>deploy_server_profile_template.yml</em>&quot; playbook.</li></ul> <p><img src="/hpe-solutions-openshift/assets/img/figure16.f1b27963.png" alt=""></p> <h3 id="create-server-profile"><a href="#create-server-profile" class="header-anchor">#</a> Create server profile</h3> <p>This section consists of Ansible playbooks developed to automate the task of creating and deploying the Server Profile on the Server Hardware in HPE OneView. It also automates the task of applying the firmware updates, iLO and BIOS settings on the Server hardware. Lastly, it automates the task of validating the firmware updates, iLO, and BIOS settings available on server hardware are matching with firmware details available in the firmware baseline or firmware bundle and iLO and BIOS settings specified by user are matching with iLO and BIOS settings on the server hardware.</p> <h3 id="prerequisites"><a href="#prerequisites" class="header-anchor">#</a> Prerequisites</h3> <p>SELINUX: To create server profile using the automation, user should set the value of &quot;SELINUX&quot; to disabled by performing the following steps.</p> <ol><li>Switch to root user account on the Ansible Installer Machine using command &quot;su root&quot; # su root</li> <li>Enter the root password to login as root user.</li> <li>Open the selinux configuration file using the following command:<div class="language- extra-class"><pre class="language-text"><code># vi /etc/selinux/config
</code></pre></div></li> <li>Change the variable &quot;SELINUX&quot; to &quot;disabled&quot; in the &quot;/etc/selinux/config&quot; file as shown : SELINUX=disabled.</li> <li>Save and exit the config file.</li> <li>Reboot the system and login as root user and check the status of &quot;SELINUX&quot; using the following command.<div class="language- extra-class"><pre class="language-text"><code># getenforce
</code></pre></div></li> <li>Expected output from &quot;getenforce&quot; command is &quot;Disabled.&quot; After setting the &quot;SELINUX&quot; to disabled and rebooting the Ansible Installer Machine, user should exit from &quot;root&quot; account and login as &quot;non root&quot; user and activate the python3 virtual environment as listed.</li></ol> <div class="language-bash extra-class"><pre class="language-bash"><code>  <span class="token comment"># cd BASE_DIR/installer</span>
  <span class="token comment"># source ocp_venv/bin/activate</span>
</code></pre></div><h4 id="input-files-3"><a href="#input-files-3" class="header-anchor">#</a> Input files</h4> <p>It is mandatory to update all the input files (*inputs.yml, hosts, secret.yml, fw_version_inputs.yml) with appropriate values before running any of the playbooks available in this repository.</p> <div class="language- extra-class"><pre><code>- Input file name: hosts
  - This file is an inventory of host details.
  - Variables from &quot;hosts&quot; that are required by playbooks under &quot;infrastructure&quot; directory are listed as follows.
  ```   
  # [server_profile_template]
  # [server_profile]
 ```
</code></pre></div><ul><li>Input file name: <em>inputs.yml</em> <ul><li>Variables from &quot;<em>inputs.yml</em>&quot; that are required by playbooks under &quot;infrastructure&quot; directory are listed as follows.</li></ul> <div class="language- extra-class"><pre class="language-text"><code># enclosure_group: &lt;Enclosure group name as per OneView&gt; 
# deployment_network_name: &lt;Deployment network name as per OneView&gt;
# server_profile_template_name: &lt;Custom name for SPT&gt;
# fw_bundle_path: &lt;Firmware Bundle file path&gt;
# fw_bundle_file_name: &lt;Firmware file name with extension&gt;
</code></pre></div></li> <li>Input file name: <em>secret.yml</em> <ul><li>This is an Ansible vault file.</li> <li>Variables from &quot;<em>secret.yml</em>&quot; that are required by playbooks under &quot;infrastructure&quot; directory are listed as follows. These variables are for OneView access, iLO new user account details and privileges, and BIOS security settings.</li></ul> <div class="language- extra-class"><pre class="language-text"><code># oneview_ip: x.x.x.x 
# oneview_username: username
# oneview_password: password
# oneview_api_version: 1200
# ilo_username: &lt;ilo_new_user&gt;
# ilo_displayname: &lt;ilo_new_user_display_name&gt;
# ilo_password: &lt;ilo_new_user_password&gt;
# ilo_user_userConfigPriv: &lt;boolean_true_or_false&gt;
# ilo_user_iLOConfigPriv: &lt;boolean_true_or_false&gt;
# ilo_user_loginPriv: &lt;boolean_true_or_false&gt;
# ilo_user_remoteConsolePriv: &lt;boolean_true_or_false&gt;
# ilo_user_virtualMediaPriv: &lt;boolean_true_or_false&gt;
# ilo_user_virtualPowerAndResetPriv: &lt;boolean_true_or_false&gt;
# ilo_user_hostBIOSConfigPriv: &lt;boolean_true_or_false&gt;
# ilo_user_hostNICConfigPriv: &lt;boolean_true_or_false&gt;
# ilo_user_hostStorageConfigPriv: &lt;boolean_true_or_false&gt;
# bios_ProcAes: &lt;Enabled_or_Disabled&gt;
# bios_AssetTagProtection: &lt;Unloacked_or_Locked&gt;
# bios_SecStartBackupImage: &lt;Enabled_or_Disabled&gt;
# bios_AdvancedMemProtection: &lt;value&gt;
# bios_F11BootMenu: &lt;Enabled_or_Disabled&gt;
# bios_Workload Profile: &lt;workload_profile&gt;
</code></pre></div></li> <li>Input file name: <em>fw_version_inputs.yml</em> <ul><li>This file contains the version information of the firmware that should be updated on the server hardware.</li> <li>Variables from &quot;<em>fw_version_inputs.yml</em>&quot; that are required by playbooks under &quot;infrastructure&quot; directory are listed as follows.<div class="language- extra-class"><pre class="language-text"><code># innovationengine: &lt; INNOVATION_ENGINE_VERSION &gt;
# systemrombios: &lt; SYSTEM_ROM_VERSION &gt;
# serverplatformservices: &lt; SERVER_PLATFORM_SERVICES &gt;
# powermanagementcontroller: &lt; POWER_MANAGEMENT_CONTROLLER &gt;
# ilo5: &lt; iLO_5_VERSION &gt;
</code></pre></div></li></ul> <div class="language- extra-class"><pre class="language-text"><code></code></pre></div></li> <li>Execute the following commands on the installer VM to upload the firmware bundle to HPE OneView.<div class="language-bash extra-class"><pre class="language-bash"><code><span class="token comment"># cd $BASE_DIR/infrastructure</span>
<span class="token comment"># ansible-playbook -i hosts playbooks/upload_firmware_bundle.yml --ask-vault-pass</span>
</code></pre></div></li></ul> <div class="custom-block tip"><p class="custom-block-title">Note</p> <ul><li>Firmware update and validation tasks will be executed if user has selected &quot;managefw&quot; variable as &quot;true&quot; in &quot;inputs.yml&quot; file while creating the &quot;server profile template&quot;.</li> <li>iLO settings update and validation tasks will be executed if user has selected &quot;manageilo&quot; variable as &quot;true&quot; in &quot;inputs.yml&quot; file while creating the &quot;server profile template&quot;.</li> <li>BIOS settings update and validation tasks will be executed if user has selected &quot;manageBios&quot; variable as &quot;true&quot; in &quot;inputs.yml&quot; file while creating the &quot;server profile template&quot;.</li></ul></div> <div class="custom-block tip"><p class="custom-block-title">Note</p> <p>BASE_DIR is defined and set in <a href="/hpe-solutions-openshift/Preparing the execution environment/Preparing-the-execution-environment.html#installer-machine">Installer machine</a> section.</p></div> <ul><li>Expected output on successful creation of Server Profile and updation of firmware based on firmware baseline specified by the user.</li></ul> <p><img src="/hpe-solutions-openshift/assets/img/figure17.bbbf3e86.png" alt=""></p> <ul><li>Expected output after successful firmware validation based on firmware baseline specified by the user in &quot;<em>fw_versions.yml</em>&quot;.</li></ul> <p><img src="/hpe-solutions-openshift/assets/img/figure18.454cc20d.png" alt=""></p> <ul><li>Expected output in &quot;firmware&quot; section of OneView server profile is as follows.</li></ul> <p><img src="/hpe-solutions-openshift/assets/img/figure19.38e44941.png" alt=""></p> <ul><li>Expected output after successful BIOS security settings validation based on security setting specified in &quot;secret.yml&quot;.</li></ul> <p><img src="/hpe-solutions-openshift/assets/img/figure20.31c93c34.png" alt=""></p> <ul><li>Expected output after successful iLO settings validation based on iLO setting specified in &quot;secret.yml&quot;.</li></ul> <p><img src="/hpe-solutions-openshift/assets/img/figure21.7e5abf6b.png" alt=""></p> <ul><li>Expected output in OneView on successful creation of Server Profile and updation of firmware, BIOS and iLO settings.</li></ul> <p><img src="/hpe-solutions-openshift/assets/img/figure22.2cd2f198.png" alt=""></p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/hpe-solutions-openshift/Preparing the execution environment/Preparing-the-execution-environment.html" class="prev">
        Preparing the execution environment
      </a></span> <span class="next"><a href="/hpe-solutions-openshift/Physical node configuration/Physical-node-configuration.html">
        Physical node configuration
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/hpe-solutions-openshift/assets/js/app.97ab5217.js" defer></script><script src="/hpe-solutions-openshift/assets/js/2.7869ffdd.js" defer></script><script src="/hpe-solutions-openshift/assets/js/5.bae94e2f.js" defer></script>
  </body>
</html>
