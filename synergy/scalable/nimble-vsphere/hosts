###
## Copyright (2019) Hewlett Packard Enterprise Development LP
##
## Licensed under the Apache License, Version 2.0 (the "License");
## You may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
## http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.
####

[vsphere_golden_image]
# "enclosure serial number, server bay number", type=<server hardware type>
"MXQXXXXXX, bay 3" type="SY 480 Gen10 1" 

[vsphere_server_profile_template]
# "enclosure serial number, server bay number", type=<server hardware type>
"MXQ8XXXXX, bay 3" type="SY 480 Gen10 1" 

[vsphere_server_profile]
# format "enclosure serial number, server bay number", type=<server hardware type> name=<server profile name> ip=<vsphere host ip> hostname=<vsphere hostname>
# servers for virtualized management nodes 
"MXQXXXXXX, bay 3" type="SY 480 Gen10 4" name="ESXI_1" ip="10.0.x.x" hostname="vspherehost1"
"MXQXXXXXX, bay 3" type="SY 480 Gen10 1" name="ESXI_2" ip="10.0.x.x" hostname="vspherehost2"
"2S1XXXXXX, bay 3" type="SY 480 Gen10 4" name="ESXI_3" ip="10.0.x.x" hostname="vspherehost3"

# servers for virtualized worker nodes, comment and ignore the following if using physical worker nodes
"MXQXXXXXX, bay 1" type="SY 480 Gen10 4" name="ESXI_4" ip="10.0.x.x" hostname="vspherehost4"
"MXQXXXXXX, bay 3" type="SY 480 Gen10 1" name="ESXI_5" ip="10.0.x.x" hostname="vspherehost5"
"2S1XXXXXX, bay 4" type="SY 480 Gen10 4" name="ESXI_6" ip="10.0.x.x" hostname="vspherehost6"
"MXQXXXXXX, bay 5" type="SY 480 Gen10 4" name="ESXI_7" ip="10.0.x.x" hostname="vspherehost7"
"MXQXXXXXX, bay 2" type="SY 480 Gen10 1" name="ESXI_8" ip="10.0.x.x" hostname="vspherehost8"
"2S1XXXXXX, bay 3" type="SY 480 Gen10 4" name="ESXI_9" ip="10.0.x.x" hostname="vspherehost9"

[virtual-nodes]
master01.twentynet.local
master02.twentynet.local
master03.twentynet.local
infra01.twentynet.local
infra02.twentynet.local
infra03.twentynet.local
lb01.twentynet.local
lb02.twentynet.local
worker01.twentynet.local
worker02.twentynet.local
worker03.twentynet.local
worker04.twentynet.local
worker05.twentynet.local
worker06.twentynet.local

[physical-nodes]

[masters]
master01.twentynet.local
master02.twentynet.local
master03.twentynet.local

[etcd]
master01.twentynet.local
master02.twentynet.local
master03.twentynet.local

[lb]
lb01.twentynet.local
lb02.twentynet.local

[infra]
infra01.twentynet.local
infra02.twentynet.local
infra03.twentynet.local

[nodes]
master01.twentynet.local openshift_node_group_name='node-config-master'
master02.twentynet.local openshift_node_group_name='node-config-master'
master03.twentynet.local openshift_node_group_name='node-config-master'
infra01.twentynet.local openshift_node_group_name='node-config-infra'
infra02.twentynet.local openshift_node_group_name='node-config-infra'
infra03.twentynet.local openshift_node_group_name='node-config-infra'
worker01.twentynet.local openshift_node_group_name='node-config-compute'
worker02.twentynet.local openshift_node_group_name='node-config-compute'
worker03.twentynet.local openshift_node_group_name='node-config-compute'
worker04.twentynet.local openshift_node_group_name='node-config-compute'
worker05.twentynet.local openshift_node_group_name='node-config-compute'
worker06.twentynet.local openshift_node_group_name='node-config-compute'

[OSEv3:children]
nodes
etcd
lb
infra

[OSEv3:vars]
openshift_disable_check=docker_image_availability,disk_availability,memory_availability,docker_storage,package_availability

# if using user other than root, uncomment the variable ansible_become
#ansible_ssh_user=oshiftuser

# if using root user, comment the variable ansible_become
ansible_ssh_user=root
#ansible_become=true

openshift_disable_check=docker_image_availability
openshift_release="3.11"
openshift_deployment_type=openshift-enterprise

# The below variables are used in case of htpass authentication
# comment the below variables to use LDAP authentication
openshift_master_identity_providers=[{'name': 'htpass_auth', 'login':'true', 'challenge': 'true','kind': 'HTPasswdPasswordIdentityProvider'}]
openshift_master_htpasswd_file='/etc/oshift-hash-pass.htpasswd'

# uncomment the below line to use LDAP authentication. it is required to have the user provided below in the LDAP 
#openshift_master_identity_providers=[{'name': 'my_ldap_provider', 'challenge': 'true', 'login': 'true', 'kind': 'LDAPPasswordIdentityProvider', 'attributes': {'id': ['dn'], 'email': ['super@tennet.local'], 'name': ['super'], 'preferredUsername': ['super']}, 'bindDN': 'cn=super,cn=users,dc=tennet,dc=local', 'bindPassword': 'Password!234', 'insecure': 'True', 'url': 'ldap://tennetdc.tennet.local/cn=users,dc=tennet,dc=local?uid'}]

public_hosted_zone=twentynet.local
load_balancer_hostname=lb01.twentynet.local
openshift_master_cluster_hostname=lb01.twentynet.local
openshift_master_cluster_public_hostname=lb01.twentynet.local
openshift_public_hostname=lb01.twentynet.local
openshift_install_examples=false
openshift_master_cluster_method=native
openshift_use_openshift_sdn=true
osm_cluster_network_cidr=12.0.0.0/8
osm_host_subnet_length=9
openshift_registry_selector='node-role.kubernetes.io/infra=true'
openshift_router_selector='node-role.kubernetes.io/infra=true'
openshift_enable_service_catalog=False
dynamic_volumes_check=False

# Red Hat credentials from the vault file
oreg_auth_user="{{ vault_rhsub_user }}" 
oreg_auth_password="{{ vault_rhsub_pass }}"

oreg_test_login=false
oreg_url="registry.access.redhat.com/openshift3/ose-${component}:${version}"
openshift_master_default_subdomain=apps.tennet.local

# cluster console
# update the variable below openshift_console_install to true to enable cluster console
openshift_console_install=false
openshift_console_hostname=adminconsole.apps.twentynet.local

# metrics
# update the openshift_metrics_install_metrics variable to true to enable metrics
openshift_metrics_install_metrics=false
openshift_metrics_storage_kind=dynamic
openshift_metrics_storage_volume_size=25Gi

# logging
# update the openshift_logging_install_logging to true to enable logging.
openshift_logging_install_logging=false
openshift_logging_es_pvc_dynamic=true
openshift_logging_es_pvc_size=200Gi
openshift_logging_elasticsearch_storage_type=pvc
openshift_logging_es_pvc_storage_class_name=sc-es
openshift_logging_es_pvc_prefix=oc-efk-log
openshift_logging_es_cluster_size=3
openshift_logging_es_nodeselector={"node-role.kubernetes.io/compute": "true"}
openshift_logging_kibana_nodeselector={"node-role.kubernetes.io/compute": "true"}
openshift_logging_curator_nodeselector={"node-role.kubernetes.io/compute": "true"}
openshift_logging_fluentd_nodeselector={"node-role.kubernetes.io/compute": "true"}
openshift_logging_es_number_of_replicas=3
openshift_logging_es_allow_external=true
openshift_logging_es_hostname=ses.router.twentynet.local
openshift_logging_kibana_hostname=3kibana.router.twentynet.local
logging_elasticsearch_rollout_override=true

# Prometheus
# update the variable openshift_cluster_monitoring_operator_install
openshift_cluster_monitoring_operator_install=false
openshift_cluster_monitoring_operator_prometheus_storage_capacity=true
openshift_cluster_monitoring_operator_alertmanager_storage_capacity=true
openshift_cluster_monitoring_operator_alertmanager_config=true
openshift_cluster_monitoring_operator_prometheus_storage_enabled=true
openshift_cluster_monitoring_operator_alertmanager_storage_enabled=true
openshift_cluster_monitoring_operator_install=true

# Monitoring
# update the variable openshift_cluster_monitoring_operator_install to true to enable monitoring
openshift_cluster_monitoring_operator_install=false
openshift_cluster_monitoring_operator_node_selector={"node-role.kubernetes.io/compute": "true"}
openshift_cluster_monitoring_operator_prometheus_storage_enabled=true
openshift_cluster_monitoring_operator_alertmanager_storage_enabled=true
openshift_cluster_monitoring_operator_prometheus_storage_capacity=50Gi
openshift_cluster_monitoring_operator_alertmanager_storage_capacity=20Gi
openshift_cluster_monitoring_operator_prometheus_storage_enabled=true
openshift_cluster_monitoring_operator_alertmanager_storage_enabled=true